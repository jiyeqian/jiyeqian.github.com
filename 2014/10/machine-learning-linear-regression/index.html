<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>机器学习：线性回归</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>机器学习：线性回归</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2014-10-19">2014-10-19</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
  <a href="/tags/#线性回归" title="线性回归">线性回归</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">模型介绍</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-19-linear_regression_0.png"><img src="/assets/images/2014-10-19-linear_regression_0.png" alt="线性回归" /></a><div class="caption">Figure 1:  线性回归 [<a href="/assets/images/2014-10-19-linear_regression_0.png">PNG</a>]</div></div></div>

<p>线性回归（Linear Regression）的回归函数（hypothesis）为
\begin{equation}
h_\boldsymbol\theta(\mathbf x) = \boldsymbol\theta^T \mathbf x = \theta_0 x_0 + \theta_1 x_1 + \cdots  + \theta_n x_n
\label{eq:hypothesis_linear_regression}
\end{equation}
其中，常数项$x_0 = 1$。</p>

<p>代价函数（cost function）为
\begin{equation}
J(\boldsymbol\theta) = \frac{1}{2m}\sum_{i=1}^{m}{\left(h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right) - y^{(i)}\right)^2}
\label{eq:cost_function_linear_regression}
\end{equation}
其中，$\mathbf x^{(i)} = \left(x_1^{(i)}, x_2^{(i)}, \cdots, x_n^{(i)}\right)$， $m$为样本数量，$n$为特征数量。</p>

<h2 id="section-1">参数估计</h2>

<p>参数估计是通过最小化代价函数\eqref{eq:cost_function_linear_regression}，求解模型参数$\boldsymbol\theta$。
\[
\min_\boldsymbol\theta J(\boldsymbol\theta)
\]
通常有两种解法：万能的梯度下降法（gradient descent）和正规方程（normal equation）的解析解（也就是线性回归的最小二乘解）。</p>

<h3 id="section-2">梯度下降法</h3>

<p>repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\boldsymbol\theta)~~~~~~(j = 0, 1, \ldots, n)
\]
}<br />
也就是<br />
repeat until convergence {
\[
\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right) x_j^{(i)}~~~~~~(j = 0, 1, \ldots, n)
\]
}  <br />
每轮循环的时候同时更新$\theta_j$（下图左边为正确更新方式。<em>不能</em>在同一轮循环中，先更新部分$\theta_j$，再利用已更新的$\theta_j$更新其它$\theta_j$）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-19-linear_regression_1.png"><img src="/assets/images/2014-10-19-linear_regression_1.png" alt="同时更新" /></a><div class="caption">Figure 2:  同时更新 [<a href="/assets/images/2014-10-19-linear_regression_1.png">PNG</a>]</div></div></div>

<p>特征的尺度如果相差太大，需要进行尺度规范化提高收敛速度，常用的规范化方法是
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\]
或者
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{std}}
\]</p>

<p><strong>注意事项：</strong>   </p>

<ul>
  <li>线性回归的代价函数$J(\boldsymbol\theta)$不存在局部极值（local optima），存在全局极值；</li>
  <li>将所有特征归一（feature scaling）到统一的尺度$-1\le x_i\le 1$有助于提高梯度下降法的速度（不归一化$x_0$）；</li>
  <li>学习率$\alpha$太小收敛慢，太大可能错过极值点而不收敛，过大的$\alpha$甚至可能导致$J(\boldsymbol\theta)$不降反升；</li>
  <li>在迭代过程中保持$\alpha$不变，梯度下降步长也会自动减小，因为梯度会不断减小。</li>
</ul>

<p>尺度归一化效果： </p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-10-19-linear_regression_2.png"><img src="/assets/images/2014-10-19-linear_regression_2.png" alt="尺度归一化效果示例" /></a><div class="caption">Figure 3:  尺度归一化效果示例 [<a href="/assets/images/2014-10-19-linear_regression_2.png">PNG</a>]</div></div></div>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-10-19-linear_regression_3.png"><img src="/assets/images/2014-10-19-linear_regression_3.png" alt="梯度下降步长自动减小" /></a><div class="caption">Figure 4:  梯度下降步长自动减小 [<a href="/assets/images/2014-10-19-linear_regression_3.png">PNG</a>]</div></div></div>

<h3 id="section-3">正规方程求解</h3>

<p>正规方程求解也就是线性回归的最小二乘解为
\begin{equation}
\boldsymbol\theta = \left(\mathbf X^T\mathbf X\right)^{-1}\mathbf X^T\mathbf y
\end{equation}</p>

<p><strong>注意事项：</strong>     </p>

<p>$\mathbf X^T\mathbf X$可能不可逆，导致的原因可能是冗余特征（redundant features）和特征数目过多（$n$太多而$m$太少），解决的办法：   </p>

<ul>
  <li>冗余特征线性相关（e.g. $x_1 = 2x_2$）：删除线性相关特征；</li>
  <li>特征数目过多（e.g. $m \le n$）：删除特征、正则化（regularization）。</li>
</ul>

<p>Matlab Code：</p>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span></code></pre></div>

<p>其中，pinv函数可以处理$\mathbf X^T\mathbf X$不可逆的情况，而inv函数不行。</p>

<h3 id="section-4">求解方法比较</h3>

<table>
  <thead>
    <tr>
      <th>Gradient Descent</th>
      <th>Normal Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>需要迭代</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$n$很大时（$10^6$）工作良好</td>
      <td>$n$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化</td>
    </tr>
  </tbody>
</table>

<h2 id="section-5">多项式回归</h2>

<p>构造多项式特征，借助多项式回归，可以利用线性回归模型解决非线性问题。当$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $时，多项式回归（polynomial regression）\eqref{eq:hypothesis_polynomial_regression}可以直接转化为线性模型\eqref{eq:hypothesis_linear_regression}。
\begin{equation}
h_\boldsymbol\theta(\mathbf x) = \boldsymbol\theta^T\mathbf x = \theta_0 x_0 + \theta_1 x + \theta_2 x^2 ＋ \theta_3 x^3 \cdots  + \theta_n x^n
\label{eq:hypothesis_polynomial_regression}
\end{equation}</p>

<p><strong>注意事项：</strong>  </p>

<p>当对特征进行高此多项式变换后，特征的取值范围可能会急剧变化，需要对多项式特征进行规范化处理<a href="#ng_ml_rlrbv_pe_2014">[1, P. 8]</a>，也就是特征的尺度归一化。</p>

<h2 id="section-6">思考问题</h2>

<ol>
  <li>如何通过收敛判断梯度下降法是否应该停止迭代？</li>
  <li>特征尺度规范化（feature scaling）为何能提高梯度下降法收敛速度？</li>
  <li>为什么正规方程估计参数不需要特征尺度规范化？</li>
</ol>

<h2 id="section-7">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_rlrbv_pe_2014">[1]A. Ng, “Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<p><a href="https://class.coursera.org/ml-007">Machine Learning （Andrew Ng）</a>    <br />
<a href="http://en.wikipedia.org/wiki/Linear_regression">Wikipedia: Linear regression</a></p>

</section>
<section align="right">
<br/>
<span>
  <a  href="/2012/07/host-your-pages-at-github-using-jekyll" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2014/10/machine-learning-logistic-regression" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2014/10/machine-learning-linear-regression" data-url="http://qianjiye.de/2014/10/machine-learning-linear-regression" data-title="机器学习：线性回归">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2012/07/host-your-pages-at-github-using-jekyll'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2014/10/machine-learning-logistic-regression'; --1> -->
<!--         url = '/2014/10/machine-learning-logistic-regression'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2014/10/machine-learning-linear-regression/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
