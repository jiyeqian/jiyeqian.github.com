<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>机器学习：支持向量机（SVM）</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>

  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>机器学习：支持向量机（SVM）</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2014-11-27">2014-11-27</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
  <a href="/tags/#支持向量机" title="支持向量机">支持向量机</a>&nbsp;
  
  <a href="/tags/#Logistic回归" title="Logistic回归">Logistic回归</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="logisticsvm">从Logistic回归到SVM</h2>

<p>本节通过Logistic回归的代价函数，演化到SVM的代价函数，然后通过代价函数最小化推导SVM的判别界<a href="#ng_ml_svm_2014">[1, P. 3—14]</a>。</p>

<p>Logistic回归的代价函数为</p>

<p>\begin{equation}
\begin{aligned}
J(\boldsymbol\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\boldsymbol\theta_j^2
\end{aligned}，
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<p>取出代价函数中的一项如下图，并绘制逼近Logistic代价函数的两个新的代价函数项$\mbox{cost}_1(\mathbf z)$和$\mbox{cost}_0(\mathbf z)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png" alt="图解Logistic回归的代价函数" /></a><div class="caption">Figure 1:  图解Logistic回归的代价函数 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png">PNG</a>]</div></div></div>

<p>将Logistic回归的代价函数提取$\lambda\over m$，令$C={1\over\lambda}$，并用新的代价函数项替代Logistic回归的代价函数项，可得SVM的代价函数</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2 ，
\label{eq:cf-svm-r}
\end{equation}</p>

<p>新的代价函数不再以$0$作为分类边界，而是以$+1$和$-1$。若代价函数\eqref{eq:cf-svm-r}要得最小值，可得SVM的判别界</p>

<p>\begin{equation}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2}\sum_{j=1}^{n}\boldsymbol\theta_j^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\label{eq:svm-decision-boundary}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png" alt="图解SVM的判别界" /></a><div class="caption">Figure 2:  图解SVM的判别界 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png">PNG</a>]</div></div></div>

<p>根据向量间的夹角公式$\cos\theta = \frac{\boldsymbol\theta^T\mathbf x^{(i)}}{\left\lVert\boldsymbol\theta\right\rVert\left\lVert\mathbf x^{(i)}\right\rVert}$，则$p^{(i)}=\left\lVert\mathbf x^{(i)}\right\rVert\cos\theta$表示向量$\mathbf x^{(i)}$在向量$\boldsymbol\theta$上的投影，SVM的判别界\eqref{eq:svm-decision-boundary}可以改写为</p>

<p>\begin{equation*}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2} \lVert\boldsymbol\theta\rVert^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\end{equation*}</p>

<p>$\boldsymbol\theta$是判别界的法线，$p^{(i)}$的值可正可负。要满足判别界的条件，$\left\vert p^{(i)}\right\vert$越大越好，这样$\lVert\boldsymbol\theta\rVert$就可以取到很小的值。因此，上图中右下比左下有更大的$\left\vert p^{(i)}\right\vert$，是更合适的SVM判别界。</p>

<h2 id="section">核函数</h2>

<p>SVM为了解决非线性可分问题，需要引入核函数（kernel）。作为SVM的核函数须满足Mercer定理。</p>

<blockquote>
  <h4 id="mercer-a-hrefjulysvm20142a">Mercer 定理<a href="#July_svm_2014">[2]</a></h4>
  <hr />
  <p>函数$\kappa$是 $\mathbb R^n \times \mathbb R^n \to \mathbb R$ 上的映射。如果$\kappa$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$\{\mathbf x_1,\mathbf x_2,\ldots,\mathbf x_n\}$，其相应的核函数矩阵是对称半正定的。</p>
</blockquote>

<p>通过特征与参考地标（landmark）$\mathbf l^{(i)}$之间的相似性，定义高斯核为<a href="#ng_ml_svm_2014">[1, P. 17—20]</a></p>

<p>\begin{equation}
\mathbf f_i = \mbox{similarity}\left(\mathbf x, \mathbf l^{(i)} \right)
= \exp\left(-\frac{\left\lVert\mathbf x - \mathbf l^{(i)}\right\rVert}{2\sigma^2} \right)。
\end{equation}</p>

<p>核函数的作用相当于特征变换，SVM引入了核函数的代价函数为</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2。
\label{eq:cf-svm-kernel-r}
\end{equation}</p>

<p>SVM代价函数和核函数的参数对SVM分类有如下影响<a href="#ng_ml_svm_2014">[1, P. 25]</a>：</p>

<ol>
  <li>大的$C~\left(C={1\over\lambda}\right)$导致Low Bias和High Variance；</li>
  <li>小的$C~\left(C={1\over\lambda}\right)$导致High Bias和Low Variance；</li>
  <li>大的$\sigma^2$使得特征$\mathbf f_i$变化平缓，导致High Bias和Low Variance；</li>
  <li>小的$\sigma^2$使得特征$\mathbf f_i$变化较大，导致Low Bias和High Variance。 </li>
</ol>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg" alt="高斯核的SVM分类效果" /></a><div class="caption">Figure 3:  高斯核的SVM分类效果 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg">SVG</a>, <a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.png">PNG</a>]</div></div></div>

<h2 id="smo">SMO算法</h2>

<h2 id="svm">使用SVM</h2>

<p>Logistic回归强调所有点尽可能地远离中间那条线，SVM更应该关心靠近中间分割线的点，让他们尽可能地远离中间线。SVM考虑局部（不关心已经确定远离的点），Logistic回归考虑全局（已经远离的点可能通过调整中间线使其能够更加远离）<a href="#JerryLead_svm1_2011">[3]</a>。</p>

<h3 id="logisticsvm-1">Logistic回归、SVM、神经网络</h3>

<p>如何在分类器Logistic回归、SVM和神经网络之间做出选择？</p>

<p>$n$为特征数目（$x\in \mathbb{R}^{n+1} $），$m$为训练集样本数目，分类器选择的方法如下<a href="#ng_ml_svm_2014">[1, P. 31]</a>：</p>

<ol>
  <li>若$n$很大（比如$n\geq m, n=10000,m=10,\ldots,1000$），采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，用Logistic回归难以训练好非线性分类器）；</li>
  <li>若$n$很小而$m$大小适中（比如$n=1,\ldots,1000,m=10,\ldots,10000$），采用高斯核的SVM；</li>
  <li>若$n$很小而$m$很大（比如$n=1,\ldots,1000,m=50000+$），先可以增加特征，然后采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，高斯核的SVM分类器训练会慢）；</li>
  <li>神经网络在以上大多数情况都工作较好，但是可能训练速度会很慢；</li>
  <li>神经网络是非凸优化，会陷入局部极值，SVM是凸优化，不用担心陷入局部极值。</li>
</ol>

<p>选择Logistic回归或者无核函数（线性核函数）的SVM，是因为Logistic回归和无核函数（线性核函数）的SVM相似。</p>

<p>核函数的Logistic回归训练较慢，核函数的SVM可以训练较快。</p>

<h2 id="section-1">应用案例</h2>

<h3 id="a-hrefngmlsvmex20144-p-1015a">垃圾邮件分类<a href="#ng_ml_svm_ex_2014">[4, P. 10—15]</a></h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg"><img src="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg" alt="Email转换为特征向量" /></a><div class="caption">Figure 4:  Email转换为特征向量 [<a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg">SVG</a>]</div></div></div>

<p>垃圾邮件分类首先需要将Email文本转换为特征向量。如上图所示，转换方法如下：</p>

<ol>
  <li>将Email文本转换为纯单词，比如：单词小写化，移除所有HTML标签，url链接均用单词<code>httpaddr</code>代替，提取词干（例如including、includes和included都用include代替）等；</li>
  <li>利用事先准备的词典，将单词用其在词典中的索引表示，实际应用中，词典通常有10000到50000个单词；</li>
  <li>将索引转换为$\{0, 1\}$特征向量，向量长度和词典中词汇数目一样，某个单词出现用$1$表示，否者用$0$表示。  </li>
</ol>

<h2 id="section-2">相关评论</h2>

<p>事实上，支持向量机是一个具有很好数学基础的分类方法，但它本质上也只不过是一个简单的两层方法：第一层可以看作是一些单元集合（一个支持向量就是一个单元），这些单元通过核函数能够度量输入向量和每个支持向量的相似度；第二层则把这些相似度做了简单的线性累加。支持向量机第一层的训练和最简单的无监督学习基本一致：利用支持向量来表示训练样本。一般来讲，通过调整核函数的平滑性（参数）能在线性分类和模板匹配之间做出平衡。从这个角度来讲，核函数只不过是一种模板匹配方法。<a href="#lecun_dl_vs_svm_2014">[5]</a></p>

<p>［@余凯_西二旗民工］很多有关SVM的教科书都有misleading: (1)KKT条件，support vectors，quadratic programing都是浮云；（2）kernel本身对理解学习问题有帮助，但实际工程上用处为0；（3）hinge loss只是众多可选项之一，logistic效果一点不差。［@老师木］做论文时迷信kernel，margin，认为这些机制和graphical model结合不就无敌了吗？等论文出来的结论是hinge loss对比无优越性。再翻vapnik的 统计学习的本质，原来他老人家也早做过对比试验，没发现svm相对于lr的优越性。看损失函数曲线，真看不出hinge loss比log loss好，只是hinge loss能得到稀疏解，看上去很美。当然理解这些优化理论本身会给人一些享受。数据线性可分的可能性随维度升高而变大，这是Thomas cover五六十年代得出的结论。要严格一点，数据线性可分和线性无关等价，张成维度高的线性空间所需的基多，其实就是VC维了。引入kernel就是引入非线性，使变换后的样例线性无关。Rbf核等价于无穷次多项式拟合，相对于有限的样例，没有不能分开的。<a href="#mu_svm_myth_2014">[6]</a></p>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_svm_2014">[1]A. Ng, “Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="July_svm_2014">[2]July, “支持向量机通俗导论（理解SVM的三层境界）.” csdn, 2014.</span>

[<a href="http://blog.csdn.net/v_july_v/article/details/7624837/">Online</a>]

</li>
<li><span id="JerryLead_svm1_2011">[3]JerryLead, “支持向量机SVM（一）.” cnblogs, 2011.</span>

[<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html">Online</a>]

</li>
<li><span id="ng_ml_svm_ex_2014">[4]A. Ng, “Programming Exercise 6: Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="lecun_dl_vs_svm_2014">[5]Y. LeCun, “深度学习与支持向量机有什么联系？.” 52cs, 2014.</span>

[<a href="http://www.52cs.org/?p=46">Online</a>]

</li>
<li><span id="mu_svm_myth_2014">[6]老师木, “SVM神话.” 52cs, 2014.</span>

[<a href="http://www.52cs.org/?p=359">Online</a>]

</li></ol>

<h3 id="section-4">脚注</h3>


</section>
<section align="right">
<br/>
<span>
  <a  href="/2014/11/machine-learning-advice-for-applying-machine-learning" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2014/12/machine-learning-clustering" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2014/11/machine-learning-support-vector-machines" data-url="http://qianjiye.de/2014/11/machine-learning-support-vector-machines" data-title="机器学习：支持向量机（SVM）">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2014/11/machine-learning-advice-for-applying-machine-learning'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2014/12/machine-learning-clustering'; --1> -->
<!--         url = '/2014/12/machine-learning-clustering'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2014/11/machine-learning-support-vector-machines/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2014 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
