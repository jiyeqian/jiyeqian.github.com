<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>机器学习：VC维
    </title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>

  <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
          (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
          e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

      _st('install','RFsGWEn74xUkvs4V5QRD');
  </script>

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>机器学习：VC维</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2014-12-23">2014-12-23</time>
</span>

 |
<span class="categories">
  <i class="fa fa-share-alt"></i>
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/#机器学习理论" title="机器学习理论">机器学习理论</a>&nbsp;
  
</span>

</section>
<section class="post">
<p>本节的主要内容来自Hsuan-Tien Lin的机器学习基石课程<a href="#lin_ml_vcd_2014">[1]</a>。</p>

<p>Learning happens, if finite $d_{VC}$, large $N$, and low $E_{in}$.</p>

<h2 id="section">学习的可行性</h2>

<p>当$N\geq 2,k\geq 3$时，成长函数满足约束条件
\begin{equation}
m_{\mathcal H}(N)\leq B(N,k)=\sum_{i=0}^{k-1}\binom{N}{i}\leq N^{k-1}，
\end{equation}
由此可见，断点$k$是判断成长函数大小的重要条件。当$k\geq 3$时，才能使用上界约束$N^{k-1}$。</p>

<p>对于$\forall g=\mathcal A(\mathcal D)\in\mathcal H$和足够大的数据集$\mathcal D$，当$k\geq 3$时，
\begin{equation}
\begin{aligned}
P_{\mathcal D}\left[\lvert E_{in}(g)-E_{out}(g)\rvert&gt;\epsilon\right] \leq &amp; P_{\mathcal D}\left[\exists h\in\mathcal H\mbox{ s.t. }\lvert E_{in}(h)-E_{out}(h)\rvert&gt;\epsilon\right] \\
\leq &amp; 4m_{\mathcal H}(2N)\exp\left(-{1\over 8}\epsilon^2N\right)\\
\leq &amp; 4(2N)^{k-1}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{aligned}
\end{equation}</p>

<p>由此可知，学习是可行的，前提是满足下列条件：</p>

<ol>
  <li>好的假设集$\mathcal H$：$m_{\mathcal H}(N)$的断点是$k$；</li>
  <li>好的数据集$\mathcal D$：$N$足够大；</li>
  <li>好的演算法$\mathcal A$：$\mathcal A$能够选到一个$g$使得$E_{in}(g)$很小。</li>
</ol>

<p>其中，前两个条件可能得到$E_{out}\approx E_{in}$。</p>

<h2 id="vc">VC维</h2>

<p>$\mathcal H$的VC维$d_{VC}是指$满足$m_{\mathcal H}(N)=2^N$的最大$N$，VC维也满足：</p>

<ul>
  <li>$\mathcal H$可以打碎的最多输入数据个数；</li>
  <li>$d_{VC}=\mbox{‘minmum }k\mbox{‘} - 1$。</li>
</ul>

<p>若$N\leq d_{VC}$，$\mathcal H$能够打碎某些$N$个点的数据子集；若$k &gt; d_{VC}$，$k$必是$\mathcal H$的断点；如果$N\geq 2, d_{VC}\geq 2$， 显然有
\begin{equation}
m_{\mathcal H}(N)\leq N^{d_{VC}}。
\end{equation}</p>

<p>1维空间的正射线和正区间的$d_{VC}$分别是$1$和$2$；2维空间感知器的$d_{VC}=3$；2维空间凸包的$d_{VC}=\infty$。</p>

<p>有限$d_{VC}$的$\mathcal H$就是好的假设集。若$d_{VC}$有限，存在$g$使得$E_{out}(g)\approx E_{in}(g)$，并且不需要受学习算法$\mathcal A$、输入数据分布$P$和目标函数$f$的制约。</p>

<blockquote>
  <h4 id="section-1">练习题</h4>
  <hr />
  <p>若存在$N$个点的数据集不能被$\mathcal H$打碎，仅仅根据这条信息，可以得到关于$d_{VC}(H)$的什么结论？       </p>

  <p>［A］$d_{VC}(\mathcal H)&gt;N$；［B］$d_{VC}(\mathcal H)=N$；［C］$d_{VC}(\mathcal H)&lt;N$；［D］无法得出以上任何结论。</p>

  <p>答案：［D］。</p>
</blockquote>

<h2 id="vc-1">感知器的VC维</h2>

<p>如何证明$d$维空间感知器的VC维$d_{VC}=d+1$？若能证明$d_{VC}\geq d+1$且$d_{VC}\leq d+1$，那么就可以得到$d_{VC}=d+1$。</p>

<blockquote>
  <h4 id="dvcgeq-d1">练习题：以下哪个表明$d_{VC}\geq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［A］。</p>
</blockquote>

<p>上面练习表明，只要在$d$维空间找到一组$d+1$个点的数据集能被感知器打碎（$d+1$个点的所有二分法都能实现），那么就证明了$d_{VC}\geq d+1$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png" alt="d维空间的一组数据点" /></a><div class="caption">图 1:  d维空间的一组数据点 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png">PNG</a>]</div></div></div>

<p>这组数据点如上图所示，红色框中的数据点表述了上方$2$维空间3个点的坐标。最左边1列灰色的$1$，表示感知器的偏移常量。</p>

<p>易知，$\mathbf X$是$(d+1)\times (d+1)$的方正，且所有列线性无关（秩为$d+1$）。因此，$\mathbf X$可逆，且是$d+1$维线性空间的基。若$\mathbf y$表示任意一种二分类结果（$\mathbf X$可被打碎），那么这样的二分类方式$\mathbf w=\mathbf X^{-1}\mathbf y$总存在，因此$d_{VC}\geq d+1$。</p>

<blockquote>
  <h4 id="dvcleq-d1">练习题：以下哪个表明$d_{VC}\leq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［D］。</p>
</blockquote>

<p>上面练习表明，若$d$维空间任意$d+2$个点的数据集均不能被感知器打碎，那么就证明了$d_{VC}\leq d+1$。</p>

<p>若$d$维空间中任取$d+2$个点，都存在一种不能实现的二分类方法，那么这$d+2$个点就不能被打碎。</p>

<p>补上常数项$1$。因为$d+2$个$d+1$维的向量必定线性相关，任意一个均可用其它$d+1$个表示，那么
\[
\mathbf x_{d+2} = a_1\mathbf x_{1} + a_2\mathbf x_{2} + \ldots a_{d+1}\mathbf x_{d+1}，
\]
其中，$a_i(i=1,2,\dots,d+1)$不全为$0$。上式两边同时乘以$\mathbf w^T$可得
\[
\mathbf w^T\mathbf x_{d+2} = a_1\mathbf w^T\mathbf x_{1} + a_2\mathbf w^T\mathbf x_{2} + \ldots a_{d+1}\mathbf w^T\mathbf x_{d+1}。
\]
若要所有二分法都可行，上式不论右边$\mathbf x_i(i=1,2,\dots,d+1)$取何值，左边$\mathbf w^T\mathbf x_{d+2}$既可取正又可取负。采用反证法，找到一组左边$\mathbf x_i(i=1,2,\dots,d+1)$的取值，使得$\mathbf w^T\mathbf x_{d+2}$只能取正（或者负）。</p>

<p>假设这$d+2$个点能被打碎，一定存在一种分类情况使得$\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)$的符号与$a_i(i=1,2,\dots,d+1)$的符号相同，那么$a_i\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)&gt;0$，这样$\mathbf w^T\mathbf x_{d+2}$就只能取正。那么，这$d+2$个点不能被打碎，与假设矛盾。因此，$d$维空间中，任意$d+2$个点都存在不可能二分类的情况，也就是$d$维空间中的任何$d+2$个输入都不能被打碎，那么$d_{VC}\leq d+1$。</p>

<h2 id="vc-2">理解VC维</h2>

<p>VC维可以理解为假设集$\mathcal H$的自由度，它衡量了$\mathcal H$的分类能力。$d_{VC}$可以直观的认为是$\mathcal H$可调节参数的个数（但不总是这样）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png" alt="VC维相当于自由度" /></a><div class="caption">图 2:  VC维相当于自由度 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png">PNG</a>]</div></div></div>

<p>如上图所示，正射线只有一个可以调节参数，自由度是$1$，$d_{VC}=1$；正区间有两个可以调节参数，自由度是$2$，$d_{VC}=2$。感知器的参数向量$\mathbf w = (w_0,w_1,\ldots,w_d)$是$d+1$维（有$d+1$个可自由调节参数）和它的$d_{VC}$一致。</p>

<p>过原点的感知器的$d_{VC}=d$，因为少了一个自由度。</p>

<p>对于机器学习是否可行，有两个判断指标：（1）$E_{out}(g)\approx E_{in}(g)$？（2）$E_{in}(g)$是否足够小。$d_{VC}$（或$M$）可以作为这两个条件的评价指标。若$d_{VC}$较小，更大概率保证$E_{out}(g)\approx E_{in}(g)$，但$\mathcal H$的能力较弱，可选择的假设较少，难以使$E_{in}(g)$较小；若$d_{VC}$较大，$\mathcal H$的能力较强，可选择的假设较多，更容易使$E_{in}(g)$较小，但$E_{out}(g)\approx E_{in}(g)$的概率偏低。</p>

<h2 id="vc-3">应用VC维</h2>

<p>对于$\forall g=\mathcal A(\mathcal D)\in \mathcal H$和大的数据集$\mathcal D$，当$d_{VC}\geq 2$时，坏事儿发生概率的$VC$界为
\begin{equation}
P_{\mathcal D}\left[\left\lvert E_{in}(g)-E_{out}(g)\right\rvert&gt;\epsilon\right]\leq 4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{equation}
令$\delta=4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)$，可得$\Omega(N,\mathcal H,\delta)=\epsilon=\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}$，那么可得$E_{out}(g)$的上界
\begin{equation}
E_{out}(g)\leq E_{in}(g)+\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}。
\end{equation}</p>

<p>$\Omega(N,\mathcal H,\delta)$用于度量模型的复杂度，揭示了限定$E_{out}(g)$和$E_{in}(g)$差异的方法。</p>

<div class="image_line" id="vc-and-errors"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png" alt="VC维与误差的关系" /></a><div class="caption">图 3:  VC维与误差的关系 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png">PNG</a>]</div></div></div>

<p>上图展示了VC维和误差的关系，随着VC维$d_{VC}$的增加，模型越来越复杂，但是误差并非越来越小，$E_{out}(g)$和$E_{in}(g)$的差异却越来越大，发生坏事儿的概率变大了。由此可见，强大的$\mathcal H$（$d_{VC}$大）不总是好事儿，要选择合适的$d_{VC}^*$。</p>

<p>给定指标$\epsilon=0.1,\delta=0.1,d_{VC}=3$，可以计算大约需要$N\approx 30000$的数据集能满足要求。</p>

<p>理论上需要$N\approx 10000d_{VC}$才能满足要求，实际上通常只需$N\approx 10d_{VC}$，这是因为在推导$VC$界的时候，不等式不断放大，得到的是一个很宽松的上界。</p>

<p>$d_{VC}$虽是一个宽松的值，但可认为对所有模型都<strong>同样一致</strong>宽松，因此在模型之间比较时，仍有重要作用。</p>

<h2 id="dvc">$d_{VC}$容易计算吗？</h2>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="lin_ml_vcd_2014">[1]H.-T. Lin, “Lecture 7: The VC Dimension.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>

</section>
<section align="right">
<p>
<img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px">
</p>
<br/>
<span>
  <a  href="/2014/12/machine-learning-theory-of-generalization" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2014/12/machine-learning-unbalanced-data-sets" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2014/12/machine-learning-the-vc-dimension" data-url="http://qianjiye.de/2014/12/machine-learning-the-vc-dimension" data-title="机器学习：VC维">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2014/12/machine-learning-theory-of-generalization'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2014/12/machine-learning-unbalanced-data-sets'; --1> -->
<!--         url = '/2014/12/machine-learning-unbalanced-data-sets'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2014/12/machine-learning-the-vc-dimension/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
