<h1 id="references">References</h1>

<p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. <i>Journal of Machine Learning Research</i>, <i>12</i>, 2825–2830.</p>

<p>Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). Recent Advances of Large-Scale Linear Classification. <i>Proceedings of the IEEE</i>, <i>100</i>(9), 2584–2603.</p>

<p>July. (2014). 支持向量机通俗导论（理解SVM的三层境界）. csdn. Retrieved from http://blog.csdn.net/v_july_v/article/details/7624837/</p>

<p>JerryLead. (2011). 支持向量机SVM（一）. cnblogs. Retrieved from http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html</p>

<p>Lin, H.-T. (2014). Lecture 4: Feasibility of Learning. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 8: Noise and Error. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 6: Theory of Generalization. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 5: Training versus Testing. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 7: The VC Dimension. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 10: Logistic Regression. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2014). Lecture 14: Regularization. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Lin, H.-T. (2015). Lecture 9: Decision Tree. Coursera. Retrieved from https://class.coursera.org/ntumltwo-001/lecture</p>

<p>Nielsen, M. A. (2014). <i>Neural Networks and Deep Learning</i>. Determination Press. Retrieved from http://neuralnetworksanddeeplearning.com</p>

<p>Pomerleau, D. A. (1989). <i>ALVINN, an autonomous land vehicle in a neural network</i>. Carnegie Mellon University. Retrieved from http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&amp;context=compsci</p>

<p>Ng, A. (2014). Explanation of Derivatives Used in Backpropagation. Coursera. Retrieved from https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning</p>

<p>Ng, A. (2014). Logistic Regression. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Regularization: The problem of overfitting. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Neural Networks: Representation. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Neural Networks: Learning. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Programming Exercise 4: Neural Networks Learning. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Linear Regression with multiple variables. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Lin, H.-T. (2014). Lecture 9: Linear Regression. Coursera. Retrieved from https://www.coursera.org/course/ntumlone</p>

<p>Ng, A. (2014). Advice for applying machine learning. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Machine Learning System Design. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Support Vector Machines. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Programming Exercise 6: Support Vector Machines. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Recommender Systems. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Application example: Photo OCR. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Anomaly detection. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Dimensionality Reduction. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Clustering. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>Ng, A. (2014). Large scale machine learning. Coursera. Retrieved from https://www.coursera.org/course/ml</p>

<p>刘鹏. (2014). 广告的基本知识. 云课堂. Retrieved from http://study.163.com/c/ad</p>

<p>威廉·阿伦斯. (2013). <i>当代广告学</i>. 人民邮电出版社.</p>

<p>吴金龙. (2010). <i>Netflix Prize 中的协同过滤算法</i> (PhD thesis). 北京大学.</p>

<p>LeCun, Y. (2014). 深度学习与支持向量机有什么联系？. 52cs. Retrieved from http://www.52cs.org/?p=46</p>

<p>老师木. (2014). SVM神话. 52cs. Retrieved from http://www.52cs.org/?p=359</p>

<p>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. <i>Journal of Machine Learning Research</i>, <i>12</i>, 2825–2830. Retrieved from http://scikit-learn.org/stable/modules/svm.html#unbalanced-problems</p>

<p>Li, F.-F., &amp; Karpathy, A. (2015). Image classification: data-driven approach, nearest neighbor, train/val/test splits. GitHub. Retrieved from http://cs231n.github.io/classification/</p>

<p>Li, F.-F., &amp; Karpathy, A. (2015). Linear classification: Support Vector Machine, Softmax. GitHub. Retrieved from http://cs231n.github.io/linear-classify/</p>

<p>Weston, J., &amp; Watkins, C. (1999). Support Vector Machines for Multi-Class Pattern Recognition. In <i>European Symposium on Artificial Neural Networks</i> (pp. 219–224). Bruges (Belgium). Retrieved from https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf</p>

<p>Rifkin, R., &amp; Klautau, A. (2004). In defense of one-vs-all classification. <i>Journal of Machine Learning Research</i>, <i>5</i>, 101–141.</p>

<p>Tsochantaridis, I., Hofmann, T., Joachims, T., &amp; Altun, Y. (2004). Support Vector Machine Learning for Interdependent and Structured Output Spaces. In <i>Proceedings of the Twenty-first International Conference on Machine Learning</i> (pp. 104–111). New York: ACM. doi:10.1145/1015330.1015341</p>

<p>李航. (2012). <i>统计学习方法</i>. 北京: 清华大学出版社.</p>

<p>Bishop, C. M. (2006). <i>Pattern Recognition and Machine Learning</i>. New York: Springer-Verlag.</p>

<p>达莱尔·哈夫. (2002). <i>统计陷阱</i>. 上海: 上海财经大学出版社.</p>

<p>Loh, W.-Y. (2011). Classification and regression trees. <i>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</i>, <i>1</i>, 14–23. doi:10.1002/widm.8</p>

<p>Rokach, L. (2010). Ensemble-based classifiers. <i>Artificial Intelligence Review</i>, <i>33</i>(1-2), 1–39. doi:10.1007/s10462-009-9124-7</p>

<p>Breiman, L. (1996). Stacked regressions. <i>Machine Learning</i>, <i>24</i>(1), 49–64.</p>

<p>Smyth, P., &amp; Wolpert, D. (1999). Linearly combining density estimators via stacking. <i>Machine Learning</i>, <i>36</i>(1-2), 59–83.</p>

<p>Wolpert, D. H., &amp; Macready, W. G. (1999). An efficient method to estimate bagging’s generalization error. <i>Machine Learning</i>, <i>35</i>(1), 41–55.</p>

<p>Clarke, B. (2003). Comparing Bayes model averaging and stacking when model approximation error cannot be ignored. <i>The Journal of Machine Learning Research</i>, <i>4</i>, 683–712.</p>

<p>Sill, J., Takács, G., Mackey, L., &amp; Lin, D. (2009). Feature-weighted linear stacking. <i>ArXiv Preprint ArXiv:0911.0460</i>.</p>

<p>Breiman, L. (1996). Bagging predictors. <i>Machine Learning</i>, <i>24</i>(2), 123–140.</p>

<p>Breiman, L. (1996). <i>Out-of-bag estimation</i>. Citeseer.</p>

<p>Breiman, L., &amp; Cutler, A. Random Forests. Retrieved from http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</p>

<p>Breiman, L. (2001). Random forests. <i>Machine Learning</i>, <i>45</i>(1), 5–32.</p>

<p>Bylander, T. (2002). Estimating generalization error on two-class datasets using out-of-bag estimates. <i>Machine Learning</i>, <i>48</i>(1-3), 287–297.</p>

<p>Liaw, A., &amp; Wiener, M. (2002). Classification and regression by randomForest. <i>R News</i>, <i>2</i>(3), 18–22.</p>

<p>Agrawal, R., Imieliński, T., &amp; Swami, A. (1993). Mining Association Rules Between Sets of Items in Large Databases. <i>Sigmod Record</i>, <i>22</i>(2), 207–216. doi:10.1145/170036.170072</p>

<p>Bayardo, R. J. (1998). Efficiently Mining Long Patterns from Databases. <i>Sigmod Record</i>, <i>27</i>(2), 85–93. doi:10.1145/276304.276313</p>

<p>Pasquier, N., Bastide, Y., Taouil, R., &amp; Lakhal, L. (1999). Discovering Frequent Closed Itemsets for Association Rules. In <i>International Conference on Database Theory</i> (pp. 398–416). doi:10.1007/3-540-49257-7_25</p>

<p>Han, J., Cheng, H., Xin, D., &amp; Yan, X. (2007). Frequent pattern mining: current status and future directions. <i>Data Mining and Knowledge Discovery</i>, <i>15</i>(1), 55–86. doi:10.1007/s10618-006-0059-1</p>
