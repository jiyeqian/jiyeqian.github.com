<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>CS231n（3）：损失函数与最优化</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />
    <link rel="stylesheet" href="/assets/css/twemoji-awesome.css" />  
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link href="/assets/css/jquery-ui-1.10.4.custom.min.css" rel="stylesheet" />
    <link href="/assets/css/ggvis.css" rel="stylesheet" />
    <link href="/assets/css/mermaid.css" rel="stylesheet" />
    <link rel="stylesheet" href="/assets/css/markdown-plus.css"/> 
    <link rel="stylesheet" href="/assets/css/flexslider.css" type="text/css" media="screen" />
      <style type="text/css">
        .flex-caption {
          width: 96%;
          padding: 2%;
          left: 0;
          bottom: 0;
          background: rgba(0,0,0,.5);
          color: #fff;
          text-shadow: 0 -1px 0 rgba(0,0,0,.3);
          font-size: 14px;
          line-height: 18px;
        }
        li.css a {
          border-radius: 0;
        }
      </style>

    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/jquery-ui-1.10.4.custom.min.js"></script>
    <script type="text/javascript" src="/assets/js/d3.min.js"></script>
    <script type="text/javascript" src="/assets/js/vega.min.js"></script>
    <script type="text/javascript" src="/assets/js/lodash.min.js"></script>
    <script>var lodash = _.noConflict();</script>
    <script type="text/javascript" src="/assets/js/ggvis.js"></script>
    <script type="text/javascript" src="/assets/js/htmlwidgets.js"></script>
    <script type="text/javascript" src="/assets/js/echarts-all.js"></script>
    <script type="text/javascript" src="/assets/js/echarts.js"></script>
    <script defer src="/assets/js/jquery.flexslider-min.js"></script>
    <script type="text/javascript">
      // $(function(){
      //   SyntaxHighlighter.all();
      // });
      $(window).load(function(){
        $('.flexslider').flexslider({
          animation: "slide",
          start: function(slider){
            $('body').removeClass('loading');
          }
        });
      });
    </script>

    <script type="text/javascript">
      function setTimeSpan(){
        var date = new Date();
        timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
      }

      Date.prototype.format = function(format)
      {
        var o =
        {
          "M+" : this.getMonth()+1, //month
          "d+" : this.getDate(),    //day
          "h+" : this.getHours(),   //hour
          "m+" : this.getMinutes(), //minute
          "s+" : this.getSeconds(), //second
          "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
          "S" : this.getMilliseconds() //millisecond
        }
        if(/(y+)/.test(format))
          format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
        for(var k in o)
          if(new RegExp("("+ k +")").test(format))
            format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
          return format;
        }
      </script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>CS231n（3）：损失函数与最优化</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <script type="text/javascript" src="/assets/js/outliner.js"></script>

<section class="meta">
<span class="time">
  <time datetime="2016-03-15">2016-03-15</time>
</span>

 |
<span class="categories">
  <i class="fa fa-share-alt"></i>
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/#CNN" title="CNN">CNN</a>&nbsp;
  
  <a href="/tags/#计算机视觉" title="计算机视觉">计算机视觉</a>&nbsp;
  
  <a href="/tags/#机器学习应用" title="机器学习应用">机器学习应用</a>&nbsp;
  
  <a href="/tags/#支持向量机" title="支持向量机">支持向量机</a>&nbsp;
  
  <a href="/tags/#熵" title="熵">熵</a>&nbsp;
  
  <a href="/tags/#回归" title="回归">回归</a>&nbsp;
  
</span>

</section>
<section class="post">
<p>本文主要参考<em>Convolutional Neural Networks for Visual Recognition</em><a href="#lifeifei_CNN_SVM_2015">[1]</a>课程笔记。</p>

<h2 id="section">预备知识</h2>

<p>k-NN在训练时只是记住了所有的样本，占用空间大，在识别时需和训练集的所有图片比较，耗费时间长。因此，需要开发新的方法克服kNN存在的问题，新方法包含两个关键部分：</p>

<ol>
  <li><strong>评分函数</strong>（score function）：将输入图像映射为所属类别的评分；</li>
  <li><strong>损失函数</strong>（loss function）：度量预测的评分与真实结果之间的一致性，也称为代价（cost）函数或目标（objective）函数。当评分函数输出结果与真实结果之间差异越大，损失函数输出越大。</li>
</ol>

<p>训练时，通过最小化损失函数，得到评分函数的参数；分类时，通过对输入图像所属类别评分，判断图像的类别。</p>

<p>训练集图像$\mathbf x_i\in\mathbb R^D(i=1,2,\ldots,N)$对应的标签$y_i\in 1,2,\ldots,K$，表示训练集有$N$张图像，每个图像表示为一个$D$维向量，图像可分为$K$个类别。对于CIFAR-10数据集，$N=50000,D=32\times 32\times 3,K=10$。评分函数$f:\mathbb R^D\mapsto \mathbb R^K$将图像数据映射为所属类别的评分。</p>

<h2 id="section-1">线性分类器</h2>

<p>线性分类器（linear classifier）就是一个评分函数，它是一个线性映射
\begin{equation}
f(\mathbf x_i,\mathbf W, \mathbf b)=\mathbf W\mathbf x_i+\mathbf b，
\label{multiclass-linear-classifier-1}
\end{equation}
其中，$\mathbf x_i$表示图像展成的$D\times 1$维的向量，$\mathbf W$是$K\times D$维的权值矩阵，$\mathbf b$是$K\times 1$维的偏移向量（bias vector）。对CIFAR-10数据集，该映射输入的输入是3072维向量，输出属于这10个类别的评分。$\mathbf W$的用每行表示一个分类器，因此可以并行计算。一旦得到这些参数$\{\mathbf W,\mathbf b\}$之后，不必像k-NN一样存储整个训练集，只需保存参数即可<sup id="fnref:CNN-f-mapping"><a href="#fn:CNN-f-mapping" class="footnote">1</a></sup>。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg" alt="线性分类器示意" /></a><div class="caption">图 1:  线性分类器示意 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg">JPG</a>]</div></div></div>

<p>上图展示了线性分类器计算过程，将图像简化为4个像素代替，🐶的得分最高导致了🐱误判为🐶，这并不是一个良好的分类器。权值的正负号表示了某个位置的像素对分类结果是赞成还是反对，权值的大小表示强弱程度，从这个角度看就是投票模型。</p>

<p>线性分类器可以理解为模版匹配（template matching），$\mathbf W$的每一行对应一个模版，通过图像与模版的内积（inner product）<sup id="fnref:inner-product-similarity"><a href="#fn:inner-product-similarity" class="footnote">2</a></sup>判断图像与哪个模版最相似。从这个角度看，线性分类器也是最近邻分类器，并且只需和每类学习到的唯一模版比较，效率更高，匹配时采用内积而非$L_1$或$L_2$度量距离。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg" alt="线性分类器权值矩阵图像化" /></a><div class="caption">图 2:  线性分类器权值矩阵图像化 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg">JPG</a>]</div></div></div>

<p>上图用图像的方式展示了线性分类器在CIFAR-10上学到的权值矩阵，这些图像相当于从每类学习到的模版。</p>

<p>若将图像追加一个值为1的“特征”，可将偏移向量$\mathbf b$合并到权值矩阵$\mathbf W$中，公式\eqref{multiclass-linear-classifier-1}可改写为更简练的形式
\begin{equation}
f(\mathbf x_i,\mathbf W)=\mathbf W\mathbf x_i。
\label{multiclass-linear-classifier-2}
\end{equation}</p>

<p>在机器学习中，将输入特征归一化是常用的技术。在实际中，将图像的每维特征（像素）减去均值中心化，对图像而言就是将每张图像减去平均图像，得到新图像的像素取值范围是$[-127,127]$。还可进一步将特征取值规范化到$[-1,1]$区间。</p>

<p>在线性分类器（评分函数）基础上，通过定义不同损失函数，可得到具备不同特性的多分类支持向量机和softmax分类器。</p>

<h2 id="multiclass-SVM">多分类支持向量机</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg" alt="损失函数示意图" /></a><div class="caption">图 3:  损失函数示意图 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg">JPG</a>]</div></div></div>

<p>支持向量机期望评分函数在固定边界$\Delta$控制下，正确分类比错误分类输出更高的评分。$f(\mathbf x_i,\mathbf W)_j$表示图像$\mathbf x_i$属于类别$j$的评分，多分类支持向量机的损失函数定义为
\begin{equation}
L_i = \sum_{j\neq y_i}\max(0, f(\mathbf x_i,\mathbf W)_j-f(\mathbf x_i,\mathbf W)_{y_i}+\Delta)，
\label{eq:m-svm-loss}
\end{equation}
损失函数期望正确分类比错误分类输出更小的值。支持向量机期望正确分类比错误分类至少多得分$\Delta$，这样就没有损失，如上图所示，如果任何类别的评分落在红色区间，就会有损失累加。作出好的预测等价于最小化损失。</p>

<p>$\max(0,\cdots)$一般称为<strong>hinge损失</strong>，也称为<strong>最大边界损失</strong>（max-margin loss），$L_2$-SVM的平方损失$\max(0,\cdots)^2$惩罚力度更强。hinge损失比平方hinge损失更标准通用，但有的数据集平方hinge损失效果更好，可以通过交叉验证选择损失度量方式。</p>

<p>假设$f(\mathbf x_i,\mathbf W)＝[13, -7, 11], y_i=0,\Delta=10$，那么
\[
L_i=\max(0, -7-13+10)+\max(0,11-13+10)=8。
\]
上式右边第1项，正确分类得分（13）要高于错误分类（-7），支持向量机只关心是否二者差异至少为10（事实上二者差距为20分），因此输出为0；上式右边第2项，正确分类得分（13）高于错误分类（11），但二者差异只有2，因此损失了8。</p>

<p>将\eqref{multiclass-linear-classifier-2}代入\eqref{eq:m-svm-loss}可得
\begin{equation}
L_i = \sum_{j\neq y_i}\max\left(0, \mathbf w_j^T\mathbf x_i - \mathbf w_{y_i}^T\mathbf x_i +\Delta\right)，
\end{equation}
其中，$\mathbf w_j^T$是$\mathbf W$的第$j$行元素。</p>

<p>学习（训练）阶段的目标就是期望得到权值矩阵$\mathbf W$，使得正确分类的评分高于其它所有类别，并且使损失函数尽量小。</p>

<p>假设$\mathbf W$将一个数据集的每个样本都正确分类（对所有的$i$，$L_i=0$），$\mathbf W$却不是唯一的存在，当$\lambda&gt;0$时$\lambda\mathbf W$都能胜任。通过正则化惩罚（regularization penalty）$R(\mathbf W)$，可以消除$\mathbf W$的歧义。常用的正则化惩罚抑制$L_2$范数大的$\mathbf W$出现，
\[
R(\mathbf W)=\sum_k\sum_l w_{k,l}^2，
\]
正则化只针对权值与输入数据无关。因此，多分类支持向量机的损失函数包括两部分，数据损失（data loss）和正则化损失（regularization loss），
\begin{equation}
L={1\over N}\sum_i\sum_{j\neq y_i}\max\left(0, \mathbf w_j^T\mathbf x_i - \mathbf w_{y_i}^T\mathbf x_i +\Delta\right)+\lambda \sum_k\sum_lw_{k,l}^2。
\label{eq:regularization-loss-function}
\end{equation}</p>

<ol>
  <li>损失函数\eqref{eq:regularization-loss-function}采用\eqref{multiclass-linear-classifier-1}的权值$\mathbf w_j$包含偏移分量$b_j$，但正则化采用\eqref{multiclass-linear-classifier-2}的权值$\mathbf W$不包含偏移向量$\mathbf b$，实际应用中即使正则化了$\mathbf b$对结果影响也不大。</li>
  <li>由于增加了正则化项，正常情况下损失函数不可能到达0。</li>
  <li>控制正则化力度的参数$\lambda$仍然只能通过交叉验证选择。</li>
  <li>采用$L_2$正则化，可以导出<a href="\sum\_k\sum\_l\mathbf W\_{k,l}^2">支持向量机的最大边界特性</a>。</li>
</ol>

<p>正则化使得没有哪一个维度可以对评分造成很大的影响，通过惩罚大的权值提高了泛化（generalization）性能，降低了过拟合（overfitting）风险。当$\mathbf x=[1,1,1,1]^T$、$\mathbf w_1=[1,0,0,0]^T$和$\mathbf w_2=[0.25,0.25,0.25,0.25]^T$时，虽然$\mathbf w_1^T\mathbf x=\mathbf w_2^T\mathbf x=1$，但$L_2$对$\mathbf w_1$的惩罚是1对$\mathbf w_2$的惩罚只有0.25，因此结果偏爱$\mathbf w_2$。$L_2$惩罚偏爱取值小且分散的权值向量，最终得到的分类器会尽量考虑所有维度的输入，而非更偏爱哪一个维度<sup id="fnref:the-golden-mean"><a href="#fn:the-golden-mean" class="footnote">3</a></sup>。</p>

<p>$\Delta$和$\lambda$虽是两个不同的参数，但都是相同的折中效果。不需交叉验证选择$\Delta$，直接设置$\Delta=1.0$即可，只需通过交叉验证调节$\lambda$。放大$\mathbf W$可以放大评分，缩小$\mathbf W$也可缩小评分
，度量评分之间的差距的$\Delta$也同步放大或缩小，而调节$\lambda$可放缩$\mathbf W$，因此固定住$\Delta$只设置$\lambda$即可。</p>

<p>对于二分类支持向量机，<a href="/2015/01/kernel-logistic-regression/#mjx-eqn-equniform-soft-margin-svm">损失函数</a>表示为
\[
L=C\sum_{i=1}^N\max\left(0, 1-y_i\mathbf w^T\mathbf x_i\right)+R(\mathbf w)，
\]
其中$C$是超参数，$y_i\in\{-1,+1\}$。它可视为本文多分类问题简化为二分类的一个特例，$C$和$\lambda$都有相同的折中作用，$C\propto{1\over \lambda}$。</p>

<p>本文的多分类支持向量机，只是支持向量机解决多分类问题的方法之一。除此之外还有一对多（OVA，one-vs-all）策略和实际中很少使用的多对多（AVA，all-vs-all）。简单的OVA策略能很好处理多分类问题<a href="#Rifkin04indefense">[2]</a>，本文的方法是OVA的增强版<a href="#Weston99supportvector">[3]</a>，理论上数据损失项可达0，常规的OVA方法则不行。</p>

<h2 id="softmax-classifier">softmax分类器</h2>

<p>softmax分类器是logistic回归分类器在多分类问题的推广。$f_j(\mathbf z)={e^{z_j}\over\sum_k e^{z_k}}$被称为softmax函数（softmax function），它将实数转换为$[0,1]$区间的值。softmax分类器采用同样的评分函数\eqref{multiclass-linear-classifier-2}，但采用<strong>交叉熵损失</strong>（cross-entropy loss）函数
\begin{equation}
L
=-{1\over N}\sum_{i=1}^N\log\left({e^{f_{y_i}}\over\sum_j e^{f_j}}\right)+R(\mathbf W)
=-{1\over N}\sum_{i=1}^N\left(f_{y_i}-\log\sum_j e^{f_j}\right)+R(\mathbf W)。
\end{equation}</p>

<p>“真实”分布$p$和估计所得分布$q$之间的交叉熵定义为
\[
H(p,q)=-\sum_xp(x)\log q(x)，
\]
交叉熵也可记为熵与<a href="http://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler散度</a>（divergence）之和的形式
\[
H(p,q)=H(p)+D_{KL}(p\| q)，
\]
其中熵$H(p)=-\sum_xp(x)\log p(x)$。对softmax分类器，属于类别$y_i$的概率估值$q={e^{f_{y_i}}\over\sum_j e^{f_j}}$，属于类别$y_i$的“真实”概率$p=1$，因此对本文分类问题$H(p)=0$。最小化代价函数等价于最小化两个分布之间的KL散度。通过交叉熵，期望能预测的分布能在正确分类的类别上概率达到最大。</p>

<p>softmax函数可以视为概率形式
\[
P(y_i|\mathbf x_i;\mathbf W)={e^{f_{y_i}}\over\sum_j e^{f_j}}，
\]
表示预测正确分类$y_i$的归一化概率。好的分类结果是$\prod_{i=1}^NP(y_i|\mathbf x_i;\mathbf W)$最大，这可认为是最大似然估计（MLE，maximum likelihood estimation）。正则化项$R(\mathbf W)$可认为源自权值矩阵的高斯先验（Gaussian prior），最小化损失函数相当于最大后验概率估计（maximum a posteriori）。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计<a href="#lihang_sml_2012">[4, P. 9]</a>。</p>

<p>编程实现时，$e^{f_{y_i}}$和$\sum_j e^{f_j}$的值可能很大，大数值相除不稳定，所以需要采用规范化技巧（normalization trick）。softmax函数可以写为等价的形式
\[
\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}=\frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}=\frac{e^{f_{y_i}+\log C}}{\sum_j e^{f_j+\log C}}，
\]
提高计算的稳定性，通常选择$\log C=-\max_jf_j$，将评分的最大值规范化为0。</p>

<h2 id="svm-vs-softmax">SVM vs. softmax</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png" alt="SVM和softmax的对比" /></a><div class="caption">图 4:  SVM和softmax的对比 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png">PNG</a>]</div></div></div>

<p>支持向量机的结果不易理解，softmax分类器给出了所属类别的“概率”解释。事实上，概率分布是平坦还是尖峰依赖于正则化参数$\lambda$。</p>

<p>当线性分类器输出$[1, -2, 0]$时，softmax函数输出“概率”
\[
[1, -2, 0]\rightarrow 
[e^1, e^{-2}, e^0]=
[2.71,0.14,1]\rightarrow
[0.7,0.04,0.26]。
\]
当增大正则化系数$\lambda$，这可能导致更小的权值，线性分类器的输出假设减小到$[0.5, -1, 0]$，此时softmax函数输出“概率”
\[
[0.5, -1, 0]\rightarrow 
[e^{0.5}, e^{-1}, e^0]=
[1.65,0.37,1]\rightarrow
[0.55,0.12,0.33]，
\]
概率分布变得更均匀。大的正则化系数$\lambda$导致权值变小，类别的输出概率更趋于均匀化（uniform）。因此，softmax函数输出的“概率”之间大小比较有意义，单独看某个“概率”值的大小没有明确意义，也就是相对大小有意义，绝对大小没意义。</p>

<p>多分类支持向量机和softmax分类器，在性能表现上通常很相近。假设$\mathbf x_i$属于第1类，对于$[10, -100, -100]$和$[10, 9, 9]$，多分类支持向量机都不再增加损失函数的值；但对于softmax分类器，$[10, 9, 9]$会比$[10, -100, -100]$让损失函数增加更大的值。softmax分类器永不满足，总是试图让正确的分类概率更大，损失函数的值更小；然而，只要多分类支持向量机满足边界条件，不论评分高低，不再调整评分，这可以看作是多分类支持向量机的一个特性，比如对于一个车辆分类器，它应该关注如何分类轿车和卡车等困难问题，而不应受已经评分很低🐸🐱🐶等的影响。</p>

<h2 id="section-2">参考文献</h2>

<ol class="bibliography"><li><span id="lifeifei_CNN_SVM_2015">[1]F.-F. Li and A. Karpathy, “Linear classification: Support Vector Machine, Softmax.” GitHub, 2015.</span>

[<a href="http://cs231n.github.io/linear-classify/">Online</a>]

</li>
<li><span id="Rifkin04indefense">[2]R. Rifkin and A. Klautau, “In defense of one-vs-all classification,” <i>Journal of Machine Learning Research</i>, vol. 5, pp. 101–141, 2004.</span>

</li>
<li><span id="Weston99supportvector">[3]J. Weston and C. Watkins, “Support Vector Machines for Multi-Class Pattern Recognition,” in <i>European Symposium on Artificial Neural Networks</i>, Bruges (Belgium), 1999, pp. 219–224.</span>

[<a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf">Online</a>]

</li>
<li><span id="lihang_sml_2012">[4]李航, <i>统计学习方法</i>. 北京: 清华大学出版社, 2012.</span>

</li></ol>

<h3 id="section-3">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:CNN-f-mapping">
      <p>CNN也会将数据像素映射为得分，只是映射$f$会更复杂，参数更多。 <a href="#fnref:CNN-f-mapping" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:inner-product-similarity">
      <p>内积就是余弦相似度？ <a href="#fnref:inner-product-similarity" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:the-golden-mean">
      <p>多么美妙的中庸之道！ <a href="#fnref:the-golden-mean" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="left">
<p></p>
<hr>
  <p><img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px"></p>
  <p></p>
<hr>
  <ul>
    
    <li class="pageNav">2016-09-28 &raquo; <a href="/2016/09/pgm_i_01_introduction_and_overview">PGM I（01）：引言与概述</a></li>
    
    <li class="pageNav">2016-09-24 &raquo; <a href="/2016/09/feasibility-about-home-camera-in-power-syetem-monitoring">家用监控设备用于电网的可行性分析</a></li>
    
    <li class="pageNav">2016-09-19 &raquo; <a href="/2016/09/feasibility-about-uav-in-cable-tunnel">无人机电缆隧道巡检可行性调研报告</a></li>
    
    <li class="pageNav">2016-09-18 &raquo; <a href="/2016/09/S13000-Introduction">鲁棒及自适应控制（1）：概论</a></li>
    
    <li class="pageNav">2016-09-13 &raquo; <a href="/2016/09/LeNet-5-Lecun">Gradient-Based Learning Applied to Document Recognition</a></li>
    
    <li class="pageNav">2016-03-14 &raquo; <a href="/2016/03/cs231n_image-classification-pipeline">CS231n（2）：图像分类流程</a></li>
    
    <li class="pageNav">2016-02-29 &raquo; <a href="/2016/02/cs231n_introduction-to-computer-vision">CS231n（1）：计算机视觉简介</a></li>
    
    <li class="pageNav">2015-10-13 &raquo; <a href="/2015/10/minimum-cut-based-inference">DILinAV（4）：基于最小割的推理</a></li>
    
  </ul>
<p></p>
<span>
  <a  href="/2016/03/cs231n_image-classification-pipeline" class="pageNav" style="float:left"   >上一篇：CS231n（2）：图像分类流程 </a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2016/09/LeNet-5-Lecun" class="pageNav" style="float:right"   >下一篇：Gradient-Based Learning Applied to Document Recognition </a>  
</span>
</section>

	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<ul class="ds-recent-visitors" data-num-items="16"></ul>
	<div class="ds-thread"  data-thread-key="/2016/03/cs231n_loss-functions-and-optimization" 	data-url="http://qianjiye.de/2016/03/cs231n_loss-functions-and-optimization" data-title="CS231n（3）：损失函数与最优化">
	</div>	


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2016/03/cs231n_image-classification-pipeline'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2016/09/LeNet-5-Lecun'; --1> -->
<!--         url = '/2016/09/LeNet-5-Lecun'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2016/03/cs231n_loss-functions-and-optimization/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2016 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
