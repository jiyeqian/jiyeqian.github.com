<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>图像分类（1）：基于k最近邻算法的简介</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>
  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>图像分类（1）：基于k最近邻算法的简介</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2015-01-19">2015-01-19</time>
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#计算机视觉" title="计算机视觉">计算机视觉</a>&nbsp;
  
  <a href="/tags/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
  <a href="/tags/#机器学习应用" title="机器学习应用">机器学习应用</a>&nbsp;
  
</span>

</section>
<section class="post">
<p>本文主要参考<em>Convolutional Neural Networks for Visual Recognition</em><a href="#lifeifei_CNN_kNN_2015">[1]</a>课程笔记。</p>

<h2 id="section">简介</h2>

<p>图像分类的任务是根据已知的确定标签集，为输入图像分配一个标签（标签就是所谓的类别）。其它一些计算机视觉问题，比如目标提取、分割等，都可以被归结到图像分类。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png" alt="图像分类示例" /></a><div class="caption">图 1:  图像分类示例 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png">PNG</a>]</div></div></div>

<p>上图展示一个图像分类模型，通过计算属于4个标签｛🐱，🐶，🎩，🍵｝的概率，为图像分配最大概率对应的标签。彩色图像用3维矩阵表示，本例中宽248像素，高400像素的图像，用248×400×3的矩阵表示。矩阵的每个元素对应一个像素值，取值是0到255的整数。具体来说，图像分类的任务是通过这些像素值，利用计算机视觉算法，得到类别标签（本例输入图片的类别标签是🐱）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg" alt="图像分类面临的主要挑战" /></a><div class="caption">图 2:  图像分类面临的主要挑战 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg">JPEG</a>]</div></div></div>

<p>计算机视觉算法进行图像分类面临的主要挑战包括：视角变化（viewpoint variation）、尺度变化（scale variation）、形变（deformation）、光照影响（illumination conditions）、背景混杂（background clutter）、类内变化（intra-class variation）等，如上图所示。好的图像分类模型应当能应对这些挑战。</p>

<p>图像分类算法与传统的计算机算法（比如排序）开发不同。首先需要给计算输入供包含每类若干示例图像的<a href="http://cs231n.github.io/assets/trainset.jpg">训练集</a>；然后开发学习算法从样本集中的每类学习；最后通过预测结果评估算法性能。这种依赖已标注样本集的方法称为<strong>数据驱动的方法</strong>（data-driven approach）。</p>

<p>图像分类算法开发的流程如下：</p>

<ol>
  <li>输入：输入包含$N$张图像，已经用K个标签之一标注了每张图像，这称为训练集（training set）。</li>
  <li>学习：开发学习算法，通过训练集学习每类的样子，这称为训练分类器（training classifier）或学习模型（learning a model）。</li>
  <li>评估：输入分类算法没有学习过的图片，通过算法预测标签，根据预测和真实结果的对比评估算法的效果。</li>
</ol>

<h2 id="section-1">最近邻分类器</h2>

<p>最近邻分类器（nearest neighbor classifier）容易实现，本文通过它介绍图像分类的基本方法流程，但是在实际应用中很少使用该方法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg" alt="［左］：CIFAR-10示例图像；［右］：与第1列最相邻的10张图片" /></a><div class="caption">图 3:  ［左］：CIFAR-10示例图像；［右］：与第1列最相邻的10张图片 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg">JPG</a>]</div></div></div>

<p>图像数据集采用<a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>，它包含尺寸32×32的60000张小图，每张图片已经被｛✈️，🚗，🐦，🐱，……｝等10个标签之一标注，如上图左所示。60000张图片被分割成50000张图片（每类5000张）的训练集和10000张图片（每类1000张）的测试集。</p>

<p>最近邻分类器训练分类器的方法，就是记住训练集即可。在预测的时候直接和训练集中的每张图片比较，得到输入图像与训练集中最相邻那张图片的标签，将该标签作为预测结果。上图右是最近邻分类器的结果，其中第8行，与第1列🐎最相邻的是🚗，🐎就会被误标记为🚗。</p>

<p>最近邻算法度量两张图片的相邻程度，通常采用像素值之间的$L_1$距离或$L_2$距离：
\[
d_1(\mathbf I_1,\mathbf I_2)=\sum_p\left\lvert\mathbf I_1^p-\mathbf I_2^p\right\rvert；\qquad d_2(\mathbf I_1,\mathbf I_2)=\sqrt{\sum_p\left(\mathbf I_1^p-\mathbf I_2^p\right)^2}。
\]
在CIFAR-10数据集上，用$L_1$距离得到的分类正确率大约是38.6%，$L_2$距离得到的分类正确率大约是35.4%，高于随机猜想10%的精度，目前最先进的卷积神经网络（CNN，convolutional neural networks）<a href="http://www.kaggle.com/c/cifar-10/leaderboard">正确率在95%以上</a><sup id="fnref:what-is-kaggle-score"><a href="#fn:what-is-kaggle-score" class="footnote">1</a></sup>。</p>

<p>在度量两个向量差异时，$L_2$的效果比$L_1$糟糕（That is, the $L_2$ distance prefers many medium disagreements to one big one. ）。$L_1$和$L_2$是常用的两个<a href="http://planetmath.org/vectorpnorm">p范数</a>特例。</p>

<h2 id="kNN">k最近邻分类器</h2>

<p>k最近邻分类器（k-NN，k-nearest neighbor classifier）是对最近邻分类器的简单扩展：从训练集中选出与输入图像最相邻的k张图像的类别标签，通过这k个标签对输入图像的类别标签进行投票。k=1时，k最近邻分类器和最近邻分类器等价。直观上理解，k最近邻分类器取大的k值，对判别边界有平滑作用，能更好的抗击噪声干扰。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg" alt="k最近邻分类器的分类效果" /></a><div class="caption">图 4:  k最近邻分类器的分类效果 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg">JPEG</a>]</div></div></div>

<p>上图的k最近邻分类器采用的是$L_2$距离。上图中可以看到，蓝色区域中的小块绿色孤岛是由于噪声点干扰导致的，这将导致预测错误；上图右的5-NN不受这些异常值的干扰，能在测试集上取得跟好的泛化（generalization）效果。上图右的灰色表示有争议的区域，在这些区域至少2个类别标签都得到了最高票。</p>

<h2 id="section-2">交叉验证</h2>

<p>在实际应用中，如何选择kNN的参数k呢？除k之外，还有度量距离的$L_1$和$L_2$等其它参数需要调节。这些候选参数称为<strong>超参数</strong>（hyperparameters）。在基于数据驱动的机器学习算法中，参数选择非常普遍。</p>

<p>在实际应用中，不能采用测试集选择参数。如果采用测试集调整参数，分类器就可能对测试集过拟合（overfit），当最终发布到应用环境，分类器的性能可能大打折扣。用测试集调整参数，相当于把测试集当训练集使用。测试集只应该在最终测试分类器泛化性能时，被使用1次。</p>

<p>合适的做法是从训练集分割出一个较小的子集，作为验证集（validation set）。对CIFAR-10数据，可以用49,000个图像作为训练集，利用剩下了1,000个图像作为验证集进行参数调节。在选择参数k的时候，在验证集上测试每个k模型的性能，从中选择使性能达到最好的k。选定k之后，在测试集上仅进行一次性能评估。</p>

<p>当训练集合测试集较小的时候，可以采用更聪明的<a href="/2015/01/validation/#v-fold-cross-validation"><strong>交叉验证</strong></a>（cross-validation）进行参数选择。通过评估不同验证集上的平均性能，选择合适的参数。以5-fold的交叉验证为例：</p>

<ol>
  <li>将训练集分割为5等份；</li>
  <li>选择1份作为验证集，剩余的4份组成训练集，在验证集上评估参数的性能；</li>
  <li>轮流将5份作为训练集，将5次性能的平均作为最终评估结果。</li>
</ol>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png" alt="5-fold交叉验证选择参数k的正确率曲线" /></a><div class="caption">图 5:  5-fold交叉验证选择参数k的正确率曲线 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png">PNG</a>]</div></div></div>

<p>上图展示了5-fold交叉验证的效果，k=7是个不错的参数。若分割的等份大于5，上图的曲线将变得更加光滑。</p>

<p>在实际应用中，由于交叉验证计算量很大，因而会选择采用单一验证集而避免采用交叉验证。通常用50%到90%的数据作为训练集，剩下的作为验证集，候选超参数集越大，验证集越大。当验证集很小时（比如仅仅几百个数据），采用交叉验证是比较保险的方法，它能减少参数选择时的噪声干扰，常用的有3-fole、5-fold、10-fold交叉验证。</p>

<p id="is-validation-set-need">另一个需要考虑的问题是，在通过验证集确定了最佳参数后，是否需要利用整个训练集和最佳参数重新学习。由于放回了验证集，整个训练集上的表现必然有所不同。实际上，最终发布的分类器不需要验证集的参与。</p>

<h2 id="section-3">评价与建议</h2>

<p>k最近邻算法的主要优点是容易理解和实现，并且训练不时耗；主要缺点是测试（预测）时耗高。在实际应用中，主要关注的是测试（预测）时耗。深度神经网络（DNN，deep neural networks）相反，训练耗时但预测高效，在实际中更适用。</p>

<p>最近邻算法也是一个活跃的研究领域。近似最近邻算法（ANN，approximate nearest neighbor）通过对精度和速度（空间）耗费的折中提高效率，比如<a href="http://www.cs.ubc.ca/research/flann/">FLANN</a>。这些算法通常需要通过kd树或k均值算法预处理或建立索引。</p>

<p>k最近邻算法有时是不错的选择，尤其是数据维数较低的时候，但在图像分类中几乎很少使用。图像是高维数据，高维空间的距离度量表现有些反直觉（counter-intuitive）。</p>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png" alt="" /></a><div class="caption">图 6:   [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png">PNG</a>]</div></div></div>

<p>利用$L_2$距离度量上图中小图的相似性，结果违反直觉。其它图都是最左图变换得到的，人眼观察这些图比较相似，但是基于$L_2$的度量表明其它图和原图差异很大。实际山，像素级别的距离度量无法判断基于直觉和语义的相似性。</p>

<div class="image_line" id="figure-7"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg" alt="利用t-SNE展示CIFAR-10图像" /></a><div class="caption">图 7:  利用t-SNE展示CIFAR-10图像 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg">JPG</a>]</div></div></div>

<p>上图用<a href="http://lvdmaaten.github.io/tsne/">t-SNE</a>展示CIFAR-10的图像，在像素级的$L_2$距离度量下，相似的图像相邻排列。结果表明并所非所期望的那样，同类别的相邻排列。实际上，这种度量严重受背景和颜色分布的影响，并非真正度量图像内容的相似性。</p>

<p>k最近邻算法在实际使用中的建议：</p>

<ol>
  <li>数据预处理：特征向量均值归0化、方差单位化<sup id="fnref:why-not-here"><a href="#fn:why-not-here" class="footnote">2</a></sup>；</li>
  <li>对高维数据用<a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">PCA</a>或<a href="http://scikit-learn.org/stable/modules/random_projection.html">随机投影</a>（random projections）等算法降维处理；</li>
  <li>根据前文建议采用验证（50%到90%数据作为训练集）或交叉验证，通常fold数越多，效果越好，但也越耗时；</li>
  <li>通过验证集选择k（候选k越多越好）和距离度量方式；</li>
  <li>如果算法太耗时，尝试采用ANN加速。</li>
</ol>

<h2 id="section-4">示例代码</h2>

<p>下文中代码所需要的函数和数据<a href="http://vision.stanford.edu/teaching/cs231n/assignment1.zip">在这里获取</a>。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">__author__</span> <span class="o">=</span> <span class="s">&#39;jiyeqian&#39;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cs231n.data_utils</span> <span class="kn">import</span> <span class="n">load_CIFAR10</span>

<span class="c"># a magic function we provide</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">load_CIFAR10</span><span class="p">(</span><span class="s">&#39;cs231n/datasets/cifar-10-batches-py&#39;</span><span class="p">)</span>
<span class="c"># Subsample the data for more efficient code execution</span>
<span class="n">num_training</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training</span><span class="p">)</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">Xte</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">Yte</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="c"># flatten out all images to be one-dimensional</span>
<span class="n">Xtr_rows</span> <span class="o">=</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c"># Xtr_rows becomes 5000 x 3072</span>
<span class="n">Xte_rows</span> <span class="o">=</span> <span class="n">Xte</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xte</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c"># Xte_rows becomes 500 x 3072</span>

<span class="k">class</span> <span class="nc">NearestNeighbor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span>
        <span class="c"># the nearest neighbor classifier simply remembers all the training data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span>
        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c"># lets make sure that the output type matches the input type</span>
        <span class="n">Ypred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c"># loop over all test rows</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
            <span class="c"># find the nearest training image to the i&#39;th test image</span>
            <span class="c"># using the L1 distance (sum of absolute value differences)</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c"># L2 distance</span>
            <span class="c"># distances = np.linalg.norm(self.Xtr - X[i,:], axis = 1)</span>
            <span class="c"># min_index = np.argmin(distances) # get the index with smallest distance</span>
            <span class="c"># Ypred[i] = self.ytr[min_index] # predict the label of the nearest example</span>
            <span class="n">fre_idx</span><span class="p">,</span> <span class="n">fre_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="p">[</span><span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]],</span> \
                                         <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">Ypred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">fre_idx</span><span class="p">[</span><span class="n">fre_num</span> <span class="o">==</span> <span class="n">fre_num</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">Ypred</span>


<span class="c"># Part 1: kNN prediction</span>

<span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span> <span class="c"># create a Nearest Neighbor classifier class</span>
<span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span> <span class="c"># train the classifier on the training images and labels</span>
<span class="n">Yte_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte_rows</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># predict labels on the test images</span>
<span class="c"># and now print the classification accuracy, which is the average number</span>
<span class="c"># of examples that are correctly predicted (i.e. label matches)</span>
<span class="k">print</span> <span class="s">&#39;accuracy: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yte_predict</span> <span class="o">==</span> <span class="n">Yte</span><span class="p">)</span> <span class="p">)</span>


<span class="c"># Part 2: validation for choosing k</span>

<span class="c"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span>
<span class="c"># recall Xtr_rows is 5,000 x 3072 matrix</span>
<span class="n">Xval_rows</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Ytr</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="c"># take first 1000 for validation</span>
<span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[</span><span class="mi">1000</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">Ytr</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span> <span class="c"># keep last 4,000 for train</span>

<span class="c"># find hyperparameters that work best on the validation set</span>
<span class="n">validation_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>

    <span class="c"># use a particular value of k and evaluation on validation data</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span>
    <span class="c"># here we assume a modified NearestNeighbor class that can take a k as input</span>
    <span class="n">Yval_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xval_rows</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yval_predict</span> <span class="o">==</span> <span class="n">Yval</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&#39;accuracy: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">,)</span>

    <span class="c"># keep track of what works on the validation set</span>
    <span class="n">validation_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span></code></pre></div>

<p>k最近邻算法预测阶段计算复杂度非常高，为了加快计算速度，上述代码只抽取了10%的数据，得到的结果如下：</p>

<div class="highlight"><pre><code># Part 1:
accuracy: 0.290000
# Part 2:
accuracy: 0.291000
accuracy: 0.269000
accuracy: 0.275000
accuracy: 0.289000
accuracy: 0.287000
accuracy: 0.285000
accuracy: 0.283000
</code></pre></div>

<h2 id="section-5">参考文献</h2>

<ol class="bibliography"><li><span id="lifeifei_CNN_kNN_2015">[1]F.-F. Li and A. Karpathy, “Image classification: data-driven approach, nearest neighbor, train/val/test splits.” GitHub, 2015.</span>

[<a href="http://cs231n.github.io/classification/">Online</a>]

</li></ol>

<h3 id="section-6">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:what-is-kaggle-score">
      <p>kaggle排名的得分（score）是如何计算的？ <a href="#fnref:what-is-kaggle-score" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-here">
      <p>We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization. <a href="#fnref:why-not-here" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="right">
<br/>
<span>
  <a  href="/2015/01/logistic-regression" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/01/image-classification-svm-and-softmax-based-linear-classifier" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2015/01/image-classification-knn-based-introduction" data-url="http://qianjiye.de/2015/01/image-classification-knn-based-introduction" data-title="图像分类（1）：基于k最近邻算法的简介">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/01/logistic-regression'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/01/image-classification-svm-and-softmax-based-linear-classifier'; --1> -->
<!--         url = '/2015/01/image-classification-svm-and-softmax-based-linear-classifier'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/01/image-classification-knn-based-introduction/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
