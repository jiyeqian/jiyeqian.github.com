<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>logistic回归</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />
    <link rel="stylesheet" href="/assets/css/twemoji-awesome.css" />  
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link href="/assets/css/jquery-ui-1.10.4.custom.min.css" rel="stylesheet" />
    <link href="/assets/css/ggvis.css" rel="stylesheet" />
    <link href="/assets/css/mermaid.css" rel="stylesheet" />
    <link rel="stylesheet" href="/assets/css/markdown-plus.css"/> 
    <link rel="stylesheet" href="/assets/css/flexslider.css" type="text/css" media="screen" />
      <style type="text/css">
        .flex-caption {
          width: 96%;
          padding: 2%;
          left: 0;
          bottom: 0;
          background: rgba(0,0,0,.5);
          color: #fff;
          text-shadow: 0 -1px 0 rgba(0,0,0,.3);
          font-size: 14px;
          line-height: 18px;
        }
        li.css a {
          border-radius: 0;
        }
      </style>

    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/jquery-ui-1.10.4.custom.min.js"></script>
    <script type="text/javascript" src="/assets/js/d3.min.js"></script>
    <script type="text/javascript" src="/assets/js/vega.min.js"></script>
    <script type="text/javascript" src="/assets/js/lodash.min.js"></script>
    <script>var lodash = _.noConflict();</script>
    <script type="text/javascript" src="/assets/js/ggvis.js"></script>
    <script type="text/javascript" src="/assets/js/htmlwidgets.js"></script>
    <script type="text/javascript" src="/assets/js/echarts-all.js"></script>
    <script type="text/javascript" src="/assets/js/echarts.js"></script>
    <script defer src="/assets/js/jquery.flexslider-min.js"></script>
    <script type="text/javascript">
      // $(function(){
      //   SyntaxHighlighter.all();
      // });
      $(window).load(function(){
        $('.flexslider').flexslider({
          animation: "slide",
          start: function(slider){
            $('body').removeClass('loading');
          }
        });
      });
    </script>

    <script type="text/javascript">
      function setTimeSpan(){
        var date = new Date();
        timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
      }

      Date.prototype.format = function(format)
      {
        var o =
        {
          "M+" : this.getMonth()+1, //month
          "d+" : this.getDate(),    //day
          "h+" : this.getHours(),   //hour
          "m+" : this.getMinutes(), //minute
          "s+" : this.getSeconds(), //second
          "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
          "S" : this.getMilliseconds() //millisecond
        }
        if(/(y+)/.test(format))
          format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
        for(var k in o)
          if(new RegExp("("+ k +")").test(format))
            format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
          return format;
        }
      </script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>logistic回归</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <script type="text/javascript" src="/assets/js/outliner.js"></script>

<section class="meta">
<span class="time">
  <time datetime="2015-01-17">2015-01-17</time>
</span>

 |
<span class="categories">
  <i class="fa fa-share-alt"></i>
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#回归" title="回归">回归</a>&nbsp;
  
  <a href="/tags/#梯度下降法" title="梯度下降法">梯度下降法</a>&nbsp;
  
  <a href="/tags/#熵" title="熵">熵</a>&nbsp;
  
</span>

</section>
<section class="post">
<p>本节的主要参考资料是机器学习基石<a href="#lin_ml_logistic_regression_2014">[1]</a>和机器学习<a href="#ng_ml_lr_2014">[2]</a>网络课程。</p>

<h2 id="soft">soft二分类</h2>

<p>soft二分类感兴趣的不仅仅是$\{-1,+1\}$类别的判断，而是属于某个类别的可能性，比如
\begin{equation}
f(\mathbf x)=P(+1|\mathbf x)\in [0,1]。
\label{eq:f-vs-probility}
\end{equation}
但是在实际应用中，难以获取属于某个类别的概率，只能得到是否属于某个类别，也就是与二分类一样的数据集。</p>

<p>不妨将$\{-1,+1\}$类别标签的数据，看作是概率标签数据被噪声污染的结果，e.g. $(\mathbf x_1,y’_1=0.9=P(+1|\mathbf x_1))+{noise}\rightarrow(\mathbf x_1,y_1=\circ\sim P(+1|\mathbf x_1))$。</p>

<p>logistic回归解决的问题，就是在$\{-1,+1\}$类别标签的训练集上，通过soft二分类产生概率标签的输出。</p>

<h2 id="logistic">logistic回归模型</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-logistic-function.png"><img src="/assets/images/2015-01-17-logistic-regression-logistic-function.png" alt="logistic函数" /></a><div class="caption">图 1:  logistic函数 [<a href="/assets/images/2015-01-17-logistic-regression-logistic-function.png">PNG</a>]</div></div></div>

<p>对于特征$\mathbf x=(x_0, x_1,\ldots,x_d)$，先计算加权的risk score，$s=\mathbf w^T\mathbf x$，然后再将此得分代入如上图所示的logistic函数<sup id="fnref:logistic-function-properties"><a href="#fn:logistic-function-properties" class="footnote">1</a></sup>
\begin{equation}
\theta(s)={1\over 1 + e^{-s}}
\label{eq:sigmoid-function}
\end{equation}
转化为概率的形式。logistic函数也称为sigmoid函数，单调、光滑。也就是可用logistic回归模型
\begin{equation}
h(\mathbf x)={1\over 1+\exp(-\mathbf w^T\mathbf x)}，
\end{equation}
作为目标函数$f(\mathbf x)=P(y|\mathbf x)$的近似。</p>

<p>建立logistic回归模型的基本思路是，找到最可能产生数据集$\mathcal D$的假设$h$。考虑数据集$\mathcal D=\{(\mathbf x_1,\circ),(\mathbf x_2,\times),\ldots,(\mathbf x_N,\times)\}$，根据\eqref{eq:f-vs-probility}可得<sup id="fnref:andrew-class-label"><a href="#fn:andrew-class-label" class="footnote">2</a></sup>
\[
P(y|\mathbf x)=
\left\{
\begin{aligned}
&amp;f(\mathbf x)&amp;\mbox{for }y=+1&amp;\\
&amp;1-f(\mathbf x)&amp;\mbox{for }y=-1&amp;，
\end{aligned}
\right.
\]
那么得到这个数据集的概率为
\[
\begin{aligned}
&amp;P(\mathbf x_1)P(\circ|\mathbf x_1)\times P(\mathbf x_2)P(\times|\mathbf x_2)\times\ldots P(\mathbf x_N)P(\times|\mathbf x_N)\\
=&amp;P(\mathbf x_1)f(\mathbf x_1)\times P(\mathbf x_2)(1-f(\mathbf x_2))\times\ldots P(\mathbf x_N)(1-f(\mathbf x_N))\Rightarrow\mbox{probability using }f\\
\approx &amp;P(\mathbf x_1)h(\mathbf x_1)\times P(\mathbf x_2)(1-h(\mathbf x_2))\times\ldots P(\mathbf x_N)(1-h(\mathbf x_N))\Rightarrow\mbox{likelihood}(h)，
\end{aligned}
\]
$h$产生这笔数据的可能性（likelihood）和$f$产生这笔数据的概率（probability）差不多，最终得到了$h$产生这笔数据的可能性。期望$f$产生这笔数据的概率相当大<sup id="fnref:why-large-probability"><a href="#fn:why-large-probability" class="footnote">3</a></sup>，因此有
\[
\mbox{likelihood}(h)\approx\mbox{probability using }f\approx\mbox{large}，
\]
选择可能性最高的那个$h$
\[
g=\arg\max_h\mbox{likelihood}(h)。
\]
根据logistic函数的性质$\theta(-s)=1-\theta(s)$，可进一步得到
\[
\mbox{likelihood}(h)=P(\mathbf x_1)h(+\mathbf x_1)\times P(\mathbf x_2)(-h(\mathbf x_2))\times\ldots P(\mathbf x_N)(-h(\mathbf x_N))，
\]
对所有$h$，$P(\mathbf x_i)$都不变，那么可得
\[
\mbox{likelihood}(\mbox{logistic }h)\varpropto\prod_{n=1}^Nh(y_n\mathbf x_n)，
\]
最佳$\mathbf w$应满足条件
\[
\max_{\mathbf w}\mbox{likelihood}(\mathbf w)\varpropto\prod_{n=1}^N\theta(y_n\mathbf w^T\mathbf x_n)。
\]
利用$\ln$将乘法化为加法，得到等价的优化问题
\begin{equation}
\min_\mathbf wE_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\ln\left(1+\exp\left(-y_n\mathbf w^T\mathbf x_n\right)\right)，
\end{equation}
其中$err(\mathbf w,\mathbf x,y)=\ln\left(1+\exp\left(-y\mathbf w^T\mathbf x\right)\right)$称为<strong>交叉熵误差</strong>（cross-entropy error）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-convex-or-non.png"><img src="/assets/images/2015-01-17-logistic-regression-convex-or-non.png" alt="［左］：平方误差代价函数；［右］交叉熵误差代价函数" /></a><div class="caption">图 2:  ［左］：平方误差代价函数；［右］交叉熵误差代价函数 [<a href="/assets/images/2015-01-17-logistic-regression-convex-or-non.png">PNG</a>]</div></div></div>

<p>上图展示了用不同误差度量方式的代价函数，上图左采用了线性回归基于平方误差的代价函数，非凸不利于优化。</p>

<h2 id="section">梯度下降法</h2>

<p>$E_{in}(\mathbf w)$是连续、可微、二次可微的凸函数，理论上取得最小值时$\mathbf w$满足$\nabla E_{in}(\mathbf w)=0$，其中
\begin{equation}
\nabla E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\theta(-y_n\mathbf w^T\mathbf x_n)(-y_n\mathbf x_n)。
\label{eq:gradient-logistic-object-function}
\end{equation}
由于$\nabla E_{in}(\mathbf w)=0$是非线性方程，并且$\theta(-y_n\mathbf w^T\mathbf x_n)＝0$的条件$y_n\mathbf w^T\mathbf x_n\gg 0$也难以满足，因此不存在类似线性回归的闭式解。</p>

<p>回顾<a href="/2014/10/pla/#pla-algorithm">感知器算法</a>的权值更新，稍微修改其表现形式
\[
\mathbf w_{t+1}\leftarrow\mathbf w_{t}+1\cdot\left(\left[\left[\mbox{sign}\left(\mathbf w_t^T\mathbf x_n\right)\neq y_n\right]\right]y_n\mathbf x_n\right)，
\]
简记为
\[
\mathbf w_{t+1}\leftarrow\mathbf w_{t}+\eta\mathbf v，
\]
其中$\eta=1$表示步长，$\mathbf v=\left[\left[\mbox{sign}\left(\mathbf w_t^T\mathbf x_n\right)\neq y_n\right]\right]y_n\mathbf x_n$表示方向。对$(\eta,\mathbf v)$和终止条件的不同定义，就会得到不同的迭代优化算法。考察梯度计算公式\eqref{eq:gradient-logistic-object-function}，$\theta(-y_n\mathbf w^T\mathbf x_n)$相当于梯度方向的加权值，当$y_n\mathbf w^T\mathbf x_n$越小的时候，权值越大，负值越大表示犯错越厉害。如果按照梯度方向更新，这和PLA有相同的含义，犯错越厉害的对更新贡献越大。</p>

<p>对于logistic回归的$E_{in}$，令$\eta&gt;0$，
\[
\min_{\lVert\mathbf v\rVert=1}E_{in}(\mathbf w_t+\eta\mathbf v)
\]
利用贪婪法找$\mathbf w_t$附近最好的一个方向，让$E_{in}$下降最多。上式仍然是非线性，且带约束条件，难以求解。当$\eta$足够小时，利用Taylor展式可得
\begin{equation}
E_{in}(\mathbf w_t+\eta\mathbf v)\approx E_{in}(\mathbf w_t)+\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)
\label{eq:taylor-expansion-Ein}
\end{equation}
若要实现
\[
\min_{\lVert\mathbf v\rVert=1}\left(E_{in}(\mathbf w_t)+\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)\right)，
\]
就要使得$\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)$最小，此时$\eta&gt;0$是常数，因此$\mathbf v$和$\nabla E_{in}(\mathbf w_t)$反向时得到的值最小，
\[
\mathbf v=-\frac{\nabla E_{in}(\mathbf w_t)}{\lVert\nabla E_{in}(\mathbf w_t)\rVert}，
\]
那么参数的更新规则为
\begin{equation}
\mathbf w_{t+1}\leftarrow\mathbf w_{t}-\eta\frac{\nabla E_{in}(\mathbf w_t)}{\lVert\nabla E_{in}(\mathbf w_t)\rVert}。
\label{eq:gd-iterate}
\end{equation}
这种参数更新方法就是梯度下降法，是一种简单且应用广泛的优化算法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-eta-example.png"><img src="/assets/images/2015-01-17-logistic-regression-eta-example.png" alt="不同步长的迭代效果" /></a><div class="caption">图 3:  不同步长的迭代效果 [<a href="/assets/images/2015-01-17-logistic-regression-eta-example.png">PNG</a>]</div></div></div>

<p id="why-fixed-eta">上图展示了利用\eqref{eq:gd-iterate}迭代的效果，$\eta$过小导致收敛太慢，过大导致不稳定，动态调整可以得到不错的效果。对于动态调整的$\eta$，坡度$\lVert\nabla E_{in}(\mathbf w_t)\rVert$大的时候采用大的$\eta$，小的时候采用小的$\eta$。当$\eta\leftarrow{\eta\over\lVert\nabla E_{in}(\mathbf w_t)\rVert}$时，固定住新的$\eta$就会达到动态调整的效果，因此可以采用固定$\eta$的自适应步长更新方式</p>
<p>\begin{equation}
\mathbf w_{t+1}\leftarrow\mathbf w_{t}-\eta\nabla E_{in}(\mathbf w_t)。
\label{eq:gd-iterate-2}
\end{equation}</p>

<blockquote>
  <h4 id="logistic-1">梯度下降法求解logistic回归参数</h4>
  <hr />
  <p>初始化$\mathbf w_0$和$\eta$；</p>

  <p>对$t=0,1,\ldots$循环以下步骤，直到$\nabla E_{in}(\mathbf w_{t+1})=0$或达到设定迭代次数：</p>

  <ol>
    <li>利用公式\eqref{eq:gradient-logistic-object-function}计算$\nabla E_{in}(\mathbf w_{t})$；</li>
    <li>利用公式\eqref{eq:gd-iterate-2}更新参数$\mathbf w_{t+1}$。</li>
  </ol>
</blockquote>

<p>梯度下降法的注意事项可以参考<a href="/2015/01/linear-regression/#gd-method">线性回归梯度法</a>的注意事项。</p>

<h2 id="section-1">参考文献</h2>

<ol class="bibliography"><li><span id="lin_ml_logistic_regression_2014">[1]H.-T. Lin, “Lecture 10: Logistic Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_lr_2014">[2]A. Ng, “Logistic Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-2">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:logistic-function-properties">
      <p>logistic函数的性质：（1）$\theta(-s)=1-\theta(s)$；（2）${d\theta(s)\over ds}=\theta(s)(1-\theta(s))$（${d\theta(s)\over d^Ns}=\theta(s)\prod_{n=0}^N(1-N\theta(s))$<strong>????</strong>）；（3）${\theta(s)\over 1-\theta(s)}=e^s$。 <a href="#fnref:logistic-function-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-class-label">
      <p>Andrew NG课程中采用了$\{0,1\}$类别标签<a href="#ng_ml_lr_2014">[2]</a>，因此得到的公式与本文形式不同。 <a href="#fnref:andrew-class-label" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-large-probability">
      <p><del>为什么$f$产生这笔数据的概率很大？</del>期望找到最合适的$h$，使产生数据集$\mathcal D$的可能性最大。 <a href="#fnref:why-large-probability" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="left">
<p></p>
<hr>
  <p><img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px"></p>
  <p></p>
<hr>
  <ul>
    
    <li class="pageNav">2015-10-13 &raquo; <a href="/2015/10/minimum-cut-based-inference">DILinAV（4）：基于最小割的推理</a></li>
    
    <li class="pageNav">2015-09-26 &raquo; <a href="/2015/09/fast-ncc">快速归一化互相关</a></li>
    
    <li class="pageNav">2015-09-25 &raquo; <a href="/2015/09/maximum-flow-and-minimum-cut">DILinAV（3）：最大流与最小割</a></li>
    
    <li class="pageNav">2015-09-22 &raquo; <a href="/2015/09/reparameterization-and-dp">DILinAV（2）：重参数化与动态规划</a></li>
    
    <li class="pageNav">2015-09-19 &raquo; <a href="/2015/09/introduction-to-av-with-dgm">DILinAV（1）：基于离散图模型的人工视觉简介</a></li>
    
    <li class="pageNav">2015-09-16 &raquo; <a href="/2015/09/haze-removal-kaiming">去雾霾：基于单图的暗通道方法</a></li>
    
    <li class="pageNav">2015-08-13 &raquo; <a href="/2015/08/tesseract-ocr">开源OCR引擎Tesseract</a></li>
    
    <li class="pageNav">2015-07-28 &raquo; <a href="/2015/07/HPC-overview">高性能计算概述</a></li>
    
  </ul>
<p></p>
<span>
  <a  href="/2015/01/adaptive-boosting" class="pageNav" style="float:left"   >上一篇：分类器融合（2）：AdaBoost </a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/01/image-classification-knn-based-introduction" class="pageNav" style="float:right"   >下一篇：CNN（2）：基于k最近邻算法的简介 </a>  
</span>
</section>

	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<ul class="ds-recent-visitors" data-num-items="16"></ul>
	<div class="ds-thread"  data-thread-key="/2015/01/logistic-regression" 	data-url="http://qianjiye.de/2015/01/logistic-regression" data-title="logistic回归">
	</div>	


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/01/adaptive-boosting'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/01/image-classification-knn-based-introduction'; --1> -->
<!--         url = '/2015/01/image-classification-knn-based-introduction'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/01/logistic-regression/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2016 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
