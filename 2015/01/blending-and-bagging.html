<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>分类器融合（1）：基于混合与自助聚集的简介</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />
    <link rel="stylesheet" href="/assets/css/twemoji-awesome.css" />  
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link href="/assets/css/jquery-ui-1.10.4.custom.min.css" rel="stylesheet" />
    <link href="/assets/css/ggvis.css" rel="stylesheet" />
    <link href="/assets/css/mermaid.css" rel="stylesheet" />
    <link rel="stylesheet" href="/assets/css/markdown-plus.css"/> 
    <link rel="stylesheet" href="/assets/css/flexslider.css" type="text/css" media="screen" />
      <style type="text/css">
        .flex-caption {
          width: 96%;
          padding: 2%;
          left: 0;
          bottom: 0;
          background: rgba(0,0,0,.5);
          color: #fff;
          text-shadow: 0 -1px 0 rgba(0,0,0,.3);
          font-size: 14px;
          line-height: 18px;
        }
        li.css a {
          border-radius: 0;
        }
      </style>

    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/jquery-ui-1.10.4.custom.min.js"></script>
    <script type="text/javascript" src="/assets/js/d3.min.js"></script>
    <script type="text/javascript" src="/assets/js/vega.min.js"></script>
    <script type="text/javascript" src="/assets/js/lodash.min.js"></script>
    <script>var lodash = _.noConflict();</script>
    <script type="text/javascript" src="/assets/js/ggvis.js"></script>
    <script type="text/javascript" src="/assets/js/htmlwidgets.js"></script>
    <script type="text/javascript" src="/assets/js/echarts-all.js"></script>
    <script type="text/javascript" src="/assets/js/echarts.js"></script>
    <script defer src="/assets/js/jquery.flexslider-min.js"></script>
    <script type="text/javascript">
      // $(function(){
      //   SyntaxHighlighter.all();
      // });
      $(window).load(function(){
        $('.flexslider').flexslider({
          animation: "slide",
          start: function(slider){
            $('body').removeClass('loading');
          }
        });
      });
    </script>

    <script type="text/javascript">
      function setTimeSpan(){
        var date = new Date();
        timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
      }

      Date.prototype.format = function(format)
      {
        var o =
        {
          "M+" : this.getMonth()+1, //month
          "d+" : this.getDate(),    //day
          "h+" : this.getHours(),   //hour
          "m+" : this.getMinutes(), //minute
          "s+" : this.getSeconds(), //second
          "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
          "S" : this.getMilliseconds() //millisecond
        }
        if(/(y+)/.test(format))
          format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
        for(var k in o)
          if(new RegExp("("+ k +")").test(format))
            format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
          return format;
        }
      </script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>分类器融合（1）：基于混合与自助聚集的简介</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <script type="text/javascript" src="/assets/js/outliner.js"></script>

<section class="meta">
<span class="time">
  <time datetime="2015-01-15">2015-01-15</time>
</span>

 |
<span class="categories">
  <i class="fa fa-share-alt"></i>
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#分类器融合" title="分类器融合">分类器融合</a>&nbsp;
  
</span>

</section>
<section class="post">
<p><strong>混合</strong>（blending）是将已学到的不同假设以均匀、线性或非线性的方式融合起来；若先从boostrapped数据集学到各种不同的假设，然后再混合，就称为<strong>自助聚集</strong>（bagging, bootstrap aggregating）。</p>

<p>混合在实际中很有用，但也增加了计算复杂度。</p>

<h2 id="section">融合简介</h2>

<p>假设$T$个👬朋友$g_1,\ldots,g_T$给出参考意见$g_t(\mathbf x)$预测股票是否会涨，如何决策呢？</p>

<ol>
  <li><a href="/2015/01/validation">验证法</a>（validation）：听从最懂股票那个朋友的意见，
\begin{equation*}
G(\mathbf x)=g_{t_*}(\mathbf x)，\qquad t_*=\underset{t\in\{1,2,\ldots,T\}}{\arg\min}E_{val}\left(g_t^-\right)，
\end{equation*}
首先在稍小训练集（去除了验证集）上得到所有候选$\{g_t^-\}$，然后在验证集上筛选出最优$g_{t_*}^-$，最后在全训练集（包含验证集）上得到$g_{t_*}$。</li>
  <li>投票法（vote）：一人投一票，听从多数人的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T1\cdot g_t(\mathbf x)\right)。
\label{eq:uniform-blending-hypothesis}
\end{equation}</li>
  <li>加权投票法：每人投票数不一样，听从多数票的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T\alpha_t\cdot g_t(\mathbf x)\right),\qquad\alpha_t\geq 0。
\label{eq:linear-blending-hypothesis}
\end{equation}
当$\alpha_t=[[E_{val}\left(g_{t}^-\right)\mbox{ smallest}]]$时，与方法1一样；当$\alpha_t=1$时，与方法2一样。</li>
  <li>有条件的组合：比如科技股听从擅长这方面的朋友，传统行业股票听从那些……
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T q_t(\mathbf x)\cdot g_t(\mathbf x)\right),\qquad q_t(\mathbf x)\geq 0，
\label{eq:conditional-blending-hypothesis}
\end{equation}
当$q_t(\mathbf x)=\alpha_t$时，与方法3一样，也就是包含了前面所有情况。</li>
</ol>

<p>上述1的验证法，若用$E_{in}(g_t)$代替$E_{val}(g_t)$选择模型，可能会付出<a href="/2015/01/three-learning-principles/#no-snooping">很大VC维的代价</a>。这种方法需要一个强大优秀的$g_t^-$，否则也只是差中择优。上述的2～4称为<strong>融合模型</strong>（aggregation model），其目的在于综合多个假设（可能是比较弱的）让效果更好。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png"><img src="/assets/images/2015-01-15-blending-and-bagging-vote-method.png" alt="［左］：水平垂直线投票；［右］PLA投票" /></a><div class="caption">图 1:  ［左］：水平垂直线投票；［右］PLA投票 [<a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png">PNG</a>]</div></div></div>

<p>上图展示的投票法中，灰色的判别界表示参与投票的分类器，合理的融合方法能提升分类性能。上图左，水平和垂直线将平面分割成6块区域，可得绿色标注的投票结果，相当于组合成了黑色的判别界。上图左的投票方法，具有<a href="/2015/02/summary-of-aggregation-models/#aggregation-performance">特征变换</a>的效果；上图右的感知器投票，具有<a href="/2015/02/summary-of-aggregation-models/#aggregation-performance">正则化</a>的效果。</p>

<h2 id="section-1">均匀混合</h2>

<p>均匀混合（uniform blending）也就是投票法\eqref{eq:uniform-blending-hypothesis}。对于多个不同的假设，简单的混合法则也可得到比单一假设更好的结果。如果所有$g_t$都一样，等价于采用任一$g_t$；如果$g_t$千差万别，就是少数服从多数。该方法可以直接推广到多类的情况，
\[
G(\mathbf x)=\arg\max_{1\leq k\leq K}\sum_{t=1}^T[[g_t(\mathbf x)=k]]。
\]</p>

<p>均匀混合解决回归问题的方法是
\[
G(\mathbf x) = {1\over T}\sum_{t=1}^Tg_t(\mathbf x)。
\]
如果所有$g_t$都一样，等价于采用任一$g_t$；对于不同的$g_t$，可能得到比单个$g_t$更准确的结果。</p>

<p>对于均匀混合的回归，
\[
\begin{aligned}
avg\left(\left(g_t(\mathbf x)-f(\mathbf x)\right)^2\right)
=&amp;avg\left(g_t^2-2g_t^2f+f^2\right)\\
=&amp;avg\left(g_t^2\right)-2Gf+f^2\\
=&amp;avg\left(g_t^2\right)-G^2+(G-f)^2\\
=&amp;avg\left(g_t^2\right)-2G^2 + G^2 +(G-f)^2\\
=&amp;avg\left(g_t^2-2g_tG + G^2\right) +(G-f)^2\\
=&amp;avg\left(\left(g_t-G\right)^2\right) +(G-f)^2。
\end{aligned}
\]</p>

<p>若对产生$\mathbf x$分布的所有点都进行上述运算，然后取期望可得
\[
avg\left(E_{out}\left(g_t\right)\right)=avg(\varepsilon(g_t-G)^2)+E_{out}(G)\geq E_{out}(G)，
\]
也就是说，均匀混合方法的效果会比选择其中一个好。</p>

<p>从$P^N$采集大小为$N$的$T$个数据集，利用上述公式，衡量演算法$\mathcal A$的表现。通过$\mathcal A(\mathcal D_t)$获得$g_t$，对其平均
\[
\bar g=\lim_{T\rightarrow\infty}G=\lim_{T\rightarrow\infty}{1\over T}\sum_{t=1}^Tg_t=\varepsilon_{\mathcal D}\mathcal A(\mathcal D)，
\]
算法$\mathcal A$的性能期望为
\begin{equation}
avg(E_{out}(g_t))=avg(\varepsilon(g_t-\bar g)^2)+E_{out}(\bar g)。
\label{eq:bias-variance-decomposition}
\end{equation}</p>

<p>上述公式中：</p>

<ul>
  <li>$avg(E_{out}(g_t))$：算法$\mathcal A$的期望性能；</li>
  <li>$E_{out}(\bar g)$：算法共识（consensus）的性能（$\bar g$就是从$\mathcal D_t\sim P^N$期望获得的$g_t$），通常称为<strong>bias</strong>；</li>
  <li>$avg(\varepsilon(g_t-\bar g)^2)$：偏离共识的期望，通常称为<strong>variance</strong>。</li>
</ul>

<p>通过bias和variance，将演算法的表现拆分为两部分。均匀混合通过减小variance获得更稳定的性能。</p>

<h2 id="section-2">线性混合</h2>

<p>通过\eqref{eq:linear-blending-hypothesis}，赋予不同假设不同的权重就是<strong>线性混合</strong>（linear blending）。线性回归的线性混合目标为
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^N\left(y_n-\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)^2，
\]
将$g(\mathbf x)$视为特征变换$\phi(\mathbf x)$，换一种表达形式
\[
\min_{\mathbf w}{1\over N}\sum_{n=1}^{N}\left(y_n-\sum_{i=1}^{\tilde d}w_i\phi_i(\mathbf x_n)\right)^2，
\]
这就类似两阶（two-level）的学习方法。</p>

<p>线性混合＝［1］线性模型＋［2］假设（hypothesis）视为变换＋［3］约束条件，
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^Nerr\left(y_n,\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)。
\]
对于二分类问题
\[
\alpha_tg_t(\mathbf x)=|\alpha_t|(-g_t(\mathbf x))\qquad\mbox{if }\alpha_t&lt;0，
\]
正负对分类器本质上没有差别，实际上有“线性混合＝线性模型＋假设（hypothesis）视为变换”，不需要$\alpha_t$的约束条件。</p>

<p>在实际中，$g_t$通常是用$E_{in}$从各模型中选的最优，$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\dots,g_T\in\mathcal H_T$。如果在这些$g_t$中用$E_{in}$再选最优的，就是best of best，将付出高复杂度$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。如果在这些$g_t$中用$E_{in}$再采用线性混合，就是aggregation of best，将付出<strong>高于</strong>$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。实际上通常采用$E_{val}$替代$E_{in}$，通过最小化$E_{train}$（也就是$E_{in}$）得到$g_t^-$。</p>

<blockquote>
  <h4 id="section-3">线性混合算法</h4>
  <hr />

  <ol>
    <li>在$\mathcal D_{train}$上选出$g_1^-,g_2^-,\ldots,g_T^-$；</li>
    <li>在$\mathcal D_{val}$中将$(\mathbf x_n,y_n)$转换为$(\mathbf z_n=\phi^-(\mathbf x_n),y_n)$，其中$\phi^-(\mathbf x)＝(g_1^-(\mathbf x),g_2^-(\mathbf x),\ldots,g_T^-(\mathbf x))$；</li>
    <li>计算$\boldsymbol\alpha=\mbox{LinearModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
    <li>返回 $G_{LINB}(\mathbf x)=\mbox{LinearHypothesis}_\boldsymbol\alpha(\phi(\mathbf x))$，其中$\phi(\mathbf x)＝(g_1(\mathbf x),g_2(\mathbf x),\ldots,g_T(\mathbf x))$。</li>
  </ol>

  <p><strong>如果将3、4两步换为：</strong></p>

  <ul>
    <li>计算$\tilde g=\mbox{AnyModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
    <li>返回$G_{ANYB}(\mathbf x)=\tilde g(\phi(\mathbf x))$。</li>
  </ul>

  <p>这就是any blending（stacking）的方法。any blending方法强大，可以实现conditional blending，但是也存在过拟合的风险。</p>
</blockquote>

<p>stacking（any blending）也称为stacked generalization，在有监督学习（比如回归<a href="#breiman1996stacked">[1]</a>）和无监督学习（比如密度估计<a href="#smyth1999linearly">[2]</a>）中都有成功应用，也可用于估计自助聚集的错误率<a href="#rokach_air_ensemble_2010">[3]</a><a href="#wolpert1999efficient">[4]</a>。它的性能优于贝叶斯模型平均法<a href="#clarke2003comparing">[5]</a>。在Netflix的竞赛中，有两只优胜队伍采用了stacking技术<a href="#sill2009feature">[6]</a>。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png"><img src="/assets/images/2015-01-15-blending-and-bagging-blending-example.png" alt="混合方法使用实例" /></a><div class="caption">图 2:  混合方法使用实例 [<a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png">PNG</a>]</div></div></div>

<p>上图的流程中，Val.-Set Blending就是any blending的方法，使得$E_{test}$降低到$456.24$。将上百个$g_t$用近似的$\tilde E_{test}$（若$\tilde E_{test}$好，真正的$E_{test}$也会好）进行线性混合得到$G$。</p>

<h2 id="section-4">自助聚集</h2>

<p>对于均匀融合（其它融合方法也如此），<strong>不同模型（假设）</strong>参与融合是关键，获取不同模型的方法包括：</p>

<ul>
  <li>从不同假设集获取模型：$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\ldots,g_T\in\mathcal H_T$；</li>
  <li>采用不同的参数：例如梯度下降法采用$\eta=0.001,0.01,\ldots,10$；</li>
  <li>利用算法的随机性：例如用不同的初始化进行PLA；</li>
  <li>利用数据的随机性：交叉验证采用不同验证集。</li>
</ul>

<p>回顾\eqref{eq:bias-variance-decomposition}，算法的性能被拆分为bias和variance两部分进行评估。当采用不同$g_t$融合时，共识（融合）的结果比$\mathcal A(\mathcal D)$的单一$g_t$效果好。能否通过有限的$T$和单一数据集$\mathcal D$得到不同$g_t$和近似的$\bar g$？✅</p>

<p><strong>bootstrapping</strong>是一种通过重采样（re-sample）$\mathcal D$模拟$\mathcal D_t$的统计学工具。bootstrap得到$\tilde{\mathcal D}_t$的方法：从$\mathcal D$中随机抽取一个点，纪录该点后然后放回，重复该过程直到抽取到$N’$个数据。利用bootstrap的不同$\tilde{\mathcal D}_t$可以得到不同的$g_t$。</p>

<blockquote>
  <h4 id="bootstrap-aggregating">自助聚集（<strong>b</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>）</h4>
  <hr />

  <ol>
    <li>利用bootstrapping技术得到$N’$点的数据集$\tilde{\mathcal D}_t$；</li>
    <li>利用$\mathcal A(\tilde{\mathcal D}_t)$，算法$\mathcal A$在数据集$\tilde{\mathcal D}_t$上得到$g_t$；</li>
    <li>$G=\mbox{Uniform}(\{g_t\})$。</li>
  </ol>
</blockquote>

<p>像自助聚集这种，建立在其它基础算法（base algorithm）$\mathcal A$之上的算法称为元算法（meta algorithm）。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png"><img src="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png" alt="bagging pocket方法" /></a><div class="caption">图 3:  bagging pocket方法 [<a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png">PNG</a>]</div></div></div>

<p>上图是bagging pocket方法的效果，通过bagging得到各不相同的$g_t$，然后通过融合得到合适的非线性分类器。</p>

<p>如果基础算法（base algorithm）对数据的随机性很敏感，或者说基础算法不稳定（instability），那么训练数据的扰动或参数的微调能使分类结果发生明显变化，通过bagging就能提升分类精度<a href="#breiman1996bagging">[7]</a>。<a href="/2016/03/cs231n_image-classification-pipeline">k最近邻算法</a>是稳定的，不适合作为bagging的基础算法<a href="#breiman1996out">[8]</a>。</p>

<h2 id="section-5">参考资料</h2>

<ol class="bibliography"><li><span id="breiman1996stacked">[1]L. Breiman, “Stacked regressions,” <i>Machine learning</i>, vol. 24, no. 1, pp. 49–64, 1996.</span>

</li>
<li><span id="smyth1999linearly">[2]P. Smyth and D. Wolpert, “Linearly combining density estimators via stacking,” <i>Machine Learning</i>, vol. 36, no. 1-2, pp. 59–83, 1999.</span>

</li>
<li><span id="rokach_air_ensemble_2010">[3]L. Rokach, “Ensemble-based classifiers,” <i>Artificial Intelligence Review</i>, vol. 33, no. 1-2, pp. 1–39, 2010.</span>

[<a href="http://dx.doi.org/10.1007/s10462-009-9124-7">Online</a>]

</li>
<li><span id="wolpert1999efficient">[4]D. H. Wolpert and W. G. Macready, “An efficient method to estimate bagging’s generalization error,” <i>Machine Learning</i>, vol. 35, no. 1, pp. 41–55, 1999.</span>

</li>
<li><span id="clarke2003comparing">[5]B. Clarke, “Comparing Bayes model averaging and stacking when model approximation error cannot be ignored,” <i>The Journal of Machine Learning Research</i>, vol. 4, pp. 683–712, 2003.</span>

</li>
<li><span id="sill2009feature">[6]J. Sill, G. Takács, L. Mackey, and D. Lin, “Feature-weighted linear stacking,” <i>arXiv preprint arXiv:0911.0460</i>, 2009.</span>

</li>
<li><span id="breiman1996bagging">[7]L. Breiman, “Bagging predictors,” <i>Machine learning</i>, vol. 24, no. 2, pp. 123–140, 1996.</span>

</li>
<li><span id="breiman1996out">[8]L. Breiman, “Out-of-bag estimation,” Citeseer, 1996.</span>

</li></ol>

<h3 id="section-6">脚注</h3>


</section>
<section align="left">
<p></p>
<hr>
  <p><img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px"></p>
  <p></p>
<hr>
  <ul>
    
    <li class="pageNav">2016-09-18 &raquo; <a href="/2016/09/S13000-Introduction">鲁棒及自适应控制（1）：概论</a></li>
    
    <li class="pageNav">2016-09-13 &raquo; <a href="/2016/09/LeNet-5-Lecun">Gradient-Based Learning Applied to Document Recognition</a></li>
    
    <li class="pageNav">2016-03-15 &raquo; <a href="/2016/03/cs231n_loss-functions-and-optimization">CS231n（3）：损失函数与最优化</a></li>
    
    <li class="pageNav">2016-03-14 &raquo; <a href="/2016/03/cs231n_image-classification-pipeline">CS231n（2）：图像分类流程</a></li>
    
    <li class="pageNav">2016-02-29 &raquo; <a href="/2016/02/cs231n_introduction-to-computer-vision">CS231n（1）：计算机视觉简介</a></li>
    
    <li class="pageNav">2015-10-13 &raquo; <a href="/2015/10/minimum-cut-based-inference">DILinAV（4）：基于最小割的推理</a></li>
    
    <li class="pageNav">2015-09-26 &raquo; <a href="/2015/09/fast-ncc">快速归一化互相关</a></li>
    
    <li class="pageNav">2015-09-25 &raquo; <a href="/2015/09/maximum-flow-and-minimum-cut">DILinAV（3）：最大流与最小割</a></li>
    
  </ul>
<p></p>
<span>
  <a  href="/2015/01/linear-regression" class="pageNav" style="float:left"   >上一篇：线性回归 </a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/01/adaptive-boosting" class="pageNav" style="float:right"   >下一篇：分类器融合（2）：AdaBoost </a>  
</span>
</section>

	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<ul class="ds-recent-visitors" data-num-items="16"></ul>
	<div class="ds-thread"  data-thread-key="/2015/01/blending-and-bagging" 	data-url="http://qianjiye.de/2015/01/blending-and-bagging" data-title="分类器融合（1）：基于混合与自助聚集的简介">
	</div>	


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/01/linear-regression'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/01/adaptive-boosting'; --1> -->
<!--         url = '/2015/01/adaptive-boosting'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/01/blending-and-bagging/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2016 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
