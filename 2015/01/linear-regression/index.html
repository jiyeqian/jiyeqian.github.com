<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>线性回归
    </title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>

  <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
          (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
          e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

      _st('install','RFsGWEn74xUkvs4V5QRD');
  </script>

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>线性回归</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2015-01-13">2015-01-13</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#回归" title="回归">回归</a>&nbsp;
  
  <a href="/tags/#最优化" title="最优化">最优化</a>&nbsp;
  
  <a href="/tags/#梯度下降法" title="梯度下降法">梯度下降法</a>&nbsp;
  
</span>

</section>
<section class="post">
<p><img src="/assets/images/2014-10-19-linear_regression_0.png" alt="线性回归" /></p>

<p>本节的主要参考资料是机器学习基石<a href="#lin_mlf_linreg_2014">[1]</a>和机器学习<a href="#ng_ml_linreg_2014">[2]</a>网络课程。</p>

<h2 id="section">线性回归模型</h2>

<p>线性回归的假设集（hypothesis）为
\begin{equation}
h(\mathbf x) = \mathbf w^T\mathbf x，
\label{eq:linear-regression-model}
\end{equation}
$\mathbf x$是含有常数项$x_0 = 1$的$d+1$维向量$\mathbf x =\left[1,x_1,\ldots,x_d\right]^T$，$\mathbf w=\left[w_0,w_1,\ldots,w_d\right]^T$，$d$是特征维数。</p>

<p>目标代价函数采用平方误差和估计
\begin{equation}
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n-y_n\right)^2，
\label{eq:cost-function-linear-regression}
\end{equation}
$N$是样本数。通过机器学习算法求得线性回归的参数
\begin{equation}
\mathbf w_{LIN} = \arg\min_{\mathbf w}E_{in}(\mathbf w)，
\end{equation}
通常方法有正规方程（normal equation）的解析方法和梯度下降法（gradient descent）。</p>

<h2 id="section-1">解析方法</h2>

<p>解析方法得到的最优解也称为<strong>线性回归的最小二乘解</strong>。</p>

<p>代价函数改写为矩阵形式
\[
E_{in}(\mathbf w) 
={1\over N}\lVert\mathbf X\mathbf w-\mathbf y\rVert^2
={1\over N}\left(\mathbf w^T\mathbf X^T\mathbf X\mathbf w-2\mathbf w^T\mathbf X^T\mathbf y+\mathbf y^T\mathbf y\right)，
\]
$\mathbf X$是样本数据矩阵，每行代表一个样本点，每列代表一个特征，第一列的向量$\mathbf 1_N$对应常数偏移。$E_{in}(\mathbf w)$是连续可微的凸函数，当$\nabla E_{in}(\mathbf w)=0$时取得最小值，
\[
\nabla E_{in}(\mathbf w)={2\over N}\left(\mathbf X^T\mathbf X\mathbf w-\mathbf X^T\mathbf y\right)，
\]
取得最小值时
\begin{equation}
\mathbf w_{LIN} = \left(\mathbf X^T\mathbf X\right)^{-1}\mathbf X^T\mathbf y = \mathbf X^\dagger \mathbf y，
\end{equation}
$\mathbf X^\dagger$称为<strong>伪逆</strong>（pseudo-inverse）。通常情况$N\gg d+1$，因此$\left(\mathbf X^T\mathbf X\right)^{-1}$通常都可逆；如果不可逆，解不唯一。</p>

<p>导致$\mathbf X^T\mathbf X$不可逆的原因可能是冗余特征（redundant features）或者特征数目过多（$d$太大而$N$太少），解决的办法：   </p>

<ul>
  <li>对于冗余的线性相关特征，例如$x_1 = 2x_2$，删除线性相关特征；</li>
  <li>对于特征数目过多，例如$N&lt;d$，删除特征或正则化（regularization）<sup id="fnref:how-to-regularize"><a href="#fn:how-to-regularize" class="footnote">1</a></sup>。</li>
</ul>

<p>Matlab的<code>pinv</code>函数可以处理$\mathbf X^T\mathbf X$不可逆的情况<sup id="fnref:pinv-vs-inv"><a href="#fn:pinv-vs-inv" class="footnote">2</a></sup>。</p>

<h3 id="mathbf-wlin">解析方法求解$\mathbf w_{LIN}$是机器学习算法吗？</h3>

<p>✅通过VC维的角度分析，能得到小的$E_{out}\left(\mathbf w_{\small{LIN}}\right)$，就是学习……</p>

<ul>
  <li>能够得到最佳的$E_{in}$；</li>
  <li>$d+1$个变量，有限的$d_{VC}$，因此有好的$E_{out}$；</li>
  <li>事实上，求解伪逆的过程也是迭代逐步最优的过程（高斯消元法）。</li>
</ul>

<p>VC维考察的是个别的$E_{in}$<sup id="fnref:some-E-in"><a href="#fn:some-E-in" class="footnote">3</a></sup>，从$E_{in}$平均误差角度分析
\begin{equation}
\bar E_{in}
=\varepsilon_{\mathcal D\sim P^N}\left\{E_{in}\left(\mathbf w_{LIN}\mbox{ w.r.t }\mathcal D\right)\right\}
=\mbox{noise level}\cdot\left(1-{d+1\over N}\right)，
\label{eq:noise-level-e-in}
\end{equation}
$\mbox{noise level}$表示数据中的噪声，${d+1\over N}$表示比噪声小的比率，数据越多二者差别越小。</p>

<p>$E_{in}\left(\mathbf w_{LIN}\right)$计算方法为
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert\mathbf y-\mathbf X\mathbf X^\dagger\mathbf y\right\rVert^2
={1\over N}\left\lVert\left(\mathbf I-\mathbf X\mathbf X^\dagger\right)\mathbf y\right\rVert^2，
\]
$\mathbf X\mathbf X^\dagger$让$\mathbf y$加帽$\wedge$变成了$\hat{\mathbf y}$，也叫<strong>帽矩阵</strong><sup id="fnref:hat-matrix-properties"><a href="#fn:hat-matrix-properties" class="footnote">4</a></sup>（hat matrix），记为$\mathbf H$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-learning-curve.png"><img src="/assets/images/2015-01-13-linear-regression-learning-curve.png" alt="［左］：图解证明；［右］：学习曲线" /></a><div class="caption">图 1:  ［左］：图解证明；［右］：学习曲线 [<a href="/assets/images/2015-01-13-linear-regression-learning-curve.png">PNG</a>]</div></div></div>

<p>由$\hat{\mathbf y}=\mathbf X\mathbf w_{LIN}$可知，$\hat{\mathbf y}$是$\mathbf X$列向量的线性组合，也就是如上图左所示，$\hat{\mathbf y}$位于$\mathbf X$张成的线性空间中。当$\mathbf y-\hat{\mathbf y}$垂直于该生成空间时，$\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2$的值最小。$\mathbf H$将$\mathbf y$投影为$\hat{\mathbf y}$，$\mathbf I-\mathbf H$将$\mathbf y$投影为$\mathbf y-\hat{\mathbf y}$。$\mathbf I-\mathbf H$的迹为$trace(\mathbf I-\mathbf H)=N-(d+1)$，表示自由度从$N$降到$N-(d+1)$。</p>

<p>观测到的数据$\mathbf y$是理想的数据空间$f\left(\mathbf X\right)$叠加一些噪声。$\mathbf y-\hat{\mathbf y}$也可以从噪声投影得到，如上图左所示，
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert(\mathbf I-\mathbf H)\cdot\mbox{noise}\right\rVert^2
={1\over N}(N-(d+1))\lVert\mbox{noise}\rVert^2，
\]
因此可得公式\eqref{eq:noise-level-e-in}的结论。$E_{out}$的证明过程叫复杂，仍然可以得到
\[
\bar E_{out}
=\mbox{noise level}\cdot\left(1+{d+1\over N}\right)。
\]</p>

<p>$\mbox{noise level}$可以用$\sigma^2$表示，从上图右可见，$\bar E_{in}$和$\bar E_{out}$在$N\rightarrow\infty$时都会趋近于$\sigma^2$。期望的泛化误差（generalization error）可以用${2(d+1)\over N}$衡量，这里是平均情况，VC维衡量的是最坏的情况。</p>

<h2 id="gd-method">梯度下降法</h2>

<p>梯度下降法就是沿梯度下降方向更新参数，也就是对每个特征的权值$w_i$，不断迭代执行更新
\begin{equation}
w_i := w_i-\alpha{\partial E_{in}(\mathbf w)\over\partial w_i}\quad(i=0,1,\ldots,d)
\end{equation}
直至收敛，其中$\alpha$表示学习速率，梯度计算公式为
\[
{\partial E_{in}(\mathbf w)\over\partial w_i}
={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n - y_n\right)x_{n,i}。
\]</p>

<p>参数须同时更新，也就是当每个$w_i$都更新完成后，才能用新的$\mathbf w$计算$E_{in}(\mathbf w)$，具体可参考<a href="/assets/images/2014-10-19-linear_regression_1.png">Andrew NG的讲义</a><sup id="fnref:andrew-simultaneous-update"><a href="#fn:andrew-simultaneous-update" class="footnote">5</a></sup>。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-curve.png"><img src="/assets/images/2015-01-13-linear-regression-error-curve.png" alt="［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；&lt;br/&gt;［右］：梯度下降路径" /></a><div class="caption">图 2:  ［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；<br />［右］：梯度下降路径 [<a href="/assets/images/2015-01-13-linear-regression-error-curve.png">PNG</a>]</div></div></div>

<p>梯度下降法需要将所有特征归一（feature scaling）到统一的尺度（不用归一化$x_0$），比如$-1\le x_i\le 1$，
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\qquad\mbox{or}\qquad
\hat{x}_i = \frac{x_i - x_{mean}}{x_{std}}，
\]
这样有助于提高梯度下降法的速度，如上图左2所示。但是，归一化改变了特征的物理意义，有时并非好事儿。</p>

<h4 id="alpha">学习率$\alpha$的注意事项：</h4>
<ol>
  <li>在迭代过程中不需调节$\alpha$大小，由于梯度会不断减小，<a href="/2015/01/logistic-regression/#why-fixed-eta">在固定$\alpha$的情况下梯度下降步长也会自动减小</a>，如上图右所示；</li>
  <li>$\alpha$太小收敛慢，太大可能错过极值点而不收敛，甚至可能导致$E_{in}(\mathbf w)$不降反升。</li>
</ol>

<p>线性回归的代价函数$E_{in}(\mathbf w)$不存在局部极值（local optima），极小值就是全局极值<sup id="fnref:no-local-optima"><a href="#fn:no-local-optima" class="footnote">6</a></sup>。</p>

<h2 id="section-2">两种方法对比</h2>

<table>
  <thead>
    <tr>
      <th>梯度下降法</th>
      <th>解析解法</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>迭代实现，可实现在线增量学习</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$d$很大时（$10^6$）工作良好</td>
      <td>$d$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化<sup id="fnref:why-not-scale"><a href="#fn:why-not-scale" class="footnote">7</a></sup></td>
    </tr>
  </tbody>
</table>

<h2 id="section-3">分类问题</h2>

<p>线性分类器和线性回归的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>指标</th>
      <th>线性分类器</th>
      <th>线性回归</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathcal Y$</td>
      <td>$\{-1,+1\}$</td>
      <td>$\mathbb R$</td>
    </tr>
    <tr>
      <td>$h(\mathbf x)$</td>
      <td>$\mbox{sign}\left(\mathbf w^T\mathbf x\right)$</td>
      <td>$\mathbf w^T\mathbf x$</td>
    </tr>
    <tr>
      <td>$err(\hat{y},y)$</td>
      <td>$[[\hat{y}\neq y]]$</td>
      <td>$(\hat{y}-y)^2$</td>
    </tr>
    <tr>
      <td>算法复杂度</td>
      <td>通常是NP-hard</td>
      <td>高效求解方法</td>
    </tr>
  </tbody>
</table>

<p>能否利用线性回归的高效，借助$g(\mathbf x)=\mbox{sign}\left(\mathbf w_{LIN}^T\mathbf x\right)$，用线性回归解决分类问题？✅</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-compare.png"><img src="/assets/images/2015-01-13-linear-regression-error-compare.png" alt="线性分类器和线性回归误差比较" /></a><div class="caption">图 3:  线性分类器和线性回归误差比较 [<a href="/assets/images/2015-01-13-linear-regression-error-compare.png">PNG</a>]</div></div></div>

<p>上图展示了两种方法误差的对比，$err_{0/1}\leq err_{sqr}$，平方误差是0/1误差的上限。从VC维的理论可知
\[
\mbox{classification }E_{out}(\mathbf w)
\leq \mbox{classification }E_{in}(\mathbf w)+\sqrt{\cdots}
\leq \mbox{regression }E_{in}(\mathbf w)+\sqrt{\cdots}，
\]
in-sample回归误差也是out-sample分类误差的上限，做好in-sample回归误差也是做好in-sample分类误差的一种方法，in-sample回归误差很小时能保证out-sample分类误差也很小。由此可见，可用线性回归解决分类问题。</p>

<p>也可直接将回归问题视为分类问题，只是用$err_{sqr}$当作$\widehat{err}$作为$err_{0/1}$误差的近似。为分类问题选择稍宽松的误差上界，这样容易求解参数。</p>

<p>在很多时候，用线性回归解决分类问题效果尚可。如果要让效果更好，可将$\mathbf w_{LIN}$当做PLA或pocket算法的初始值$\mathbf w_0$，加速PLA或pocket算法。</p>

<h2 id="section-4">多项式回归</h2>

<p>构造多项式特征，利用线性回归模型解决非线性问题，称为<strong>多项式回归</strong>（polynomial regression）。例如利用$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $，<a href="/2015/01/nonlinear-transformation">构造新的特征向量</a>$\mathbf x$，带入线性回归模型\eqref{eq:linear-regression-model}求解。从另一个角度看，当特征是多项式时，可直接利用线性模型求解。</p>

<p>当对特征进行高次多项式变换后，取值范围可能急剧变化，需要对多项式特征进行尺度归一化处理<a href="#ng_ml_rlrbv_pe_2014">[3, P. 8]</a>。</p>

<h2 id="section-5">参考资料</h2>

<ol class="bibliography"><li><span id="lin_mlf_linreg_2014">[1]H.-T. Lin, “Lecture 9: Linear Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_linreg_2014">[2]A. Ng, “Linear Regression with multiple variables.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_rlrbv_pe_2014">[3]A. Ng, “Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-6">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:how-to-regularize">
      <p>如何进行正则化？ <a href="#fnref:how-to-regularize" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pinv-vs-inv">
      <p>Matlab的<code>pinv</code>和<code>inv</code>有何区别？ <a href="#fnref:pinv-vs-inv" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:some-E-in">
      <p>为什么VC维考察的是个别的$E_{in}$？ <a href="#fnref:some-E-in" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:hat-matrix-properties">
      <p>帽矩阵的性质（可从文中图示的角度理解）：（1）$\mathbf H$是对称的；（2）$\mathbf H^2=\mathbf H$；（3）$(\mathbf I-\mathbf H)^2=\mathbf I-\mathbf H$。 <a href="#fnref:hat-matrix-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-simultaneous-update">
      <p>事实上，不同时更新的情况很少发生，因为在更新每个$w_i$前，已经用$\mathbf w$计算过了$E_{in}(\mathbf w)$，更新过程中，不再需要重复计算$E_{in}(\mathbf w)$。 <a href="#fnref:andrew-simultaneous-update" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:no-local-optima">
      <p>这是真的吗？ <a href="#fnref:no-local-optima" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-scale">
      <p>为什么解析方法不需要规范化特征？ <a href="#fnref:why-not-scale" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="right">
<p>
<img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="width: 200px">
</p>
<br/>
<span>
  <a  href="/2015/01/support-vector-regression" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/01/blending-and-bagging" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2015/01/linear-regression" data-url="http://qianjiye.de/2015/01/linear-regression" data-title="线性回归">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/01/support-vector-regression'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/01/blending-and-bagging'; --1> -->
<!--         url = '/2015/01/blending-and-bagging'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/01/linear-regression/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
