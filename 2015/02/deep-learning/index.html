<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>深度学习
    </title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>

  <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
          (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
          e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

      _st('install','RFsGWEn74xUkvs4V5QRD');
  </script>

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>深度学习</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2015-02-04">2015-02-04</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#深度学习" title="深度学习">深度学习</a>&nbsp;
  
  <a href="/tags/#神经网络" title="神经网络">神经网络</a>&nbsp;
  
  <a href="/tags/#特征学习" title="特征学习">特征学习</a>&nbsp;
  
  <a href="/tags/#异常检测" title="异常检测">异常检测</a>&nbsp;
  
  <a href="/tags/#密度估计" title="密度估计">密度估计</a>&nbsp;
  
  <a href="/tags/#正则化" title="正则化">正则化</a>&nbsp;
  
  <a href="/tags/#降维" title="降维">降维</a>&nbsp;
  
  <a href="/tags/#主成分分析" title="主成分分析">主成分分析</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">深度神经网络</h2>

<p>确定神经网络的结构是非常核心也是非常困难的问题。</p>

<p>需要多少神经元？网络多少层？神经元之间如何连接？……这些选择一方面凭主观意愿，另一方面通过验证的方法确定。</p>

<p>浅层（shallow）神经网络只有少量的隐层，深度（deep）神经网络有很多层，它们之间的对比如下：</p>

<table>
  <thead>
    <tr>
      <th>浅层神经网络</th>
      <th>深度神经网络</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>✅训练较高效</td>
      <td>❌训练极具挑战性</td>
    </tr>
    <tr>
      <td>✅结构简单</td>
      <td>❌结构复杂</td>
    </tr>
    <tr>
      <td>✅非常强大</td>
      <td>✅非常强大</td>
    </tr>
    <tr>
      <td> </td>
      <td>✅能提取有意义的特征</td>
    </tr>
  </tbody>
</table>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png"><img src="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png" alt="深度学习提取有意义的特征" /></a><div class="caption">图 1:  深度学习提取有意义的特征 [<a href="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png">PNG</a>]</div></div></div>

<p>若只有一层，它需要完成的任务就很复杂。层数增多，每层或层与层之间功能简单，如上图所示，只需从输入中提取简单的笔画，若还有下一层，继续从简单笔画中找出更复杂的笔画。层与层之间，实现从简单到复杂的特征转换。若原始特征是raw feature（每个特征物理意义有限，比如图像的每个像素），最终需要完成复杂的分类动作，深度学习处理这样的问题更自然。</p>

<h4 id="section-1">一、深度学习的挑战与关键技术</h4>

<ul>
  <li>决定网络结构很困难。
    <ul>
      <li>验证能帮忙但并非易事；</li>
      <li>借助领域知识（domain knowledge），比如用于图像的convolutional NNet，像素物理位置有意义，相邻像素连接到下层同一神经元可描述更高阶的特征，相距太远的像素不连接在一起。</li>
    </ul>
  </li>
  <li>模型复杂度很高。
    <ul>
      <li>数据足够多时模型复杂度不是问题，但计算复杂度会更高；</li>
      <li>利用正则化（在某些神经元坏掉时，dropout能使网络很好工作；输入出问题时，denoising能使网络很好工作）。</li>
    </ul>
  </li>
  <li>最优化很困难。局部极小值比神经网络更容易出现。
    <ul>
      <li>精心初始化，避免陷入糟糕的局部极小值（利用pre-training）。</li>
    </ul>
  </li>
  <li>计算复杂度很高，尤其是面对大数据的时候。
    <ul>
      <li>利用新的硬件软件架构，比如mini-batch+GPU。</li>
    </ul>
  </li>
</ul>

<p>其中，正则化和初始化是尤为关键的两项技术。</p>

<blockquote>
  <h4 id="section-2">二、深度学习两步走策略</h4>
  <hr />

  <ol>
    <li>预训练：对$\ell=1,\dots,L$，假设$\mathbf w_*^{(1)},\ldots\mathbf w_*^{(\ell-1)}$已知，逐层训练权值$\left\{w_{ij}^{(\ell)}\right\}$。</li>
    <li>训练：利用BP算法在预训练的基础上调整（fine-tune）权值$\left\{w_{ij}^{(\ell)}\right\}$。</li>
  </ol>
</blockquote>

<h2 id="section-3">非线性自编码器：神经网络</h2>

<p>权值的作用是进行特征转换，将数据换成另一种表现形式，也就是编码（encoding）。在预训练深度神经网络时，并不清楚当前层的权重对以后层有何影响。因此，好的权值需要能保持信息（information-preserving），不同层的权值以不同的形式表示信息。保持信息的意思是数据经过编码之后能重建或精确解码（decode accurately）出原来的信息。预训练权值就是保持信息的编码。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png"><img src="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png" alt="自编码器（保持信息的神经网络）" /></a><div class="caption">图 2:  自编码器（保持信息的神经网络） [<a href="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png">PNG</a>]</div></div></div>

<p>上图的$d-\tilde d-d$结构的神经网络力求输出与输入一致，$g(\mathbf x) = \mathbf x$，这种保持信息的神经网络称为<strong>自编码器</strong>（autoencoder），它的目的是做函数到它本身的逼近（approximate identity function）。$w_{ij}^{(1)}$称为编码权值，$w_{ji}^{(2)}$称为解码权值。</p>

<p>自编码器的变换$g(\mathbf x) = \mathbf x$采用了数据集上的隐含结构（hidden structure），其价值在于：</p>

<ul>
  <li>对于监督学习：$\mathbf x$的隐含结构（权值）很好的保持了信息，可用作特征变换$\Phi(\mathbf x)$。——informative representation of data</li>
  <li>对于非监督学习：（1）密度估计（density estimation）：稠密的区域$g(\mathbf x)\approx\mathbf x$效果好<sup id="fnref:why-density-estimate"><a href="#fn:why-density-estimate" class="footnote">1</a></sup>，新的数据若表现好则位于稠密区域；（2）异常检测（outlier detection）：异常点的$g(\mathbf x)\ne\mathbf x$。——typical representation of data</li>
</ul>

<p>自编码器通过学习identity function得到了数据的表现形式。</p>

<p>基本的自编码器就是$d-\tilde d-d$结构的神经网络，误差函数为
$
\sum_{i=1}^d(g_i(\mathbf x)-x_i)^2。
$
这是浅层网络，可以容易利用BP算法训练。通常情况$\tilde d&lt;d$，这是数据的压缩表示。数据集是$\{(\mathbf x_1, \mathbf y_1=\mathbf x_1),(\mathbf x_2,\mathbf y_2=\mathbf x_2),\ldots,(\mathbf x_N, \mathbf y_N=\mathbf x_N)\}$，因此这也被视为非监督学习。有时会用约束条件$w_{ij}^{(1)}=w_{ji}^{(2)}$让网络更简单，也就是进行正则化，这也会让算法更复杂。</p>

<p>在深度学习中，采用基本的自编码器在数据集$\left\{\mathbf x_n^{(\ell-1)}\right\}$上训练（其中$\tilde d=d^{(\ell)}$），将自编码器权值$\left\{w_{ij}^{(1)}\right\}$作为深度神经网络预训练的权值$\left\{w_{ij}^{(\ell)}\right\}$。</p>

<p>许多成功的预训练技术，利用不同的网络结构或正则化机制等方法，实现了更精妙的自编码器。</p>

<h2 id="section-4">去噪自编码器：正则化</h2>

<p>复杂的神经网络模型复杂度高，需要利用正则化避免过拟合。通常采用的正则化技术包括：</p>

<ul>
  <li>选择简单的网络结构；</li>
  <li><a href="/2015/02/neural-network/#regularization-weighted-methods">weight-decay或weight-elimination正则化</a>；</li>
  <li><a href="/2015/02/neural-network/#regularization-early-stopping">尽早停止迭代</a>。</li>
</ul>

<p>深度学习和自编码器采用了不同的正则化方法。</p>

<p>噪声是导致过拟合的重要原因，<a href="/2015/01/hazard-of-overfitting/#overfitting-illustration">数据越少噪声越多时，越容易过拟合</a>。当模型和数据确定时，去噪是避免过拟合的正则化方法。最直接的去噪技术可以采用<a href="/2015/01/hazard-of-overfitting/#data-cleaning">数据清洗或者数据剪枝</a>。能否反其道而行之，加入噪声？✅</p>

<p>对于鲁棒的自编码器，不仅能对原始数据$\mathbf x$有$g(\mathbf x)\approx \mathbf x$，而且对噪声数据$\tilde{\mathbf x}$有$g(\tilde{\mathbf x})\approx \mathbf x$，这就是<strong>去噪自编码器</strong>（denoising autoencoder）的任务。在数据集$\{(\tilde{\mathbf x}_1, \mathbf y_1=\mathbf x_1),(\tilde{\mathbf x}_2,\mathbf y_2=\mathbf x_2),\ldots,(\tilde{\mathbf x}_N, \mathbf y_N=\mathbf x_N)\}$训练自编码器，其中$\tilde{\mathbf x}_n$是对${\mathbf x}_n$加入了人工噪声的数据。在图像处理领域，$g(\tilde{\mathbf x})$可以得到$\tilde{\mathbf x}$的去噪版本。</p>

<p>在有噪声的数据集上训练出正则化的$g$具备抗噪的能力，这种正则化方法在神经网络模型中非常实用<sup id="fnref:data-hinting"><a href="#fn:data-hinting" class="footnote">2</a></sup>。</p>

<h2 id="linear-autoencoder">线性自编码器／主成分分析</h2>

<p>神经网络是复杂的非线性自编码器，能否利用简单高效且不易过拟合的线性自编码器呢？✅</p>

<p>对于第$k$个元素的线性模型
\[
h_k(\mathbf x)=\sum_{j=0}^{\tilde d}w_{jk}^{(2)}\left(
\sum_{i=0}^{d}w_{ij}^{(1)}x_i
\right)，
\]
加入限制条件：</p>

<ul>
  <li>去除掉常数项$x_0$，使$i$和$k$取值范围相同；</li>
  <li>加入正则化约束$w_{ij}^{(1)}=w_{ji}^{(2)}=w_{ij}$，$\mathbf W=[w_{ij}]$是$d\times\tilde d$的权值矩阵；</li>
  <li>$\tilde d&lt;d$；</li>
</ul>

<p>可得
\[
h_k(\mathbf x)=\sum_{j=0}^{\tilde d}w_{kj}\left(
\sum_{i=1}^{d}w_{ij}x_i
\right)。
\]</p>

<p>因此，线性自编码器的假设（hypothesis）可以表示为
\begin{equation}
h(\mathbf x)=\mathbf W\mathbf W^\top\mathbf x。
\end{equation}
好的自编码器就是对$\mathbf W$做最优化
\[
\mathbf W=\arg\min_{\mathbf W} E_{in}(\mathbf h)=E_{in}(\mathbf W)=
{1\over N}\sum_{n=1}^N\left\lVert\mathbf x_n-\mathbf W\mathbf W^\top\mathbf x_n\right\rVert^2，
\]
这是$w_{ij}$的4次多项式，不易得到解析解。</p>

<p>$\mathbf W\mathbf W^\top$是半正定矩阵，进行特征值分解$\mathbf W\mathbf W^\top=\mathbf V\boldsymbol\Gamma\mathbf V^\top$：</p>

<ul>
  <li>$d\times d$的矩阵$\mathbf V$是正交的（orthogonal）$\mathbf V\mathbf V^\top=\mathbf V^\top\mathbf V=\mathbf I_d$；</li>
  <li>$d\times d$的对角（diagonal）矩阵$\boldsymbol\Gamma$非零对角线元素最多$\tilde d$个。</li>
</ul>

<p>由此可得$\mathbf W\mathbf W^\top\mathbf x_n=\mathbf V\boldsymbol\Gamma\mathbf V^\top\mathbf x_n$：</p>

<ul>
  <li>$\mathbf V^\top\mathbf x_n$表示对$\mathbf x_n$的坐标转换，几何上看是一种旋转或镜射（rotate or reflect），$\mathbf x_n$的长度不会改变；</li>
  <li>$\boldsymbol\Gamma(\cdots)$中$\boldsymbol\Gamma$对角线有$d-\tilde d$个0元素，其余的非0元素进行尺度缩放；</li>
  <li>$\mathbf V(\cdots)$表示还原到原始坐标系。</li>
</ul>

<p>由于$\mathbf x_n=\mathbf V\mathbf I\mathbf V^\top\mathbf x_n$，最优化问题可变为
\[
\min_{\mathbf V}\min_{\boldsymbol\Gamma}{1\over N}\sum_{n=1}^N\left\lVert\mathbf V\mathbf I\mathbf V^\top\mathbf x_n-\mathbf V\boldsymbol\Gamma\mathbf V^\top\mathbf x_n\right\rVert^2，
\]
可先对$\boldsymbol\Gamma$最优化再对$\mathbf V$最优化，并且$\mathbf V$不影响向量在变换过程中的长度，可省去$\mathbf V$。最优化的形式可记为$\min_{\boldsymbol\Gamma}\sum\lVert(\mathbf I-\boldsymbol\Gamma)(\cdots)\rVert^2$，这需要$\mathbf I-\boldsymbol\Gamma$的0元素越多越好，也就是$\boldsymbol\Gamma$的对角线1越多越好。由于$\boldsymbol\Gamma$的秩不超过$\tilde d$，$\boldsymbol\Gamma$的对角线最多$\tilde d$个1，$\boldsymbol\Gamma$的最佳形式为
\[
\boldsymbol\Gamma=
\left[
\begin{aligned}
\mathbf I_{\tilde d}&amp;\quad 0\\
0&amp;\quad 0
\end{aligned}
\right]。
\]
接下来对$\mathbf V$优化
\[
\min_{\mathbf V}
\sum_{n=1}^N
\left\lVert
\left[
\begin{aligned}
0&amp;\quad 0\\
0&amp;\quad \mathbf I_{d-\tilde d}
\end{aligned}
\right]
\mathbf V^\top\mathbf x_n
\right\rVert^2，
\]
上式的意思是保留$\mathbf V^\top\mathbf x_n$的$d-\tilde d$个维度实现最小化，这可以通过最大化实现
\[
\max_{\mathbf V}
\sum_{n=1}^N
\left\lVert
\left[
\begin{aligned}
\mathbf I_{\tilde d}&amp;\quad 0\\
0&amp;\quad 0
\end{aligned}
\right]
\mathbf V^\top\mathbf x_n
\right\rVert^2。
\]
当$\tilde d=1$时，只有$\mathbf V^\top$的第一行$\mathbf v^\top$参与优化，也就是
\[
\max_{\mathbf v}\sum_{n=1}^N\mathbf v^\top\mathbf x_n\mathbf x_n^\top\mathbf v\quad\mbox{ s.t. }\mathbf v^\top\mathbf v=1，
\]
最佳的$\mathbf v$满足$\sum_{n=1}^N\mathbf x_n\mathbf x_n^\top\mathbf v=\lambda\mathbf v$（$\lambda$是拉格朗日乘子）<sup id="fnref:get-optimal-v"><a href="#fn:get-optimal-v" class="footnote">3</a></sup>，此时最佳目标值为$\lambda$，这个最佳的$\mathbf v$是$\mathbf X^\top\mathbf X$的“最大”（topmost）特征向量。一般地，$\{\mathbf v_j\}_{j=1}^{\tilde d}$是$\mathbf X^\top\mathbf X$“最大”的$\tilde d$个特征向量。</p>

<p>最佳的$\{\mathbf w_j\}$就是$\mathbf X^\top\mathbf X$“最大”的特征向量。线性自编码器就是投影到最符合数据$\{\mathbf x_n\}$的正交模式（orthogonal pattern）$\mathbf w_j$。</p>

<p>线性编码器的目标是最大化$\sum(\mbox{maginitude after projection})^2$；主成分分析（PCA，principal component analysis）的目标是最大化$\sum(\mbox{variance after projection})$，变化量$\mbox{variance}$是相对于平均数差距的平方。线性编码器和主成分分析都是线性降维方法，主成分分析＝数据0均值化 ＋ 线性自编码器，在降维中应用更普遍。</p>

<blockquote>
  <h4 id="section-5">线性编码器／主成分分析</h4>
  <hr />

  <ol>
    <li>令$\bar{\mathbf x}={1\over N}\sum_{n=1}^N\mathbf x_n$，$\mathbf x_n\leftarrow\mathbf x_n-\bar{\mathbf x}$<sup id="fnref:zero-mean-vector"><a href="#fn:zero-mean-vector" class="footnote">4</a></sup>；</li>
    <li>计算$\mathbf X^\top\mathbf X$的最大$\tilde d$个特征向量$\mathbf w_1,\mathbf w_2,\ldots,\mathbf w_{\tilde d}$；</li>
    <li>返回转换后的特征$\Phi(\mathbf x)=\mathbf W(\mathbf x-\bar{\mathbf x})$。</li>
  </ol>
</blockquote>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:why-density-estimate">
      <p>为啥稠密区域效果好？ <a href="#fnref:why-density-estimate" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:data-hinting">
      <p>可视为<a href="/2015/02/neural-network/#data-hinting">data hinting</a>的正则化技术。 <a href="#fnref:data-hinting" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:get-optimal-v">
      <p>利用拉格朗日乘子法求导可得。 <a href="#fnref:get-optimal-v" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:zero-mean-vector">
      <p>在0均值数据集基础上进行线性编码，自然实现了投影方向上变化最大。 <a href="#fnref:zero-mean-vector" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="right">
<p>
<img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px">
</p>
<br/>
<span>
  <a  href="/2015/02/neural-network" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/02/radial-basis-function-network" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2015/02/deep-learning" data-url="http://qianjiye.de/2015/02/deep-learning" data-title="深度学习">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/02/neural-network'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/02/radial-basis-function-network'; --1> -->
<!--         url = '/2015/02/radial-basis-function-network'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/02/deep-learning/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
