<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>神经网络
    </title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />

    <script type="text/javascript" src="/assets/js/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="/assets/js/outliner.js"></script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->
  <script type="text/javascript">
    function setTimeSpan(){
    	var date = new Date();
    	timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
    }

    Date.prototype.format = function(format)
		{
    var o =
    	{
    	    "M+" : this.getMonth()+1, //month
    	    "d+" : this.getDate(),    //day
    	    "h+" : this.getHours(),   //hour
    	    "m+" : this.getMinutes(), //minute
    	    "s+" : this.getSeconds(), //second
    	    "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
    	    "S" : this.getMilliseconds() //millisecond
    	}
    	if(/(y+)/.test(format))
    	format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
    	for(var k in o)
    	if(new RegExp("("+ k +")").test(format))
    	format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
    	return format;
		}
  </script>

  <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
          (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
          e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

      _st('install','RFsGWEn74xUkvs4V5QRD');
  </script>

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>神经网络</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="Logs" href="/logs/">Logs</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Subscribe" href="/feed/">Subscribe</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <section class="meta">
<span class="time">
  <time datetime="2015-02-04">2015-02-04</time>
</span>

 |
<span class="categories">
  categories
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  tags
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#神经网络" title="神经网络">神经网络</a>&nbsp;
  
  <a href="/tags/#特征学习" title="特征学习">特征学习</a>&nbsp;
  
  <a href="/tags/#正则化" title="正则化">正则化</a>&nbsp;
  
  <a href="/tags/#BP算法" title="BP算法">BP算法</a>&nbsp;
  
  <a href="/tags/#R" title="R">R</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">感知器融合</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png" alt="感知器的线性融合" /></a><div class="caption">图 1:  感知器的线性融合 [<a href="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png">PNG</a>]</div></div></div>

<p>上图展示了感知器的线性融合结构，其中包含两层的权重$\mathbf w_t$和$\boldsymbol\alpha$，还包含两层的符号函数（sign function）$g_t$和$G$。这样的融合可以表示什么样的分类边界呢？</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-and-xor-operator.png"><img src="/assets/images/2015-02-04-neural-network-and-xor-operator.png" alt="AND和XOR运算" /></a><div class="caption">图 2:  AND和XOR运算 [<a href="/assets/images/2015-02-04-neural-network-and-xor-operator.png">PNG</a>]</div></div></div>

<p>单层感知器可以实现如上图所示的AND运算（+1表示TRUE，-1表示FALSE）
\[
G(\mathbf x)=\mbox{sign}(-1+g_1(\mathbf x)+g_2(\mathbf x))，
\]
也可实现OR和NOT运算。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-enough-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-enough-perceptrons.png" alt="足够多感知器的融合" /></a><div class="caption">图 3:  足够多感知器的融合 [<a href="/assets/images/2015-02-04-neural-network-enough-perceptrons.png">PNG</a>]</div></div></div>

<p>即使感知器的线性组合，分类能力也很强大，如上图所示。只要对足够多的感知器融合，可以在空间中切割出任意凸集（convex set），但$d_{VC}\rightarrow\infty$。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png" alt="两层感知器的线性融合实现XOR运算" /></a><div class="caption">图 4:  两层感知器的线性融合实现XOR运算 [<a href="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png">PNG</a>]</div></div></div>

<p>但是，单层感知器不能实现XOR运算，经过XOR运算的特征转换$\phi(\mathbf x)=(g_1(\mathbf x),g_2(\mathbf x))$后数据线性不可分。线性不可分数据继续特征转换，最终将XOR用AND和OR实现
\[
XOR(g_1,g_2)=OR(AND(-g_1,g_2),AND(g_1,-g_2))，
\]
并且可以用上图所示的两层级连结构表示<sup id="fnref:five-input-xor"><a href="#fn:five-input-xor" class="footnote">1</a></sup>。</p>

<p>由此可见，虽然感知器算法简单，经过线性融合，以及多层级连，可以得到功能强大的分类器。多层感知器就是基本的神经网络结构。</p>

<h2 id="section-1">神经网络</h2>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-simple-NN.png"><img src="/assets/images/2015-02-04-neural-network-simple-NN.png" alt="简单的神经网络模型" /></a><div class="caption">图 5:  简单的神经网络模型 [<a href="/assets/images/2015-02-04-neural-network-simple-NN.png">PNG</a>]</div></div></div>

<p>事实上，对最后一层OUTPUT神经元，除了用感知器外，可以采用其它的<a href="/2015/01/linear-models-for-classification/#linear-models">线性模型</a>。若要分类，OUTPUT采用线性分类模型；若要回归分析，OUTPUT采用线性回归（不做任何处理）；若要soft分类，OUTPUT采用logistic回归。</p>

<p>中间层神经元采用的转换函数（transformation function），除了使用阶梯（符号）函数，也可采用其它转换函数。若所有神经元都采用线性回归，整个网络都是线性运算，用一个线性模型就可以实现。因此，很少用线性回归作为转换函数。阶梯函数是离散的，难以通过最优化求解$\mathbf w$，也很少使用。通常使用的是S形的转换函数
\[
\tanh(s)={\exp(s)-\exp(-s)\over\exp(s)+\exp(-s)}=2\theta(2s)-1，
\]
可以通过<a href="/2015/01/logistic-regression/#mjx-eqn-eqsigmoid-function">logistic函数</a>得到。该函数是阶梯函数的近似，且容易优化，传说和生物神经元也相近。</p>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-common-NN.png"><img src="/assets/images/2015-02-04-neural-network-common-NN.png" alt="常用的神经网络模型" /></a><div class="caption">图 6:  常用的神经网络模型 [<a href="/assets/images/2015-02-04-neural-network-common-NN.png">PNG</a>]</div></div></div>

<p>常用的神经网络采用$\tanh$作为转换函数，输出采用线性回归，如上图所示。$d^{(0)},d^{(1)},\ldots,d^{(L)}$表示每层神经元数目（节点数目），每层的权值为
\[
w_{ij}^{(\ell)}:\left\{
\begin{aligned}
&amp;1\leq\ell\leq L&amp;\mbox{layers}&amp;\\
&amp;0\leq i\leq d^{(\ell-1)}&amp;\mbox{inputs}&amp;\\
&amp;1\leq j\leq d^{(\ell)}&amp;\mbox{outputs}&amp;，
\end{aligned}
\right.
\]
常数+1的神经元相当于偏移项，评分函数为
\begin{equation}
s_j^{(\ell)}=\sum_{i=0}^{d^{(\ell-1)}}w_{ij}^{(\ell)}x_i^{(\ell-1)}，
\end{equation}
转换后的特征为
\begin{equation}
x_j^{(\ell)}=\left\{
\begin{aligned}
&amp;\tanh\left(s_j^{(\ell)}\right)&amp;\mbox{if }\ell&lt;L\\
&amp;s_j^{(\ell)}&amp;\mbox{if }\ell=L
\end{aligned}
\right.
\label{eq:forward-x}
\end{equation}</p>

<p>神经网络将$\mathbf x$当作输入层$\mathbf x^{(0)}$，隐层计算变换后的特征$\mathbf x^{(\ell)}$，输出层计算预测结果$x_1^{(L)}$。</p>

<p>神经网络隐层相当于模式（特征）提取（pattern extraction），进行$\mathbf x^{(\ell)}$和权值向量的模式匹配（利用基于内积的余弦相似度），每个神经元提取一种特征，权值向量纪录了从数据中学到的模式。</p>

<h2 id="bp">BP算法</h2>

<p>如何通过最小化$E_{in}\left(\left\{w_{ij}^{(\ell)}\right\}\right)$学到权值$\left\{w_{ij}^{(\ell)}\right\}$？</p>

<p>如果只有一个隐层，神经网络相当于感知器的融合，可以通过GradientBoost方法一个接一个的确定隐层的神经元。如果有多个隐层，问题就变得复杂了。</p>

<p>每个数据的误差记为
\[
e_n=(y_n-\mbox{NNet}(\mathbf x_n))^2，
\]
若能计算$\partial e_n\over \partial w_{ij}^{(\ell)}$，就可以通过梯度下降法求解。对输出层
\[
e_n=\left(y_n-s_1^{(L)}\right)^2=\left(y_n-\sum_{i=0}^{d^{(L-1)}}w_{i1}^{(L)}x_i^{(L-1)}\right)^2，
\]
对输出层和其它层分别求偏微分
\[
\begin{aligned}
&amp;{\partial e_n\over\partial w_{i1}^{(L)}}
={\partial e_n\over\partial s_{1}^{(L)}}\cdot{\partial s_{1}^{(L)}\over\partial w_{i1}^{(L)}}
=-2\left(y_n-s_1^{(L)}\right)\cdot x_i^{(L-1)}，\\
&amp;{\partial e_n\over\partial w_{ij}^{(\ell)}}
={\partial e_n\over\partial s_{j}^{(\ell)}}\cdot{\partial s_{j}^{(\ell)}\over\partial w_{ij}^{(\ell)}}
=x_i^{(\ell-1)}\delta_j^{(\ell)}。
\end{aligned}
\]
对输出层，令<sup id="fnref:tanh_output"><a href="#fn:tanh_output" class="footnote">2</a></sup>
\begin{equation}
\delta_1^{(L)}=-2\left(y_n-s_1^{(L)}\right)。
\label{eq:backpropagation-delta-L}
\end{equation}
对其它层
\begin{equation}
\delta_j^{(\ell)}
={\partial e_n\over\partial s_{j}^{(\ell)}}
=\sum_{k=1}^{d^{(\ell+1)}}{\partial e_n\over\partial s_{k}^{(\ell+1)}}{\partial s_{k}^{(\ell+1)}\over\partial x_{j}^{(\ell)}}{\partial x_{j}^{(\ell)}\over\partial s_{j}^{(\ell)}}
=\tanh’\left(s_j^{(\ell)}\right)\sum_{k=1}^{d^{(\ell+1)}}\delta_k^{(\ell+1)}w_{jk}^{(\ell+1)}，
\label{eq:backpropagation-delta2}
\end{equation}
也就是前一层的$\delta_j^{(\ell)}$，可以通过后一层$\delta_k^{(\ell+1)}$回推计算，<a href="http://www.wolframalpha.com/input/?i=d%2Fdx+tanh">其中</a>
\[
\tanh’(s)=1-\tanh^2(s)=\left({2\over \exp(s) + \exp(-s)}\right)^2，
\]
那么
\begin{equation}
\delta_j^{(\ell)}
=\left(1-\left(x_j^{(\ell)}\right)^2\right)\sum_{k=1}^{d^{(\ell+1)}}\delta_k^{(\ell+1)}w_{jk}^{(\ell+1)},\quad(j\geq 1)。
\label{eq:backpropagation-delta}
\end{equation}</p>

<blockquote>
  <h4 id="bpbackpropagation">BP（backpropagation）算法</h4>
  <hr />
  <p>用小的随机值初始化所有的$w_{ij}^{(\ell)}$；</p>

  <p>对于$t=1,2,\ldots,T$，循环执行： </p>

  <ol>
    <li>随机化：随机选取$n\in\{1,2,\ldots,N\}$；</li>
    <li>前向传播：从$\mathbf x^{(0)}=\mathbf x_n$开始，利用\eqref{eq:forward-x}计算所有$x_i^{(\ell)}$；</li>
    <li>误差回传：对$\mathbf x^{(0)}=\mathbf x_n$，利用\eqref{eq:backpropagation-delta-L}和\eqref{eq:backpropagation-delta}计算所有$\delta_j^{(\ell)}$；</li>
    <li>梯度下降：$w_{ij}^{(\ell)}\leftarrow w_{ij}^{(\ell)}-\eta x_i^{(\ell-1)}\delta_j^{(\ell)}$；</li>
  </ol>

  <p>返回$g_{\mbox{NNET}}(\mathbf x)=\left(\ldots\tanh\left(\sum_jw_{jk}^{(2)}\cdot\tanh\left(\sum_iw_{ij}^{(1)}x_i\right)\right)\right)$。</p>

  <p>在实际应用中，第1步至第3步可以先执行多次后，再用$x_i^{(\ell-1)}\delta_j^{(\ell)}$的平均值执行第4步的更新，这就是mini-batch的方法。</p>
</blockquote>

<p>神经网络通过最小化
\[
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^Nerr\left(\left(\ldots\tanh\left(\sum_jw_{jk}^{(2)}\cdot\tanh\left(\sum_iw_{ij}^{(1)}x_i\right)\right)\right),y_n\right)
\]
计算权值。通常多隐层神经网络的误差函数是非凸的（non-convex），难以达到全局最小值（global minimum），梯度下降法通过BP算法也仅仅得到局部极小值（local minimum）。不同的初始化$w_{ij}^{(\ell)}$，会得到不同的局部极值：</p>

<ul>
  <li>BP算法对权重初始值敏感；</li>
  <li>若权值太大，会落到$\tanh$的saturate区域（梯度很小），每次按梯度更新很小；</li>
  <li>用小的随机值初始化权值$w_{ij}^{(\ell)}$。</li>
</ul>

<p>若初始化$w_{ij}^{(\ell)}＝0$，由于$x_0^{(\ell)}=1$，除了${\partial e_n\over \partial w_{01}^{(L)}}\neq 0$外，其它的导数都为0；若初始化$w_{ij}^{(\ell)}＝1$，那么$w_{ij}^{(1)}=w_{i(j+1)}^{(1)}$。因此，$w_{ij}^{(\ell)}$不能初始化为0和1。</p>

<p>虽然神经网络很难最优化，但在实际中很有用。</p>

<h2 id="regularization">正则化</h2>

<p>若用形如$\tanh$的转换函数，神经网络的$d_{VC}=O(VD)$，$V$为神经元数量，$D$为神经元之间的权值数量。当神经元数目足够时，可以做任意逼近，也更容易导致过拟合。为了避免过拟合，需要采取正则化方法。</p>

<h4 id="regularization-weighted-methods">一、weight-elimination正则化</h4>

<p>常用的方法是基于$L_2$的weight-decay正则化，$\Omega(\mathbf w)=\sum\left(w_{ij}^{(\ell)}\right)^2$。这种正则化对权值的压缩（shrink）力度和权值大小“成比例”，大的权值压缩厉害，小的权值压缩较小。</p>

<p>如果通过正则化使权值部分为0（稀疏），就能有效减小$d_{VC}$，常用的方法是$L_1$正则化，$\Omega(\mathbf w)=\sum\left\lvert w_{ij}^{(\ell)}\right\rvert$，但是不可微。采用weight-elimination正则化（放缩的$L_2$正则化），大的权值中等幅度的压缩（median shrink），小的权值也中等幅度的压缩，小的权值就会接近0，具有权值稀疏化的效果。weight-elimination正则化采用
\begin{equation}
\Omega(\mathbf w)=\sum{\left(w_{ij}^{(\ell)}\right)^2\over 1+\left(w_{ij}^{(\ell)}\right)^2}，
\end{equation}
那么
\[
{\partial\Omega(\mathbf w)\over \partial w_{ij}^{(\ell)}}
={2w_{ij}^{(\ell)}\over\left(1+\left(w_{ij}^{(\ell)}\right)^2\right)^2}。
\]</p>

<h4 id="regularization-early-stopping">二、尽早停止迭代</h4>

<div class="image_line" id="figure-7"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-early-stopping.png"><img src="/assets/images/2015-02-04-neural-network-early-stopping.png" alt="迭代次数对误差的影响" /></a><div class="caption">图 7:  迭代次数对误差的影响 [<a href="/assets/images/2015-02-04-neural-network-early-stopping.png">PNG</a>]</div></div></div>

<p>迭代的次数$t$越多，选择过的$\mathbf w$也就越多，有效的$d_{VC}$也越大。小的$t$使得$d_{VC}$也较小。尽早停止（early stopping）迭代，通过如上图右的最佳$t^*$，获得如上图左的最佳$d_{VC}^*$，克服过拟合。通过验证（validation）确定停止迭代的参数$t$。</p>

<p>所有和梯度有关的优化算法，都可利用尽早停止迭代的机制，实现某种正则化。</p>

<h2 id="section-2">程序示例</h2>

<div class="highlight"><pre><code class="language-R">nnet_train <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>X<span class="p">,</span> Y<span class="p">,</span> struct<span class="p">,</span> r<span class="p">,</span> eta<span class="p">,</span> TT<span class="p">)</span> <span class="p">{</span>
  <span class="c1"># X -- a matrix with each feature row </span>
  <span class="c1"># Y -- a matrix </span>
  <span class="c1"># struct -- a vector with neuron number of each layer, including the input layer</span>
  <span class="c1"># r -- initialization range</span>
  <span class="c1"># eta -- learning rate</span>
  <span class="c1"># TT -- iteration times</span>
  
  X <span class="o">&lt;-</span> <span class="kp">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> X<span class="p">)</span>
  num_data <span class="o">&lt;-</span> <span class="kp">nrow</span><span class="p">(</span>X<span class="p">)</span>
  num_layer <span class="o">&lt;-</span> <span class="kp">length</span><span class="p">(</span>struct<span class="p">)</span> 
  nnetwork <span class="o">&lt;-</span> Xs <span class="o">&lt;-</span> Deltas <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">()</span>
  
  <span class="c1"># initialization</span>
  <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> <span class="p">(</span>num_layer <span class="o">-</span> <span class="m">1</span><span class="p">))</span> <span class="p">{</span>
    nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kt">matrix</span><span class="p">(</span>runif<span class="p">((</span>struct<span class="p">[</span>i<span class="p">]</span> <span class="o">+</span> <span class="m">1</span><span class="p">)</span> <span class="o">*</span> struct<span class="p">[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">],</span> <span class="o">-</span>r<span class="p">,</span> r<span class="p">),</span>
                            struct<span class="p">[</span>i<span class="p">]</span> <span class="o">+</span> <span class="m">1</span><span class="p">,</span> struct<span class="p">[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">])</span>
  <span class="p">}</span>
  
  <span class="kr">for</span> <span class="p">(</span>t <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> TT<span class="p">)</span> <span class="p">{</span>
    n <span class="o">&lt;-</span> <span class="kp">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span>num_data<span class="p">,</span> <span class="m">1</span><span class="p">)</span>
    
    <span class="c1"># forward</span>
    Xs<span class="p">[[</span><span class="m">1</span><span class="p">]]</span> <span class="o">&lt;-</span> <span class="kp">t</span><span class="p">(</span>X<span class="p">[</span>n<span class="p">,,</span>drop <span class="o">=</span> <span class="bp">F</span><span class="p">])</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">2</span> <span class="o">:</span> num_layer<span class="p">)</span> <span class="p">{</span>
      xx <span class="o">&lt;-</span> <span class="kp">tanh</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span>Xs<span class="p">[[</span>i <span class="o">-</span> <span class="m">1</span><span class="p">]])</span> <span class="o">%*%</span> nnetwork<span class="p">[[</span>i <span class="o">-</span> <span class="m">1</span><span class="p">]]))</span>
      <span class="kp">ifelse</span><span class="p">(</span>i <span class="o">!=</span> num_layer<span class="p">,</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kp">rbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> xx<span class="p">),</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> xx<span class="p">)</span>
    <span class="p">}</span>
    
    <span class="c1"># backward</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> num_layer <span class="o">:</span> <span class="m">2</span><span class="p">)</span> <span class="p">{</span>
      <span class="kp">ifelse</span><span class="p">(</span>i <span class="o">==</span> num_layer<span class="p">,</span>
             Deltas<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> 
               <span class="m">-2</span> <span class="o">*</span> <span class="p">(</span><span class="kp">t</span><span class="p">(</span>Y<span class="p">[</span>n<span class="p">,,</span>drop<span class="o">=</span><span class="bp">F</span><span class="p">])</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]])</span> <span class="o">*</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">^</span> <span class="m">2</span><span class="p">),</span>
             Deltas<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> 
               <span class="p">((</span>nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">%*%</span> Deltas<span class="p">[[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">]])</span> <span class="o">*</span>
                  <span class="p">(</span><span class="m">1</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">^</span> <span class="m">2</span><span class="p">))[</span><span class="m">2</span><span class="o">:</span><span class="kp">nrow</span><span class="p">(</span>nnetwork<span class="p">[[</span>i<span class="p">]]),,</span>drop<span class="o">=</span><span class="bp">F</span><span class="p">])</span>
    <span class="p">}</span>
    
    <span class="c1"># update weight</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> <span class="p">(</span>num_layer <span class="o">-</span> <span class="m">1</span><span class="p">))</span> <span class="p">{</span>
      nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">-</span> eta <span class="o">*</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">%*%</span> <span class="kp">t</span><span class="p">(</span>Deltas<span class="p">[[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">]])</span>
    <span class="p">}</span>
    
  <span class="p">}</span>
  <span class="kr">return</span><span class="p">(</span>nnetwork<span class="p">)</span>
<span class="p">}</span></code></pre></div>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-4">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:five-input-xor">
      <p>如果用神经网络实现$XOR(x_1,\dots,x_5)$，采用结构为5-D-1，那么<a href="https://class.coursera.org/ntumltwo-001/forum/thread?thread_id=243">最小的D</a>是多少？ <a href="#fnref:five-input-xor" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tanh_output">
      <p>对于二分类问题的神经网络，输出为$\{-1,+1\}$，输出层的神经元也要采用$\tanh$，此时$e_n=\left(y_n-\tanh\left(s_1^{(L)}\right)\right)^2$，那么$\delta_1^{(L)}=-2\left(y_n-x_1^{(L)}\right)\left(1-\left(x_1^{(L)}\right)^2\right)$。 <a href="#fnref:tanh_output" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</section>
<section align="right">
<p>
<img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px">
</p>
<br/>
<span>
  <a  href="/2015/02/summary-of-aggregation-models" class="pageNav"  >上一篇</a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/02/deep-learning" class="pageNav"  >下一篇</a>
</span>
</section>

	
	<ul class="ds-recent-visitors"></ul>
	<div class="ds-thread" data-thread-key="/2015/02/neural-network" data-url="http://qianjiye.de/2015/02/neural-network" data-title="神经网络">
	</div>
	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
		
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/02/summary-of-aggregation-models'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/02/deep-learning'; --1> -->
<!--         url = '/2015/02/deep-learning'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/02/neural-network/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2015 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
