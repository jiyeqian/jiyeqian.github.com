<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Jiye Qian" />
    <title>CS231n（4）：最优化</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <link href="/feed/" rel="alternate" title="Jiye Qian" type="application/atom+xml" />
    <link rel="stylesheet" href="/assets/css/style.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default.css" />
    <link rel="stylesheet" href="/assets/css/pygments/default_inline.css" />
    <link rel="stylesheet" href="/assets/css/coderay.css" />
    <link rel="stylesheet" href="/assets/css/twemoji-awesome.css" />  
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
    <link href="/assets/css/jquery-ui-1.10.4.custom.min.css" rel="stylesheet" />
    <link href="/assets/css/ggvis.css" rel="stylesheet" />
    <link href="/assets/css/mermaid.css" rel="stylesheet" />
    <link rel="stylesheet" href="/assets/css/markdown-plus.css"/> 
    <link rel="stylesheet" href="/assets/css/flexslider.css" type="text/css" media="screen" />
      <style type="text/css">
        .flex-caption {
          width: 96%;
          padding: 2%;
          left: 0;
          bottom: 0;
          background: rgba(0,0,0,.5);
          color: #fff;
          text-shadow: 0 -1px 0 rgba(0,0,0,.3);
          font-size: 14px;
          line-height: 18px;
        }
        li.css a {
          border-radius: 0;
        }
      </style>

    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/jquery-ui-1.10.4.custom.min.js"></script>
    <script type="text/javascript" src="/assets/js/d3.min.js"></script>
    <script type="text/javascript" src="/assets/js/vega.min.js"></script>
    <script type="text/javascript" src="/assets/js/lodash.min.js"></script>
    <script>var lodash = _.noConflict();</script>
    <script type="text/javascript" src="/assets/js/ggvis.js"></script>
    <script type="text/javascript" src="/assets/js/htmlwidgets.js"></script>
    <script type="text/javascript" src="/assets/js/echarts-all.js"></script>
    <script type="text/javascript" src="/assets/js/echarts.js"></script>
    <script defer src="/assets/js/jquery.flexslider-min.js"></script>
    <script type="text/javascript">
      // $(function(){
      //   SyntaxHighlighter.all();
      // });
      $(window).load(function(){
        $('.flexslider').flexslider({
          animation: "slide",
          start: function(slider){
            $('body').removeClass('loading');
          }
        });
      });
    </script>

    <script type="text/javascript">
      function setTimeSpan(){
        var date = new Date();
        timeSpan.innerText=date.format('yyyy-MM-dd hh:mm:ss');
      }

      Date.prototype.format = function(format)
      {
        var o =
        {
          "M+" : this.getMonth()+1, //month
          "d+" : this.getDate(),    //day
          "h+" : this.getHours(),   //hour
          "m+" : this.getMinutes(), //minute
          "s+" : this.getSeconds(), //second
          "q+" : Math.floor((this.getMonth()+3)/3),  //quarter
          "S" : this.getMilliseconds() //millisecond
        }
        if(/(y+)/.test(format))
          format=format.replace(RegExp.$1,(this.getFullYear()+"").substr(4 - RegExp.$1.length));
        for(var k in o)
          if(new RegExp("("+ k +")").test(format))
            format = format.replace(RegExp.$1,RegExp.$1.length==1 ? o[k] : ("00"+ o[k]).substr((""+ o[k]).length));
          return format;
        }
      </script>

    <!-- MathJax for LaTeX -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        "HTML-CSS": { extensions: ["handle-floats.js"] },
        TeX: { equationNumbers: { autoNumber: "AMS" } },
        tex2jax: {
            inlineMath: [['$$$', '$$$'], ['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        }
    });
    </script>
    <!-- <script type="text/javascript" src="/assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <!-- <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F0b514f17fd99b9fb4be74c94bdd2b7db' type='text/javascript'%3E%3C/script%3E"));
</script>
 -->
  </head>
<!--  <body>
-->

  <body onLoad="setInterval(setTimeSpan,1000);">
    <div id="container">
      <div id="main" role="main">
        <header>
        <h1>CS231n（4）：最优化</h1>
        </header>
        <nav id="real_nav">
        
          <span><a title="Home" href="/">Home</a></span>
        
          <span><a title="Categories" href="/categories/">Categories</a></span>
        
          <span><a title="Tags" href="/tags/">Tags</a></span>
        
          <span><a title="About" href="/about/">About</a></span>
        
          <span><a title="Search" href="/search/">Search</a></span>
        
        </nav>
        <article class="content">
        <script type="text/javascript" src="/assets/js/outliner.js"></script>

<section class="meta">
<span class="time">
  <time datetime="2015-02-13">2015-02-13</time>
</span>

 |
<span class="categories">
  <i class="fa fa-share-alt"></i>
  
  <a href="/categories/#研究学术" title="研究学术">研究学术</a>&nbsp;
  
</span>


 |
<span class="tags">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/#CNN" title="CNN">CNN</a>&nbsp;
  
  <a href="/tags/#计算机视觉" title="计算机视觉">计算机视觉</a>&nbsp;
  
  <a href="/tags/#机器学习基础" title="机器学习基础">机器学习基础</a>&nbsp;
  
  <a href="/tags/#梯度下降法" title="梯度下降法">梯度下降法</a>&nbsp;
  
  <a href="/tags/#BP算法" title="BP算法">BP算法</a>&nbsp;
  
  <a href="/tags/#Python" title="Python">Python</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="section">理解损失函数</h2>

<p>图像分类的两个关键步骤包括：（1）通过<strong>评分函数</strong>将图像映射到类别评分；（2）通过<strong>损失函数</strong>度量评分。多分类支持向量机采用的是线性评分函数$f\left(\mathbf x_i,\mathbf W\right)=\mathbf W\mathbf x_i$，需要最小化的损失函数为
\[
L={1\over N}\sum_i\sum_{j\neq y_i}
\max\left(
0, f\left(\mathbf x_i,\mathbf W\right)_j-f\left(\mathbf x_i,\mathbf W\right)_{y_i}+1
\right)
+\alpha R(\mathbf W)。
\]</p>

<p>图像分类的第三个关键步骤：（3）通过最优化求解最小化损失函数的参数$\mathbf W$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg"><img src="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg" alt="损失函数图" /></a><div class="caption">图 1:  损失函数图 [<a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg">JPG</a>]</div></div></div>

<p>将$\mathbf W$视为空间中的一个点，损失函数（不含正则化项）可以用上图表示。$\mathbf W$、$\mathbf W_1$和$\mathbf W_2$是随机产生的矩阵。上图左表示损失函数$L(\mathbf W+a\mathbf W_1)$，横轴为$a$，纵轴为$L$；上图中和右表示损失函数$L(\mathbf W+a\mathbf W_1+b\mathbf W_2)$，横轴和纵轴分别为$a$和$b$，越红损失$L$越大，越蓝损失$L$越小。上图左和中，只计算一张图片的损失；上图右的碗状图是100张图片损失的平均值，相当于100张上图中的平均图。</p>

<p>每张图$i$的损失为（不含正则化项）
\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+1
\right)，
\]
它是分段线性（piecewise-linear）函数。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png"><img src="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png" alt="1维损失函数" /></a><div class="caption">图 2:  1维损失函数 [<a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png">PNG</a>]</div></div></div>

<p>三张图片$\mathbf x_0$、$\mathbf x_1$和$\mathbf x_2$分别属于类0、1和2，它们的损失计算如下
\[
\begin{aligned}
L_0 &amp;= \max\left(0, \mathbf w_1^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)\\
L_1 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)\\
L_2 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)+
\max\left(0, \mathbf w_1^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)\\
L &amp;= {1\over 3}\left(L_0+L_1+L_2\right)，
\end{aligned}
\]
如上图所示，横轴表示权值，纵轴表示损失。</p>

<p>多分类SVM的代价函数是凸函数的一个范例，当采用神经网络的评分函数$f$时，代价函数就是非凸的。损失函数不可微分（non-differentiable），因此梯度未定义，通常使用次梯度（subgradient）代替梯度。本文不区分梯度和次梯度。</p>

<p>损失函数可以评估任意权值$\mathbf W$，最优化的目标是找出最小化损失函数的权值$\mathbf W$。神经网络的优化不能方便地使用凸函数的优化工具。</p>

<h2 id="section-1">随机方法</h2>

<h3 id="section-2">随机搜索</h3>

<p>比较糟糕的优化策略是随机搜索（random search）。由于验证参数$\mathbf W$比较简单，随机搜索通过尝试不同的随机权值$\mathbf W$，从中选择最优的。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span> <span class="c"># Python assigns the highest possible float value</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span> <span class="c"># generate random parameters</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the loss over the entire training set</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span> <span class="c"># keep track of the best solution</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="n">bestW</span> <span class="o">=</span> <span class="n">W</span>
  <span class="k">print</span> <span class="s">&#39;in attempt </span><span class="si">%d</span><span class="s"> the loss was </span><span class="si">%f</span><span class="s">, best </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，随机搜索可以做到约15.5%的精度，优于随机猜想的10%。找到最优的$\mathbf W$很困难甚至不可行，但是找到更好一些的$\mathbf W$就不难么困难了。基于这个思路，优化算法可以从随机的$\mathbf W$开始，不断更新权值，使得每次更新都提升一点性能。随机搜索如同徒步者带上眼罩向山脚走。针对CIFAR-10数据集，这山有30730维，山上的每一点都相当于特定的损失值。</p>

<h3 id="section-3">随机局部搜索</h3>

<p>从随机初始化的$\mathbf W$开始，若增加扰动$\delta\mathbf W$，损失在$\mathbf W+\delta\mathbf W$比在$\mathbf W$时更低，那么更新$\mathbf W\leftarrow\mathbf W+\delta\mathbf W$。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># generate random starting W</span>
<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.0001</span>
  <span class="n">Wtry</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">Xtr_cols</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Wtry</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">Wtry</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
  <span class="k">print</span> <span class="s">&#39;iter </span><span class="si">%d</span><span class="s"> loss is </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，精度提高到了21.4%。但这仍然是低效耗时的算法。</p>

<h2 id="section-4">梯度下降法</h2>

<p>随机搜索对寻找最佳的权值改变方向没有帮助。沿着最佳的方向改变权值，在数学上可以保证损失最速下降（steepest descend）。这个方向和梯度（gradient）相关。</p>

<p>一维函数的斜率（slope）是函数值在某点的瞬时（instantaneous）变化率。梯度是斜率在多维空间函数的推广，它是每维斜率构成的向量，通常也称为导数（derivative）。一维函数的导数为
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x)\over h}，
\]
将其推广到多变量函数时称为偏导数（partial derivative）。梯度就是每一维偏导数组成的向量，有两种计算方法：</p>

<ul>
  <li>数值梯度（numerical gradient）：近似计算，较慢，但容易；</li>
  <li>解析梯度（analytic gradient）：计算快，但是容易出错（error-prone）。</li>
</ul>

<h3 id="section-5">数值梯度</h3>

<p>梯度的数值计算方法如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">  a naive implementation of numerical gradient of f at x </span>
<span class="sd">  - f should be a function that takes a single argument</span>
<span class="sd">  - x is the point (numpy array) to evaluate the gradient at</span>
<span class="sd">  &quot;&quot;&quot;</span> 

  <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evaluate function value at original point</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>

  <span class="c"># iterate over all indexes in x</span>
  <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;readwrite&#39;</span><span class="p">])</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

    <span class="c"># evaluate function at x+h</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c"># increment by h</span>
    <span class="n">fxh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evalute f(x + h)</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="c"># restore to previous value (very important!)</span>

    <span class="c"># compute the partial derivative</span>
    <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span> <span class="c"># the slope</span>
    <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c"># step to next dimension</span>

  <span class="k">return</span> <span class="n">grad</span></code></pre></div>
<p>上述代码计算损失函数在$\mathbf x$每个维度的偏导数。在实际应用中通常使用中心差分公式（centered difference formula）
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x-h)\over 2h}。
\]</p>

<p>梯度只表明了最快增长的方向，还需要在这个方向前进的步长（也就是学习率）。步长是神经网络需要设定的重要超参数。</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># to use the generic code above we want a function that takes a single argument</span>
<span class="c"># (the weights in our case) so we close over X_train and Y_train</span>
<span class="k">def</span> <span class="nf">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># random weight vector</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">CIFAR10_loss_fun</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the gradient</span>

<span class="n">loss_original</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c"># the original loss</span>
<span class="k">print</span> <span class="s">&#39;original loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_original</span><span class="p">,</span> <span class="p">)</span>

<span class="c"># lets see the effect of multiple step sizes</span>
<span class="k">for</span> <span class="n">step_size_log</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">step_size_log</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">df</span> <span class="c"># new position in the weight space</span>
  <span class="n">loss_new</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>
  <span class="k">print</span> <span class="s">&#39;for step size </span><span class="si">%f</span><span class="s"> new loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">loss_new</span><span class="p">)</span>

<span class="c"># prints:</span>
<span class="c"># original loss: 2.200718</span>
<span class="c"># for step size 1.000000e-10 new loss: 2.200652</span>
<span class="c"># for step size 1.000000e-09 new loss: 2.200057</span>
<span class="c"># for step size 1.000000e-08 new loss: 2.194116</span>
<span class="c"># for step size 1.000000e-07 new loss: 2.135493</span>
<span class="c"># for step size 1.000000e-06 new loss: 1.647802</span>
<span class="c"># for step size 1.000000e-05 new loss: 2.844355</span>
<span class="c"># for step size 1.000000e-04 new loss: 25.558142</span>
<span class="c"># for step size 1.000000e-03 new loss: 254.086573</span>
<span class="c"># for step size 1.000000e-02 new loss: 2539.370888</span>
<span class="c"># for step size 1.000000e-01 new loss: 25392.214036</span></code></pre></div>
<p>上述代码中，<code>step_size</code>表示步长（学习率），步长小损失函数减小慢，但步长太大损失函数不降反增。当损失函数有30730个参数时，每次更新参数需要计算30731次损失函数，计算复杂度非常高。</p>

<h3 id="section-6">解析梯度</h3>

<p>数值梯度虽简单但耗时，解析梯度计算高效但易错。在实际应用中，采用解析梯度时，通过比较它与数值梯度确定梯度计算是否正确，这称为<strong>梯度校验</strong>（gradient check）。</p>

<p>对每个数据，多分类SVM的损失函数为</p>

<p>\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta
\right)，
\]</p>

<p>对$\mathbf w_{y_i}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{y_i}}L_i=
-\left(\sum_{j\neq y_i}\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\right)\mathbf x_i，
\]</p>

<p>对$\mathbf w_{j}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{j}}L_i=
\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\mathbf x_i。
\]</p>

<p>利用梯度更新参数的方法称为<strong>梯度下降法</strong>（gradient descent），通常的形式：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>梯度下降法是到目前为止最常用的优化神经网络损失函数的方法。</p>

<p>对大数据集上的应用，在整个数据集上计算损失函数的梯度非常耗时，通常从中抽取从中抽取一部分数据计算梯度，这称为mini-batch梯度下降法，例如采用256个样本计算梯度：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Minibatch Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">data_batch</span> <span class="o">=</span> <span class="n">sample_training_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c"># sample 256 examples</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>在实际中，采用mini-batch的方法能提高参数更新频率，更快收敛。若mini-batch梯度下降法只采用一个样本，称为随机梯度下降法（SGD，stochastic gradient descent）或在线（on-line）梯度下降法。但这不太常用，在实际中由于向量化代码的优化，计算100个样本的梯度比1个样本的梯度计算100次高效。虽然人们使用SGD这个称谓，实际通常指的是mini-batch的梯度下降法（MGD／BGD，minbatch/batch gradient descent）。min-batch的样本数量虽是超参数，但很少采用验证法确定，通常根据内存大小来决定，或者直接设定为100左右的值。</p>

<h2 id="bp">BP梯度</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit2.png" alt="BP梯度计算" /></a><div class="caption">图 3:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png">PNG</a>]</div></div></div>

<p>对函数$f(x,y,z)=(x+y)z$，令$q=x+y$，那么根据链式法则（chain rule）有${\partial f\over \partial x}={\partial f\over \partial q}{\partial q\over \partial x}$。梯度${\partial f\over \partial x}$从右到左反向计算如上图“电路”所示，通过相邻节点的局部梯度链式相乘得到，每个节点用门（gate）表示。当${\partial f\over \partial q}=-4$和${\partial q\over \partial x}=1$时，${\partial f\over \partial x}=-4\times 1 = -4$。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit3.png" alt="BP梯度计算" /></a><div class="caption">图 4:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png">PNG</a>]</div></div></div>

<p>当
$
f(\mathbf w,\mathbf x)={1\over 1 + e^{-\left(w_0x_0+w_1x_1+w_2\right)}}
$
时，链式计算如上如所示。链式法则的依据是复合函数的求导法则。门可以是任何可导函数（differentiable function），通常选择导数容易计算的函数作为门，如sigmoid函数$\sigma(x)={1\over 1 + e^{-x}}$。将${df\over dx}$简记为<code>dx</code>，${d\sigma(x)\over dx}=(1-\sigma(x))\sigma(x)$，上图的计算过程简化如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="c"># assume some random weights and data</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>

<span class="c"># forward pass</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">))</span> <span class="c"># sigmoid function</span>

<span class="c"># backward pass through the neuron (backpropagation)</span>
<span class="n">ddot</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">f</span> <span class="c"># gradient on dot variable, using the sigmoid gradient derivation</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into x</span>
<span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into w</span>
<span class="c"># we&#39;re done! we have the gradients on the inputs to the circuit</span></code></pre></div>

<p>对于复杂的函数，一次求导很复杂，如果采用链式法则，可以降低计算复杂度。对函数
\[
f(x,y)={x+\sigma(y)\over\sigma(x)+(x+y)^2}，
\]
前向计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># example values</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="c"># forward pass</span>
<span class="n">sigy</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c"># sigmoid in numerator   #(1)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sigy</span> <span class="c"># numerator                               #(2)</span>
<span class="n">sigx</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c"># sigmoid in denominator #(3)</span>
<span class="n">xpy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                                              <span class="c">#(4)</span>
<span class="n">xpysqr</span> <span class="o">=</span> <span class="n">xpy</span><span class="o">**</span><span class="mi">2</span>                                          <span class="c">#(5)</span>
<span class="n">den</span> <span class="o">=</span> <span class="n">sigx</span> <span class="o">+</span> <span class="n">xpysqr</span> <span class="c"># denominator                        #(6)</span>
<span class="n">invden</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">den</span>                                       <span class="c">#(7)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">num</span> <span class="o">*</span> <span class="n">invden</span> <span class="c"># done!                                 #(8)</span></code></pre></div>

<p>反向梯度计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># backprop f = num * invden</span>
<span class="n">dnum</span> <span class="o">=</span> <span class="n">invden</span> <span class="c"># gradient on numerator                             #(8)</span>
<span class="n">dinvden</span> <span class="o">=</span> <span class="n">num</span>                                                     <span class="c">#(8)</span>
<span class="c"># backprop invden = 1.0 / den </span>
<span class="n">dden</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">den</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dinvden</span>                                <span class="c">#(7)</span>
<span class="c"># backprop den = sigx + xpysqr</span>
<span class="n">dsigx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                                <span class="c">#(6)</span>
<span class="n">dxpysqr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                              <span class="c">#(6)</span>
<span class="c"># backprop xpysqr = xpy**2</span>
<span class="n">dxpy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">xpy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpysqr</span>                                        <span class="c">#(5)</span>
<span class="c"># backprop xpy = x + y</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="c"># backprop sigx = 1.0 / (1 + math.exp(-x))</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigx</span> <span class="c"># Notice += !! See notes below  #(3)</span>
<span class="c"># backprop num = x + sigy</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                  <span class="c">#(2)</span>
<span class="n">dsigy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                <span class="c">#(2)</span>
<span class="c"># backprop sigy = 1.0 / (1 + math.exp(-y))</span>
<span class="n">dy</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigy</span>                                 <span class="c">#(1)</span>
<span class="c"># done! phew</span></code></pre></div>

<h4 id="section-7">注意事项：</h4>

<ol>
  <li>将前向计算结果缓存起来，供后向计算使用，如<code>xpy</code>；</li>
  <li>当变量的导数分成几部分计算时，结果要加起来，采用<code>+=</code>。</li>
</ol>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit4.png" alt="BP梯度计算" /></a><div class="caption">图 5:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png">PNG</a>]</div></div></div>

<p>神经网络中常使用的三种门是$+,\times,\max$，计算法则如上图。</p>

<p>对于线性分类器$\mathbf w^\top\mathbf x_i$，采用乘法门，输入数据的大小会影响权值梯度。输入数据对梯度影响很大，因此数据预处理能极大影响梯度。如果输入数据增大1000倍，权值的梯度也会增大1000倍，此时可以降低学习率补偿数据的影响。</p>

</section>
<section align="left">
<p></p>
<hr>
  <p><img/ src="/assets/images/alipay2me.png" alt="打赏作者" style="height: 160px"></p>
  <p></p>
<hr>
  <ul>
    
    <li class="pageNav">2016-09-18 &raquo; <a href="/2016/09/S13000-Introduction">鲁棒及自适应控制（1）：概论</a></li>
    
    <li class="pageNav">2016-09-13 &raquo; <a href="/2016/09/LeNet-5-Lecun">Gradient-Based Learning Applied to Document Recognition</a></li>
    
    <li class="pageNav">2016-03-15 &raquo; <a href="/2016/03/cs231n_loss-functions-and-optimization">CS231n（3）：损失函数与最优化</a></li>
    
    <li class="pageNav">2016-03-14 &raquo; <a href="/2016/03/cs231n_image-classification-pipeline">CS231n（2）：图像分类流程</a></li>
    
    <li class="pageNav">2016-02-29 &raquo; <a href="/2016/02/cs231n_introduction-to-computer-vision">CS231n（1）：计算机视觉简介</a></li>
    
    <li class="pageNav">2015-10-13 &raquo; <a href="/2015/10/minimum-cut-based-inference">DILinAV（4）：基于最小割的推理</a></li>
    
    <li class="pageNav">2015-09-26 &raquo; <a href="/2015/09/fast-ncc">快速归一化互相关</a></li>
    
    <li class="pageNav">2015-09-25 &raquo; <a href="/2015/09/maximum-flow-and-minimum-cut">DILinAV（3）：最大流与最小割</a></li>
    
  </ul>
<p></p>
<span>
  <a  href="/2015/02/summary-of-extraction-models" class="pageNav" style="float:left"   >上一篇：特征学习模型总结 </a>
  &nbsp;&nbsp;&nbsp;
  <a  href="/2015/03/r-essential" class="pageNav" style="float:right"   >下一篇：R Essential </a>  
</span>
</section>

	<script type="text/javascript">
	var first_image = document.getElementsByClassName("post")[0].getElementsByTagName("img")[0]; 
	if (first_image != undefined) {
	document.getElementsByClassName("ds-thread")[0].setAttribute("data-image", first_image.src);
	}
	</script>
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"jiyeqian"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		|| document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<ul class="ds-recent-visitors" data-num-items="16"></ul>
	<div class="ds-thread"  data-thread-key="/2015/02/image-classification-optimization" 	data-url="http://qianjiye.de/2015/02/image-classification-optimization" data-title="CS231n（4）：最优化">
	</div>	


<!-- <script type="text/javascript"> -->
<!-- $(function(){ -->
<!--   $(document).keydown(function(e) { -->
<!--     var url = false; -->
<!--         if (e.which == 37 || e.which == 72) {  // Left arrow and H -->
<!--          -->
<!--         url = '/2015/02/summary-of-extraction-models'; -->
<!--          -->
<!--         } -->
<!--         else if (e.which == 39 || e.which == 76) {  // Right arrow and L -->
<!--          -->
<!--         <1!-- url = 'http://qianjiye.de/2015/03/r-essential'; --1> -->
<!--         url = '/2015/03/r-essential'; -->
<!--          -->
<!--         } else if (e.which == 75) {  // K -->
<!--           url = '#'; -->
<!--         } else if (e.which == 74) { // J -->
<!--         url = '/2015/02/image-classification-optimization/#timeSpan'; -->
<!--         } -->
<!--         if (url) { -->
<!--             window.location = url; -->
<!--         } -->
<!--   }); -->
<!-- }) -->
<!-- </script> -->

        </article>
      </div>

    <footer>
        <p><small>
            Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a> | Copyright 2014 - 2016 by <a href="/about/">Jiye Qian</a> | <span class="label label-info" id="timeSpan"></span></small></p>
    </footer>

    </div>
  </body>
</html>
