<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-01-15T07:04:31+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>线性回归</title>
      <link href="http://qianjiye.de/2015/01/linear-regression" />
      <pubdate>2015-01-13T20:17:01+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/linear-regression</guid>
      <content:encoded>&lt;![CDATA[<p><img src="/assets/images/2014-10-19-linear_regression_0.png" alt="线性回归" /></p>

<p>本节的主要参考资料是机器学习<a href="#ng_ml_linreg_2014">[1]</a>和机器学习基石<a href="#lin_mlf_linreg_2014">[2]</a>网络课程。</p>

<h2 id="section">线性回归模型</h2>

<p>线性回归的假设集（hypothesis）为
\begin{equation}
h(\mathbf x) = \mathbf w^T\mathbf x，
\label{eq:linear-regression-model}
\end{equation}
$\mathbf x$是含有常数项$x_0 = 1$的$d+1$维向量$\mathbf x =\left[1,x_1,\ldots,x_d\right]^T$，$\mathbf w=\left[w_0,w_1,\ldots,w_d\right]^T$，$d$是特征维数。</p>

<p>目标代价函数采用平方误差和估计
\begin{equation}
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n-y_n\right)^2，
\label{eq:cost-function-linear-regression}
\end{equation}
$N$是样本数。通过机器学习算法求得线性回归的参数
\begin{equation}
\mathbf w_{LIN} = \arg\min_{\mathbf w}E_{in}(\mathbf w)，
\end{equation}
通常方法有正规方程（normal equation）的解析方法和梯度下降法（gradient descent）。</p>

<h2 id="section-1">解析方法</h2>

<p>解析方法得到的最优解也称为<strong>线性回归的最小二乘解</strong>。</p>

<p>代价函数改写为矩阵形式
\[
E_{in}(\mathbf w) 
={1\over N}\lVert\mathbf X\mathbf w-\mathbf y\rVert^2
={1\over N}\left(\mathbf w^T\mathbf X^T\mathbf X\mathbf w-2\mathbf w^T\mathbf X^T\mathbf y+\mathbf y^T\mathbf y\right)，
\]
$\mathbf X$是样本数据矩阵，每行代表一个样本点，每列代表一个特征，第一列的向量$\mathbf 1_N$对应常数偏移。$E_{in}(\mathbf w)$是连续可微的凸函数，当$\nabla E_{in}(\mathbf w)=0$时取得最小值，
\[
\nabla E_{in}(\mathbf w)={2\over N}\left(\mathbf X^T\mathbf X\mathbf w-\mathbf X^T\mathbf y\right)，
\]
取得最小值时
\begin{equation}
\mathbf w_{LIN} = \left(\mathbf X^T\mathbf X\right)^{-1}\mathbf X^T\mathbf y = \mathbf X^\dagger \mathbf y，
\end{equation}
$\mathbf X^\dagger$称为<strong>伪逆</strong>（pseudo-inverse）。通常情况$N\gg d+1$，因此$\left(\mathbf X^T\mathbf X\right)^{-1}$通常都可逆；如果不可逆，解不唯一。</p>

<p>导致$\mathbf X^T\mathbf X$不可逆的原因可能是冗余特征（redundant features）或者特征数目过多（$d$太大而$N$太少），解决的办法：   </p>

<ul>
  <li>对于冗余的线性相关特征，例如$x_1 = 2x_2$，删除线性相关特征；</li>
  <li>对于特征数目过多，例如$N&lt;d$，删除特征或正则化（regularization）<sup id="fnref:how-to-regularize"><a href="#fn:how-to-regularize" class="footnote">1</a></sup>。</li>
</ul>

<p>Matlab的<code>pinv</code>函数可以处理$\mathbf X^T\mathbf X$不可逆的情况<sup id="fnref:pinv-vs-inv"><a href="#fn:pinv-vs-inv" class="footnote">2</a></sup>。</p>

<h3 id="mathbf-wlin">解析方法求解$\mathbf w_{LIN}$是机器学习算法吗？</h3>

<p>✅通过VC维的角度分析，能得到小的$E_{out}\left(\mathbf w_{\small{LIN}}\right)$，就是学习……</p>

<ul>
  <li>能够得到最佳的$E_{in}$；</li>
  <li>$d+1$个变量，有限的$d_{VC}$，因此有好的$E_{out}$；</li>
  <li>事实上，求解伪逆的过程也是迭代逐步最优的过程（高斯消元法）。</li>
</ul>

<p>VC维考察的是个别的$E_{in}$<sup id="fnref:some-E-in"><a href="#fn:some-E-in" class="footnote">3</a></sup>，从$E_{in}$平均误差角度分析
\begin{equation}
\bar E_{in}
=\varepsilon_{\mathcal D\sim P^N}\left\{E_{in}\left(\mathbf w_{LIN}\mbox{ w.r.t }\mathcal D\right)\right\}
=\mbox{noise level}\cdot\left(1-{d+1\over N}\right)，
\label{eq:noise-level-e-in}
\end{equation}
$\mbox{noise level}$表示数据中的噪声，${d+1\over N}$表示比噪声小的比率，数据越多二者差别越小。</p>

<p>$E_{in}\left(\mathbf w_{LIN}\right)$计算方法为
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert\mathbf y-\mathbf X\mathbf X^\dagger\mathbf y\right\rVert^2
={1\over N}\left\lVert\left(\mathbf I-\mathbf X\mathbf X^\dagger\right)\mathbf y\right\rVert^2，
\]
$\mathbf X\mathbf X^\dagger$让$\mathbf y$加帽$\wedge$变成了$\hat{\mathbf y}$，也叫<strong>帽矩阵</strong><sup id="fnref:hat-matrix-properties"><a href="#fn:hat-matrix-properties" class="footnote">4</a></sup>（hat matrix），记为$\mathbf H$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-learning-curve.png"><img src="/assets/images/2015-01-13-linear-regression-learning-curve.png" alt="［左］：图解证明；［右］：学习曲线" /></a><div class="caption">Figure 1:  ［左］：图解证明；［右］：学习曲线 [<a href="/assets/images/2015-01-13-linear-regression-learning-curve.png">PNG</a>]</div></div></div>

<p>由$\hat{\mathbf y}=\mathbf X\mathbf w_{LIN}$可知，$\hat{\mathbf y}$是$\mathbf X$列向量的线性组合，也就是如上图左所示，$\hat{\mathbf y}$位于$\mathbf X$张成的线性空间中。当$\mathbf y-\hat{\mathbf y}$垂直于该生成空间时，$\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2$的值最小。$\mathbf H$将$\mathbf y$投影为$\hat{\mathbf y}$，$\mathbf I-\mathbf H$将$\mathbf y$投影为$\mathbf y-\hat{\mathbf y}$。$\mathbf I-\mathbf H$的迹为$trace(\mathbf I-\mathbf H)=N-(d+1)$，表示自由度从$N$降到$N-(d+1)$。</p>

<p>观测到的数据$\mathbf y$是理想的数据空间$f\left(\mathbf X\right)$叠加一些噪声。$\mathbf y-\hat{\mathbf y}$也可以从噪声投影得到，如上图左所示，
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert(\mathbf I-\mathbf H)\cdot\mbox{noise}\right\rVert^2
={1\over N}(N-(d+1))\lVert\mbox{noise}\rVert^2，
\]
因此可得公式\eqref{eq:noise-level-e-in}的结论。$E_{out}$的证明过程叫复杂，仍然可以得到
\[
\bar E_{out}
=\mbox{noise level}\cdot\left(1+{d+1\over N}\right)。
\]</p>

<p>$\mbox{noise level}$可以用$\sigma^2$表示，从上图右可见，$\bar E_{in}$和$\bar E_{out}$在$N\rightarrow\infty$时都会趋近于$\sigma^2$。期望的泛化误差（generalization error）可以用${2(d+1)\over N}$衡量，这里是平均情况，VC维衡量的是最坏的情况。</p>

<h2 id="section-2">梯度下降法</h2>

<p>梯度下降法就是沿梯度下降方向更新参数，也就是对每个特征的权值$w_i$，不断迭代执行更新
\begin{equation}
w_i := w_i-\alpha{\partial E_{in}(\mathbf w)\over\partial w_i}\quad(i=0,1,\ldots,d)
\end{equation}
直至收敛，其中$\alpha$表示学习速率，梯度计算公式为
\[
{\partial E_{in}(\mathbf w)\over\partial w_i}
={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n - y_n\right)x_{n,i}。
\]</p>

<p>参数须同时更新，也就是当每个$w_i$都更新完成后，才能用新的$\mathbf w$计算$E_{in}(\mathbf w)$，具体可参考<a href="/assets/images/2014-10-19-linear_regression_1.png">Andrew NG的讲义</a><sup id="fnref:andrew-simultaneous-update"><a href="#fn:andrew-simultaneous-update" class="footnote">5</a></sup>。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-curve.png"><img src="/assets/images/2015-01-13-linear-regression-error-curve.png" alt="［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；&lt;br/&gt;［右］：梯度下降路径" /></a><div class="caption">Figure 2:  ［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；<br />［右］：梯度下降路径 [<a href="/assets/images/2015-01-13-linear-regression-error-curve.png">PNG</a>]</div></div></div>

<p>梯度下降法需要将所有特征归一（feature scaling）到统一的尺度（不用归一化$x_0$），比如$-1\le x_i\le 1$，
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\qquad\mbox{or}\qquad
\hat{x}_i = \frac{x_i - x_{mean}}{x_{std}}，
\]
这样有助于提高梯度下降法的速度，如上图左2所示。</p>

<h4 id="alpha">学习率$\alpha$的注意事项：</h4>
<ol>
  <li>在迭代过程中不需调节$\alpha$大小，由于梯度会不断减小，在固定$\alpha$的情况下梯度下降步长也会自动减小，如上图右所示；</li>
  <li>$\alpha$太小收敛慢，太大可能错过极值点而不收敛，甚至可能导致$E_{in}(\mathbf w)$不降反升。</li>
</ol>

<p>线性回归的代价函数$E_{in}(\mathbf w)$不存在局部极值（local optima），极小值就是全局极值<sup id="fnref:no-local-optima"><a href="#fn:no-local-optima" class="footnote">6</a></sup>。</p>

<h2 id="section-3">两种方法对比</h2>

<table>
  <thead>
    <tr>
      <th>梯度下降法</th>
      <th>解析解法</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>迭代实现，可实现在线增量学习</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$d$很大时（$10^6$）工作良好</td>
      <td>$d$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化<sup id="fnref:why-not-scale"><a href="#fn:why-not-scale" class="footnote">7</a></sup></td>
    </tr>
  </tbody>
</table>

<h2 id="section-4">分类问题</h2>

<p>线性分类器和线性回归的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>指标</th>
      <th>线性分类器</th>
      <th>线性回归</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathcal Y$</td>
      <td>$\{-1,+1\}$</td>
      <td>$\mathbb R$</td>
    </tr>
    <tr>
      <td>$h(\mathbf x)$</td>
      <td>$\mbox{sign}\left(\mathbf w^T\mathbf x\right)$</td>
      <td>$\mathbf w^T\mathbf x$</td>
    </tr>
    <tr>
      <td>$err(\hat{y},y)$</td>
      <td>$[[\hat{y}\neq y]]$</td>
      <td>$(\hat{y}-y)^2$</td>
    </tr>
    <tr>
      <td>算法复杂度</td>
      <td>通常是NP-hard</td>
      <td>高效求解方法</td>
    </tr>
  </tbody>
</table>

<p>能否利用线性回归的高效，借助$g(\mathbf x)=\mbox{sign}\left(\mathbf w_{LIN}^T\mathbf x\right)$，用线性回归解决分类问题？✅</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-compare.png"><img src="/assets/images/2015-01-13-linear-regression-error-compare.png" alt="线性分类器和线性回归误差比较" /></a><div class="caption">Figure 3:  线性分类器和线性回归误差比较 [<a href="/assets/images/2015-01-13-linear-regression-error-compare.png">PNG</a>]</div></div></div>

<p>上图展示了两种方法误差的对比，$err_{0/1}\leq err_{sqr}$，平方误差是0/1误差的上限。从VC维的理论可知
\[
\mbox{classification }E_{out}(\mathbf w)
\leq \mbox{classification }E_{in}(\mathbf w)+\sqrt{\cdots}
\leq \mbox{regression }E_{in}(\mathbf w)+\sqrt{\cdots}，
\]
in-sample回归误差也是out-sample分类误差的上限，做好in-sample回归误差也是做好in-sample分类误差的一种方法，in-sample回归误差很小时能保证out-sample分类误差也很小。由此可见，可用线性回归解决分类问题。</p>

<p>也可直接将回归问题视为分类问题，只是用$err_{sqr}$当作$\widehat{err}$作为$err_{0/1}$误差的近似。为分类问题选择稍宽松的误差上界，这样容易求解参数。</p>

<p>在很多时候，用线性回归解决分类问题效果尚可。如果要让效果更好，可将$\mathbf w_{LIN}$当做PLA或pocket算法的初始值$\mathbf w_0$，加速PLA或pocket算法。</p>

<h2 id="section-5">多项式回归</h2>

<p>构造多项式特征，利用线性回归模型解决非线性问题，称为<strong>多项式回归</strong>（polynomial regression）。例如利用$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $，构造新的特征向量$\mathbf x$，带入线性回归模型\eqref{eq:linear-regression-model}求解。从另一个角度看，当特征是多项式时，可直接利用线性模型求解。</p>

<p>当对特征进行高次多项式变换后，取值范围可能急剧变化，需要对多项式特征进行尺度归一化处理<a href="#ng_ml_rlrbv_pe_2014">[3, P. 8]</a>。</p>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_linreg_2014">[1]A. Ng, “Linear Regression with multiple variables.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="lin_mlf_linreg_2014">[2]H.-T. Lin, “Lecture 9: Linear Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_rlrbv_pe_2014">[3]A. Ng, “Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:how-to-regularize">
      <p>如何进行正则化？ <a href="#fnref:how-to-regularize" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pinv-vs-inv">
      <p>Matlab的<code>pinv</code>和<code>inv</code>有何区别？ <a href="#fnref:pinv-vs-inv" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:some-E-in">
      <p>为什么VC维考察的是个别的$E_{in}$？ <a href="#fnref:some-E-in" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:hat-matrix-properties">
      <p>帽矩阵的性质（可从文中图示的角度理解）：（1）$\mathbf H$是对称的；（2）$\mathbf H^2=\mathbf H$；（3）$(\mathbf I-\mathbf H)^2=\mathbf I-\mathbf H$。 <a href="#fnref:hat-matrix-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-simultaneous-update">
      <p>事实上，不同时更新的情况很少发生，因为在更新每个$w_i$前，已经用$\mathbf w$计算过了$E_{in}(\mathbf w)$，更新过程中，不再需要重复计算$E_{in}(\mathbf w)$。 <a href="#fnref:andrew-simultaneous-update" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:no-local-optima">
      <p>这是真的吗？ <a href="#fnref:no-local-optima" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-scale">
      <p>为什么解析方法不需要规范化特征？ <a href="#fnref:why-not-scale" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（6）：支持向量回归</title>
      <link href="http://qianjiye.de/2015/01/svm-support-vector-regression" />
      <pubdate>2015-01-12T18:31:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-support-vector-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">脊回归</h2>

<p>有正则化项的回归称为<strong>脊回归</strong>（ridge regression）。脊回归的核模型有解析解么？</p>

<p>脊回归的最优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\left(y_n-\mathbf w^T\mathbf z_n\right)^2\right)，
\]
根据表示定理可知，存在形如$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$的最优解，将其带入并表示为核形式
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\left(y_n-\sum_{n=1}^N\beta_mK(\mathbf x_n,\mathbf x_m)\right)\right)。
\end{equation}
<strong>脊回归的核模型</strong>就是利用表示定理将脊回归核化。目标函数写为矩阵的形式
\begin{equation}
E_{aug}(\boldsymbol\beta)={\lambda\over N}\boldsymbol\beta^T\mathbf K\boldsymbol\beta + {1\over N}\left(\boldsymbol\beta^T\mathbf K^T\mathbf K\boldsymbol\beta-2\boldsymbol\beta^T\mathbf K^T\mathbf y + \mathbf y^T\mathbf y\right)，
\end{equation}
无约束最优化问题可以通过
\[
\nabla E_{aug}(\boldsymbol\beta)={2\over N}\left(\lambda\mathbf K^T\mathbf I\boldsymbol\beta+\mathbf K^T\mathbf K\boldsymbol\beta-\mathbf K^T\right)={2\over N}\mathbf K^T\left((\lambda\mathbf I+\mathbf K)\boldsymbol\beta-\mathbf y\right)
\]
令$\nabla E_{aug}(\boldsymbol\beta)=0$求解，
\begin{equation}
\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y。
\end{equation}
根据Mercer条件可知$\mathbf K$半正定，并且$\lambda&gt;0$，因此矩阵总可逆。稠密矩阵求逆的时间复杂度为$O\left(N^3\right)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png" alt="［左］：脊回归的线性模型；［右］：脊回归的核模型" /></a><div class="caption">Figure 1:  ［左］：脊回归的线性模型；［右］：脊回归的核模型 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png">PNG</a>]</div></div></div>

<p>上图中的蓝线是脊回归的效果。线性模型与核模型的选择是速度与效率的折中权衡，它们之间的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>线性模型</th>
      <th>核模型</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathbf w = (\lambda\mathbf I+\mathbf X^TX)^{-1}\mathbf X^T\mathbf y$</td>
      <td>$\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y$</td>
    </tr>
    <tr>
      <td>功能受限</td>
      <td>通过$K$实现强大的功能</td>
    </tr>
    <tr>
      <td>训练时间复杂度为$O\left(d^3+d^2N\right)$</td>
      <td>训练时间复杂度为$O\left(N^3\right)$</td>
    </tr>
    <tr>
      <td>预测时间复杂度为$O(d)$，当$N\gg d$时效率高</td>
      <td>预测时间复杂度为$O(N)$，当训练样本大时效率低</td>
    </tr>
  </tbody>
</table>

<p>当参数获得后，回归函数就可以用核表示为
\begin{equation}
g(\mathbf x)=\sum_{n=1}^N\beta_nK\left(\mathbf x_n, \mathbf x\right)。
\end{equation}</p>

<h2 id="section-1">最小二乘支持向量机</h2>

<p><strong>最小二乘支持向量机</strong>（LSSVM，least-squares SVM）就是将脊回归的核模型用于分类。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png" alt="最小二乘与soft-margin支持向量机的对比" /></a><div class="caption">Figure 2:  最小二乘与soft-margin支持向量机的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png">PNG</a>]</div></div></div>

<p>上图可以看出，最小二乘与soft-margin支持向量机的分类面很相似，但是LSSVM的支持向量要多得多，预测速度会很慢。</p>

<p>LSSVM和logistic回归的核模型得到的参数$\boldsymbol\beta$是稠密的，标准支持向量机的参数$\boldsymbol\alpha$是稀疏的。能否得到像标准支持向量机一样稀疏的$\boldsymbol\beta$呢？</p>

<h2 id="tube">tube回归</h2>

<p>定义tube回归的误差为
\begin{equation}
err(y, s) = \max(0, \lvert s-y\rvert-\epsilon)，
\end{equation}
在tube区域内不计误差，在该区域外到tube的距离记为误差。</p>

<div class="image_line" id="tube-error-illustration"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png" alt="tube与平方误差对比" /></a><div class="caption">Figure 3:  tube与平方误差对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png">PNG</a>]</div></div></div>

<p>如上图所示，tube与平方误差很相似，尤其是在$\lvert s-y\rvert$很小的区域内，当$\lvert s-y\rvert$很大时，tube误差不如平方误差变化陡峭，因此受噪声影响更小。</p>

<p>基于tube误差的模型能否得到稀疏的系数呢？</p>

<h2 id="section-2">支持向量回归</h2>

<p>基于$L_2$正则化的tube回归模型为
\begin{equation*}
\min\limits_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽无约束，但$\max$导致不可微；利用表示定理可核化，但无法明确得到稀疏的系数。仿照<a href="/2015/01/svm-kernel-logistic-regression/#mjx-eqn-equniform-soft-margin-svm">无约束形式</a>的soft-margin支持向量机，分离出$b$后改写为
\begin{equation*}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n+b-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽然不可微，但是QP问题；对偶问题可核化，KKT条件能得到系数稀疏。再对比<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">约束形式</a>的soft-margin支持向量机，改写为带约束的优化问题
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad \left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert\leq \epsilon+\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\end{equation*}
约束条件线性化
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi^\vee,\boldsymbol\xi^\wedge}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left(\xi_n^\vee+\xi_n^\wedge\right)\\
\mbox{s.t.}&amp;\quad -\epsilon-\xi_n^\vee\leq y_n - \mathbf w^T\mathbf z_n-b\leq \epsilon+\xi_n^\wedge\\
&amp;\quad\xi_n^\vee\geq 0,\xi_n^\wedge\geq 0\mbox{ for all }n，
\end{aligned}
\end{equation}
这就是标准的<strong>支持向量回归</strong>（SVR，support vector regression）原问题，$\xi_n^\vee$和$\xi_n^\wedge$分别记录tube下届和上界违规，如<a href="#tube-error-illustration">上图左</a>所示的分界线下边和上边标红的线段。通过$C$对正则化和tube违规进行折中，调节参数$\epsilon$可控制tube的高度。该QP模型有$\tilde d+1+2N$个变量，$2N+2N$个约束条件。</p>

<p>将支持向量回归的原问题转成对偶问题，可移除对$\tilde d$的依赖。</p>

<h2 id="section-3">支持向量回归的对偶模型</h2>

<p>通过拉格朗日乘子法的KKT条件${\partial\mathcal L\over\partial\mathbf w}=0$可得
\begin{equation}
\mathbf w = \sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)\mathbf z_n = \sum_{n=1}^N\beta_n\mathbf z_n，
\end{equation}
通过${\partial\mathcal L\over\partial b}=0$可得
\[
\sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)＝0，
\]
互补松弛条件（complementary slackness）为
\begin{equation}
\left\{
\begin{aligned}
\alpha_n^\wedge\left(\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\right)=&amp;0\\
\alpha_n^\vee\left(\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\right)=&amp;0。
\end{aligned}
\right.
\label{eq:complementary-slackness-svm-regession}
\end{equation}</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png" alt="QP原问题与对偶问题的对比" /></a><div class="caption">Figure 4:  QP原问题与对偶问题的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png">PNG</a>]</div></div></div>

<p>上图左上和左下分别表示soft-margin支持向量机的原问题和对偶问题的QP模型；上图右上和右下分别表示支持向量回归的原问题和对偶问题的QP模型。上图中，相同颜色的符号展示了如何从原问题变化到对偶问题。</p>

<p>当数据点位于tube中有$\left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert&lt;\epsilon$，不计误差，$\xi_n^\wedge=\xi_n^\vee=0$，根据互补松弛条件\eqref{eq:complementary-slackness-svm-regession}可知
\[
\left\{
\begin{aligned}
\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\neq &amp;0\\
\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\neq &amp;0，
\end{aligned}
\right.
\]
以及$\alpha_n^\wedge=\alpha_n^\vee＝0$，因此可得$\beta_n=0$。由此可知，支持向量回归问题中$\beta_n\neq 0$的支持向量刚好位于tube的边界上或在tube之外。</p>

<p>参考<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-complementary-slackness">soft-margin支持向量机</a>可得
\begin{equation}
b=
\left\{
\begin{aligned}
y_n-\mathbf w^T\mathbf z_n-\epsilon&amp;\quad(0&lt;\alpha_n^\wedge&lt;C)\\
y_n-\mathbf w^T\mathbf z_n+\epsilon&amp;\quad(0&lt;\alpha_n^\vee&lt;C)。
\end{aligned}
\right.
\end{equation}</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（5）：核logistic回归</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-logistic-regression" />
      <pubdate>2015-01-09T15:01:15+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png" alt="4种形式的支持向量机" /></a><div class="caption">Figure 1:  4种形式的支持向量机 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png">PNG</a>]</div></div></div>

<h2 id="section">支持向量机的正则化形式</h2>

<p>回顾<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">soft-margin支持向量机</a>，当违反边界的时候$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n + b\right)$，当没有违反边界的时候$\xi_n = 0$，边界违法的情况可以统一定义为$\xi_n=\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)$，于是无约束形式的soft-margin支持向量机为
\begin{equation}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)\right)，
\label{eq:uniform-soft-margin-svm}
\end{equation}
可以简写为
\[
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum\widehat{\mbox{err}}\right)。
\]
这是目标函数的正则化形式表示方法，soft-margin可以看作一种特殊的误差$\widehat{\mbox {err}}$度量。$L_2$正则化是对$\mathbf w$长度的约束
\[
\min\limits_{b,\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w + {1\over N}\sum\mbox{err}\right)。
\]</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png" alt="SVM与正则化" /></a><div class="caption">Figure 2:  SVM与正则化 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png">PNG</a>]</div></div></div>

<p>加入了大分类的边界限制，可以得到更少的分类情况，也可以通过$L_2$正则化实现。
从上图对比正则化方法可知，支持向量机可以看作为特殊的正则化方法。大的$C$，对应于小的$\lambda$，更弱的正则化。</p>

<h2 id="section-1">误差度量</h2>

<p>令线性项输出$s=\mathbf w^T\mathbf z_n + b$，感知器算法、支持向量机和logistic回归的误差度量为
\begin{equation}
\left\{
\begin{aligned}
err_{0/1}(s, y)&amp;=[[ys\neq 1]]\\
\widehat{err}_{SVM}(s, y)&amp;=\max(1-ys, 0)\\
err_{SCE}(s, y)&amp;=\log(1+\exp(-ys))。
\end{aligned}
\right.
\end{equation}</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png" alt="误差度量比较" /></a><div class="caption">Figure 3:  误差度量比较 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png">PNG</a>]</div></div></div>

<p>几种误差曲线对比如上图所示。其中，支持向量机的这种误差度量方式通常称为hinge error measure。支持向量机和logistic回归的误差是0/1误差的上界，对于分类问题，通过上界的最小化，间接做好0/1误差的最优化。从误差曲线还可以看出，支持向量机和$L_2$正则化的logistic的误差度量非常相似。这几种分类器的比较如下：</p>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>优化方法</th>
      <th>优势</th>
      <th>劣势</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>感知器算法</td>
      <td>最小化$err_{0/1}$</td>
      <td>线性可分时效率较高</td>
      <td>只适用线性可分，否则采用pocket算法</td>
    </tr>
    <tr>
      <td>soft-margin支持向量机</td>
      <td>QP最小化误差$err_{0/1}$</td>
      <td>容易优化，理论基础较好</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
    <tr>
      <td>正则化logistic回归</td>
      <td>GD／SGD最小化误差$err_{SCE}$</td>
      <td>容易优化，正则化控制模型</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
  </tbody>
</table>

<p>从比较可以看出，logistic回归是soft-margin支持向量机的近似。</p>

<h2 id="section-2">支持向量机的概率模型</h2>

<p>如何让支持向量机输出$[0,1]$之间的概率？</p>

<p>一种思路是将支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$作为logistic回归中线性判别部分的参数
$
g(\mathbf x) = \theta\left(\mathbf w_{SVM}^T\mathbf x + b_{SVM}\right)
$，
直接利用支持向量机和logistic回归。这在实际应用中表现尚佳，但丧失了logistic回归的优良特性（比如maxmum likehood）。</p>

<p>另一种思路将向支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$当作logistic的起始点，最终得到logistic回归模型。这和直接利用logistic结果差不多，还丧失了支持向量机核方法等优良特性。</p>

<p>如何融合支持向量机和logistic回归的优点？</p>

<p>组合logistic回归和支持向量机
\begin{equation}
g(\mathbf x) = \theta\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x) + b_{SVM}\right)+B\right)，
\label{eq:mixture-svm-logistic-model}
\end{equation}
这样既融合了支持向量机的核特性，又通过$A$和$B$两个自由度调节分类超平面适合最大似然。如果$A&gt;0$，表示支持向量机得到的$\mathbf w_{SVM}$较好；如果$B\approx 0$，表示支持向量机得到的$b_{SVM}$较好。新的logistic问题就变为了
\begin{equation}
\min_{A, B}{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}\right)+B\right)\right)\right)，
\end{equation}
该模型可以分为2阶段，首先利用支持向量机得到1维特征，然后采用简单的logistic模型，这称为<strong>支持向量机的Platt概率模型</strong>：</p>

<ol>
  <li>先在数据集上$\mathcal D$上利用支持向量机得到模型参数$\mathbf w_{SVM}$和$b_{SVM}$（或者$\boldsymbol\alpha$），再进行变换$\mathbf z’_n=\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}$；</li>
  <li>在$\{\left(\mathbf z’_n, y_n\right)\}_{n=1}^N$上得到logistic模型的参数$A,B$；</li>
  <li>将公式\eqref{eq:mixture-svm-logistic-model}的结果作为模型输出。</li>
</ol>

<p>从模型可以看出，这并不是严格的$\mathcal Z$空间logistic回归。</p>

<h2 id="logistic">核logistic回归</h2>

<p>最佳的$\mathbf w$是$\mathbf z_n$的线性组合，这是能使用核方法的关键。</p>

<p>从SGD来看，logistic回归的$\mathbf w$也是$\mathbf z_n$的线性组合
\[
\mathbf w_{LOGREG}＝\sum_{n=1}^N\left(\alpha_ny_n\right)\mathbf z_n。
\]</p>

<blockquote>
  <h4 id="representer-theorem">表示定理（representer theorem）</h4>
  <hr />
  <p>对任意的$L_2$正则化线性模型     <br />
\begin{equation*}
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^Nerr\left(y_n,\mathbf w^T\mathbf z_n\right)\right)，
\end{equation*} 
存在能用$\mathbf z_n$线性表示的最佳解$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$。</p>
</blockquote>

<p>任意$L_2$正则化的线性模型都能使用核方法。$L_2$正则化的logistic回归优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\mathbf w^T\mathbf z_n\right)\right)\right)。
\]
由上述定理可知，最佳$\mathbf w$一定是$\mathbf z_n$的线性组合。直接将用$\mathbf z_n$表示的$\mathbf w$代入上式，再用核方法表示，可以得到基于$L_2$正则化的核logistic回归优化模型
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\sum_{m=1}^N\beta_mK(\mathbf x_m,\mathbf x_n)\right)\right)\right)，
\end{equation}
这是无约束最优化问题，GD／SGD等都可求解。与支持向量机不同，KLR的大部分$\beta_n\neq 0$。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（4）：soft-margin支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-soft-margin-svm" />
      <pubdate>2015-01-07T17:24:31+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-soft-margin-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="soft-margin">soft-margin支持向量机</h2>

<p>在求解支持向量机参数的二次规划中，约束条件要求所有点被正确分类的叫<strong>hard-margin支持向量机</strong>。这类支持向量机过拟合原因：（1）特征转换功能太强；（2）坚持所有点都被正确分类。</p>

<p>可以通过放宽条件，容忍部分噪声（容许这部分噪声数据分类错误），使得支持向量机具有更好的泛化性能。</p>

<p>对于pocket PLA，目标函数为
\[
\min\limits_{b,\mathbf w}\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\]
结合<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqlinear-svm-model">线性支持向量机</a>，可以得到容忍错分的优化模型
\[
\begin{aligned}
\min\limits_{b,\mathbf w}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\infty\cdot\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\end{aligned}
\]
$C$是调节最大边界和噪声容忍度的参数。但是，上述模型不是QP，并且不能区别分类错误时误差的大小，进一步降模型变为<strong>soft-margin支持向量机</strong>
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\label{eq:soft-margin-primal-svm}
\end{equation}
该QP模型有$\tilde d+1+N$个变量和$2N$个约束条件，也被称为基于$\ell_1$损失的soft-margin。如果采用$\xi_n^2$，则被称为基于$\ell_2$损失的soft-margin
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n^2\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n，
\end{aligned}
\end{equation*}
此时不再需要约束条件$\xi_n\geq 0$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png" alt="边界容忍度" /></a><div class="caption">Figure 1:  边界容忍度 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png">PNG</a>]</div></div></div>

<p>$C$是调节最大边界和边界容忍度的参数，$\xi_n$是容忍误差的大小，如上图所示。$C$越小，对最大边界要求越高；$C$越大，能容忍的边界误差越小。</p>

<h2 id="soft-margin-1">对偶soft-margin支持向量机</h2>

<p>soft-margin支持向量机的拉格朗日函数为
\[
\begin{aligned}
\mathcal L(b,\mathbf w,\boldsymbol\alpha, \boldsymbol\beta)=
&amp;{1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
&amp;+\sum_{n=1}^N\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)+\sum_{n=1}^N\beta_n\left(-\xi_n\right)，
\end{aligned}
\]
根据${\partial\mathcal L\over\partial\xi_n}=0$可得$C-\alpha_n-\beta_n=0$，利用化解<a href="/2015/01/svm-dual-svm/#lagrange-dual-problem">拉格朗日对偶问题</a>相同的方法可得
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad 0\leq\alpha_n\leq C,\mbox{ for }n=1,2,\ldots,N \\
\mbox{implicitly}&amp;\quad \mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n\\
&amp;\quad\beta_n=C-\alpha_n,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-soft-margin-dual-svm}
\end{equation}
与hard-margin对偶支持向量机不同的地方只是$\alpha_n$多了一个上界$C$，这是$N$个变量$2N+1$个约束条件的QP。$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf 0_N\leq \mathbf I_N\boldsymbol\alpha \leq C\cdot \mathbf 1_N。
\end{equation}</p>

<h2 id="soft-margin-2">核soft-margin支持向量机</h2>

<p>soft-margin是实际中广泛应用的支持向量机。</p>

<p>soft-margin的支持向量机与<a href="/2015/01/svm-kernel-svm/#kernel-trick">hard-margin的支持向量机</a>基本相同，$\alpha_n$上界$C$的限制，导致$b$的计算不同。</p>

<p>利用complementary slackness条件可得
\begin{equation}
\begin{aligned}
\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=&amp;0\\
\left(C-\alpha_n\right)\xi_n=&amp;0，
\end{aligned}
\label{eq:soft-margin-complementary-slackness}
\end{equation}
当$\alpha_s&gt;0$时，$b=y_s-y_s\xi_s-\mathbf w^T\mathbf z_s$；当$\alpha_s&lt;C$时，$\xi_s=0$。满足$0&lt;\alpha_s&lt;C$的点称为<strong>自由支持向量</strong>$\left(\mathbf x_s, y_s\right)$，利用这些点容易得到
\begin{equation}
b=y_s-\sum\limits_{SV}\alpha_ny_nK\left(\mathbf x_n, \mathbf x_s\right)。
\end{equation}
在极少数情况下，不存在自由支持向量，$b$通过不等式限定，只要满足KKT条件的取值都是合理的$b$。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png" alt="soft-margin高斯核支持向量机" /></a><div class="caption">Figure 2:  soft-margin高斯核支持向量机 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png">PNG</a>]</div></div></div>

<p>上图展示了soft-margin高斯核支持向量机的效果，灰色的区域表示最大分类间隔。从图中可以看出，$C$越大，对误差的容忍越弱，越容易导致过拟合。</p>

<h2 id="alphan">$\alpha_n$的物理含义</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png" alt="不同类型的数据点" /></a><div class="caption">Figure 3:  不同类型的数据点 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png">PNG</a>]</div></div></div>

<p>通过公式\eqref{eq:soft-margin-complementary-slackness}可知，$\alpha_n$将数据点分为如上图所示的3种类型：</p>

<ol>
  <li>当$\alpha_n=0$时，非支持向量，$\xi_n=0$，位于边界之外，极少数可能在边界上；</li>
  <li>当$0&lt;\alpha_n&lt;C$时，自由（free）支持向量$\square$，$\xi_n=0$，位于边界上，用于计算$b$；</li>
  <li>当$\alpha_n=C$时，有界（bounded）支持向量$\triangle$，$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n+b\right)$，落在边界内，可能正确分类也可能分错，极少数可能在边界上。</li>
</ol>

<h2 id="section">模型选择</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png" alt="［中］：交叉验证误差；［右］：支持向量个数" /></a><div class="caption">Figure 4:  ［中］：交叉验证误差；［右］：支持向量个数 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png">PNG</a>]</div></div></div>

<p>上图左是soft-margin高斯核支持向量机的分类效果，横轴是$C$的变化，纵轴是$\gamma$的变化。由于$E_{cv}(C,\gamma)$不光滑，通常的模型选择方法是通过$C$和$\gamma$的数据网格，利用交叉验证的方法选择合适的模型，上图中所示，选择了左下角的模型。</p>

<p>交叉验证中，将数据分为$N$份的验证称为<strong>leave-one-out交叉验证</strong>，它的误差上界是
\begin{equation}
E_{loocv}\leq\frac{\#SV}{N}，
\label{eq:eloocv-upper-bound}
\end{equation}
$\#SV$表示支持向量的个数。可以通过支持向量的个数进行模型选择。由于支持向量个数的函数也是非光滑的，难以优化，也采取利用$C$和$\gamma$的数据网格，多次计算后做选择。</p>

<p>由于\eqref{eq:eloocv-upper-bound}也只是给出了$E_{loocv}$的上界，通常用于当$E_{cv}$计算量很大时模型的安全检查，剔除那些支持向量过多的危险模型，然后再在剩余模型中进一步做交叉验证选择合适的模型。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（3）：核支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-svm" />
      <pubdate>2015-01-06T14:25:58+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">回顾对偶支持向量机</h2>

<p>当特征空间维数$\tilde d$很大时，计算$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$是对偶支持向量机的求解瓶颈。</p>

<p>能否找到比$O(\tilde d)$快的方法计算$\mathbf z_n^T\mathbf z_m=\Phi(\mathbf x_n)^T\Phi(\mathbf x_m)$？能否将先特征转换再计算内积的两步合为一步呢？</p>

<h2 id="kernel-trick">核技巧</h2>

<p>对于2阶多项式变换
\[
\Phi_2(\mathbf x)=\left(1, x_1,x_2,\ldots,x_d,x_1^2,x_1x_2,\ldots,x_1x_d,x_2x_1,x_2^2,\ldots,x_2x_d,\ldots,x_d^2\right)，
\]
为了简化同时包含了$x_1x_2$和$x_2x_1$这样的项。变换之后$Z$空间的内积可以可直接通过$X$空间计算
\[
\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)=1+\left(\mathbf x^T\mathbf x’\right)+\left(\mathbf x^T\mathbf x’\right)^2。
\]
这种特征转换和内积合并的方法称之为<strong>核函数</strong>，
\begin{equation*}
K_{\Phi_2}\left(\mathbf x,\mathbf x’\right)=\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)。
\end{equation*}
利用核函数，可以简化对偶支持向量机的实现，二次项的系数为
\begin{equation}
q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m=y_ny_mK\left(\mathbf x_n,\mathbf x_m\right)，
\end{equation}
利用支持向量$\left(\mathbf x_s, y_s\right)$计算偏移量
\begin{equation}
\begin{aligned}
b
=&amp;y_s-\mathbf w^T\mathbf z_s\\
=&amp;y_s-\left(\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right)^T\mathbf z_s\\
=&amp;y_s-\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x_s\right)
，
\end{aligned}
\end{equation}
对于特定的输入$\mathbf x$，判别函数为
\begin{equation}
\begin{aligned}
g_{SVM}(\mathbf x)
=&amp;\mbox{sign}\left(\mathbf w^T\Phi(\mathbf x)+b\right)\\
=&amp;\mbox{sign}\left(\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x\right)+b\right)
。
\end{aligned}
\end{equation}
从上面的公式可以看出，计算不再依赖变换后的空间，只依赖原空间，$b$和判别函数只依赖原空间的支持向量，大大简化了计算。</p>

<h2 id="section-1">多项式核</h2>

<p>仿照2阶多项式核的定义，可以推导高阶多项式核的定义为
\begin{equation}
K_Q(\mathbf x,\mathbf x’)=\left(\zeta + \gamma\mathbf x^T\mathbf x’\right)^Q\quad\zeta\geq 0,\gamma &gt; 0。
\end{equation}
事实上，系数$\zeta$和$\gamma$的取值不会改变多项式所在的空间，这些系数会被$\mathbf w$所吞噬。但是，不同的系数会得到不同的支持向量和判别函数。选择不同的核，相当于改变了边界的定义。多项式核可以在几乎不增加计算量的情况下，得到复杂的判别界。支持向量机通过large-margin控制判别界的复杂度。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png" alt="2阶多项式核SVM的效果" /></a><div class="caption">Figure 1:  2阶多项式核SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是2阶多项式核支持向量机的效果，不同系数下的支持向量和判别界不同。</p>

<p>当$\zeta=0,\gamma=1$时，多项式核就变为了线性核。线性核利用原始支持向量机就比较高效。因此，应当首先尝试线性核，当线性核不能满足要求时再尝试其它高阶核。</p>

<h2 id="section-2">高斯核</h2>

<p>对于1维数据的高斯核，利用Taylor展式可得
\begin{equation*}
\begin{aligned}
K(x,x’)
=&amp;\exp\left(-\left(x-x’\right)^2\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\exp\left(2xx’\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\sum_{i=0}^\infty\frac{\left(2xx’\right)^i}{i!}\\
=&amp;\sum_{i=0}^\infty\exp\left(-x^2\right)\exp\left(-x’^2\right)\sqrt{\frac{2^i}{i!}}\sqrt{\frac{2^i}{i!}}x^ix’^i\\
=&amp;\Phi(x)^T\Phi(x’)，
\end{aligned}
\end{equation*}
其中$\Phi(x)=\exp\left(-x^2\right)\cdot\left(1,\sqrt{\frac{2^1}{1!}}x,\sqrt{\frac{2^2}{2!}}x^2,\ldots\right)$，容易看出这是无穷维的特征变换。更一般的高斯核定义为
\begin{equation}
K\left(\mathbf x,\mathbf x’\right)=\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x’\right\rVert^2\right)\quad \gamma&gt;0。
\end{equation}
高斯核也成为径向基函数（RBF，Radial Basis Function）核。基于高斯核的判别函数为
\begin{equation}
g_{SVM}(\mathbf x)=\mbox{sign}\left(\sum_{SV}\alpha_ny_n\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_n\right\rVert^2\right)+b\right)，
\end{equation}
它是以支持向量为中心的高斯函数的线性组合，不再依赖$\mathbf w$，只依赖于支持向量和系数$\alpha_n$。</p>

<p>高斯核相当于进行了无限维的特征转换，可以得到复杂的判别界，泛化性能通过最大边界“保证”。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png" alt="高斯核的SVM的效果" /></a><div class="caption">Figure 2:  高斯核的SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是不同系数的高斯核支持向量机的效果，$\gamma$越大，高斯函数越尖，越容易过拟合。当$\gamma\rightarrow\infty$时，$K_{lim}\left(\mathbf x,\mathbf x’\right)=[[\mathbf x＝\mathbf x’]]$，相当于严格判断是否与支持向量一致。</p>

<h2 id="section-3">小结</h2>

<p>各种核函数的比较如下表：</p>

<table>
  <thead>
    <tr>
      <th>核函数</th>
      <th>优势</th>
      <th>局限性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>线性核</td>
      <td>计算快；<br />通过$\mathbf w$和支持向量容易解释</td>
      <td>无法处理线性不可分数据</td>
    </tr>
    <tr>
      <td>多项式核</td>
      <td>可以通过次数$Q$控制复杂度</td>
      <td>当$Q$很大时数值计算困难<br />（当$\left\lvert\zeta+\gamma\mathbf x^T\mathbf x’\right\rvert&lt;1$时，$K(\mathbf x, \mathbf x’)\rightarrow 0$）；<br />3个参数难以选择</td>
    </tr>
    <tr>
      <td>高斯核</td>
      <td>强大；<br />$K(\mathbf x, \mathbf x’)$有界；<br />1个参数难以选择；</td>
      <td>没有显示的$\mathbf w$供解读；<br />计算比线性核慢；<br />太强大导致容易过拟合</td>
    </tr>
  </tbody>
</table>

<p>当采用多项式核时，如果$Q$较小，可以尝试直接利用特征变换和原始支持向量机，求解速度可能比核方法更快。</p>

<p>核是一种特殊的相似性度量，但是不是所有的相似性度量都可以作为核。有效的核必须满足<strong>Mercer条件</strong>（充要条件）：核函数矩阵
\begin{equation*}
\begin{aligned}
\mathbf K
=&amp;\begin{bmatrix}
\Phi(\mathbf x_1)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_1)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_1)^T\Phi(\mathbf x_N)\\
\Phi(\mathbf x_2)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_2)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_2)^T\Phi(\mathbf x_N)\\
\ldots&amp;\ldots&amp;\ldots&amp;\ldots\\
\Phi(\mathbf x_N)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_N)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_N)^T\Phi(\mathbf x_N)
\end{bmatrix}\\
=&amp;\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]^T\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]\\
=&amp;\mathbf Z\mathbf Z^T
\end{aligned}
\end{equation*}
必须是对称半正定。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（2）：对偶支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-dual-svm" />
      <pubdate>2015-01-05T18:24:26+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-dual-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">对偶支持向量机的价值</h2>

<p>对于求解非线性支持向量机系数的QP，有$\tilde d+1$个变量和$N$个约束条件，对于经过非线性特征变换后$Z$空间的特征维数$\tilde d$一般很大。当$\tilde d$很大时，求解比较具有挑战性。</p>

<p>对偶支持向量机是支持向量机的另一种形式，它不依赖$\tilde d$，有$N$个变量和$N+1$个约束条件。</p>

<h2 id="section-1">拉格朗日乘子法</h2>

<p>正则化采用带约束的最小化方法
\[
\min\limits_{\mathbf w} E_{in}(\mathbf w)\mbox{ s.t. }\mathbf w^T\mathbf w \leq C，
\]
可以通过如下等价的拉格朗日乘子法实现
\[
\min\limits_{\mathbf w} E_{aug}(\mathbf w)=E_{in}(\mathbf w)+{\lambda\over N}\mathbf w^T\mathbf w。
\]
正则化通过将$\lambda$作为<strong>确定值</strong>，代替$C$作为约束条件，求解更容易。对偶支持向量机不同于正则化，它将$N$个$\lambda$视为<strong>变量</strong>求解。</p>

<p>当$\boldsymbol\alpha_n$为拉格朗日乘子时，求解支持向量机参数的拉格朗日函数为
\begin{equation}
\mathcal L(b, \mathbf w, \boldsymbol \alpha) = {1\over 2}\mathbf w^T\mathbf w + \sum_{n=1}^N\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)。
\end{equation}</p>

<h2 id="lagrange-dual-problem">拉格朗日对偶问题</h2>

<p>根据拉格朗日函数，可得
\begin{equation}
\begin{aligned}
\mbox{SVM}\equiv &amp;\min\limits_{b,\mathbf w}\max\limits_{\mbox{all }\alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\\
=&amp;\min\limits_{b,\mathbf w}\left(\infty\mbox{ if violating };{1\over 2}\mathbf w^T\mathbf w\mbox{ if feasible}\right)。
\end{aligned}
\end{equation}
根据$b$和$\mathbf w$取值的划分，分两种情况解释上述公式：</p>

<ol>
  <li>任意violating的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{some positive})\right)\rightarrow\infty$；</li>
  <li>任意feasible的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{all non-positive})\right)=\square$。</li>
</ol>

<p>对$\alpha’_n\geq 0$的任意$\boldsymbol\alpha’$可得
\[
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha’)，
\]
于是有
\begin{equation}
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\max\limits_{\mbox{all } \alpha_n\geq 0}\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha)。
\end{equation}</p>

<p>若果解决了对偶问题，就得到原问题的下界。上式右边<strong>将对$b$和$\mathbf w$的优化问题转化成了对$\alpha_n$的优化问题</strong>，同时内层是对$b$和$\mathbf w$无约束条件的最优化，方便求解。</p>

<p>如果上式中只是“$\geq$”，则表示弱对偶（weak duality）；如果上式中“$=$”成立，则为强对偶（strong duality）。对于QP，“$=$”成立的条件是：（1）原问题是凸的；（2）原问题有解（对本问题而言，数据是可分的）；（3）线性约束条件。</p>

<p>对于支持向量机，“$=$”成立，求解右边的问题即可。</p>

<h2 id="section-2">化简拉格朗日对偶问题</h2>

<p>对于对偶问题，内层无约束优化取得最小值的条件是
\[
{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial b} = 0;\quad{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial w_i} = 0，
\]
也就是
\[
\sum_{n=1}^N\alpha_ny_n=0;\quad\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\]
将上述变量带入对偶问题，可以化解为
\[
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\min\limits_{b,\mathbf w}\left(\frac{1}{2}\mathbf w^T\mathbf w+\sum_{n=1}^N\alpha_n-\mathbf w^T\mathbf w\right)，
\]
继续将$\mathbf w$的取值带入，可得
\begin{equation*}
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\left(-\frac{1}{2}\left\lVert\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right\rVert^2+\sum_{n=1}^N\alpha_n\right)，
\end{equation*}
化为二次规划的标准形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm}
\end{equation}</p>

<p>该优化问题有$N$个变量，$N+1$个约束条件。根据<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqqp-standard-format">QP的标准形式</a>，可以得到利用QP求解对偶二次规划的系数
\[
\begin{aligned}
&amp;q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m;\quad\mathbf p=-\mathbf 1_N;\\
&amp;\mathbf a_{\geq}=\mathbf y,c_{\geq}=0;\quad\mathbf a_{\leq}=-\mathbf y,c_{\leq}=0;\\
&amp;\mathbf a_n^T=\mbox{n-th unit direction},c_n=0。
\end{aligned}
\]
为了利用标准的QP，将“$=$”约束转换成了“$\geq$”和“$\leq$”约束。通常情况$q_{n,m}\neq 0$，系数对应着$N\times N$的非稀疏矩阵，$N$很大时需要占用很大的存储空间。因此，通常会采用针对支持向量机设计的特殊QP加速求解过程。</p>

<p>对偶支持向量机优化问题的矩阵形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\boldsymbol\alpha^T\mathbf Q_D\boldsymbol\alpha-\mathbf 1^T\boldsymbol\alpha \\
\mbox{subject to}&amp;\quad\mathbf y^T\boldsymbol\alpha=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm2}
\end{equation}
其中$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$，$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf I_N\boldsymbol\alpha \geq \mathbf 0_N。
\end{equation}</p>

<h2 id="kkt">KKT条件</h2>

<p>原问题和对偶问题都是最佳解需要$b,\mathbf w,\boldsymbol\alpha$之间满足如下条件：</p>

<ol>
  <li>原问题可行：$y_n(\mathbf w^T\mathbf z_n+b)\geq 1$；</li>
  <li>对偶问题可行：$\alpha_n\geq 0$；</li>
  <li>对偶问题内最优化：$\sum_{n=1}^N\alpha_ny_n=0;\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n$；</li>
  <li>原问题内最优化：$\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=0$，这个条件也称为complementary slackness，其中至少一项为$0$。</li>
</ol>

<p>这称为KKT条件，它是原问题和对偶问题都是最佳解的必要（necessary）条件，此处也是充分（sufficient）条件。</p>

<blockquote>
  <h4 id="example">Example</h4>
  <hr />
  <p>For a single variable $w$, consider minimizing ${1\over 2}w^2$ subject to two linear constraints $w\geq 1$ and $w\leq 3$. We know that the Lagrange function $\mathcal L(w,\alpha)={1\over 2}w^2+\alpha_1(1-w)+\alpha_2(w-3)$. Which of the following equations that contain $\alpha$ are among the KKT conditions of the optimization problem?</p>

  <ol>
    <li>$\alpha_1\geq 0$ and $\alpha_2\geq 0$</li>
    <li>$w=\alpha_1-\alpha_2$</li>
    <li>$\alpha_1(1-w)=0$ and $\alpha_2(w-3)=0$ </li>
    <li>all of the above</li>
  </ol>

  <p>Answer：4</p>
</blockquote>

<p>通过KKT条件，可以利用$\boldsymbol\alpha$求解$b$和$\mathbf w$。利用KKT条件3容易求解$\mathbf w$，利用KKT条件4，当$\alpha_n&gt;0$时，$1-y_n\left(\mathbf w^T\mathbf z_n+b\right)=0$两边同时乘以$y_n$可得$b$，
\begin{equation}
\left\{
\begin{aligned}
b=&amp;y_n-\mathbf w^T\mathbf z_n \quad\mbox{if }\alpha_n\neq 0；\\
\mathbf w=&amp;\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\end{aligned}
\right.
\end{equation}
利用不同$\alpha_n&gt;0$时的数据，理论上求解到的$b$应该是一样的，可以计算多个$b$然后平均得到更稳定的解。</p>

<h2 id="section-3">支持向量</h2>

<p>$\alpha_n&gt;0$对应的点一定在边界上，这些点称为<strong>支持向量</strong>。也有些在边界上的点，对应的$\alpha_n$不一定大于$0$。容易发现，计算$b$和$\mathbf w$只需要支持向量就够了。</p>

<p>从计算公式可以发现，$\mathbf w$可以由$y_n\mathbf z_n$的线性组合表示，也就是$\mathbf w$可由数据表示，这和PLA算法相似
\begin{equation*}
\mathbf w_{SVM}=\sum_{n=1}^N\alpha_n\left(y_n\mathbf z_n\right)，
\mathbf w_{PLA}=\sum_{n=1}^N\beta_n\left(y_n\mathbf z_n\right)，
\end{equation*}
其中$\beta_n$表示犯错误的次数。</p>

<h2 id="section-4">两种形式的支持向量机</h2>

<p>原始支持向量机适合特征维数$\tilde d$较少的情形，对偶形式的支持向量机适合数据点$N$较少的情形，两者都是通过最优化找到最大边界的判别界。</p>

<p>事实上，对偶形式的支持向量机和特征维数$\tilde d$也有关系，$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$的计算也是$\mathbb R^{\tilde d}$空间的内积。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（1）：线性支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-linear-svm" />
      <pubdate>2015-01-03T18:57:13+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-linear-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">最佳判别界</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png" alt="最佳判别界" /></a><div class="caption">Figure 1:  最佳判别界 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png">PNG</a>]</div></div></div>

<p>对相同的数据集，PLA可能得到不同的判别界，如上图所示，那条判别界最好呢？</p>

<ul>
  <li>测量是有误差的，能容忍误差越大的分界线越好，如上图上排所示，灰色的圆半径越大表示对误差的容忍度越大。噪声是导致过拟合的主要原因，对误差的容忍度越好，过拟合的可能性越低。</li>
  <li>判别界能膨胀得越胖越鲁棒，如上图下排所示。</li>
</ul>

<p>根据上两条标准，最右的判别界最佳。判别界的胖瘦程度称为边界（margin），最大边界的判别界最佳，也就是离判别界距离最近的点离判别界距离越大越好，</p>

<p>\begin{equation*}
\begin{aligned}
\max\limits_{\mathbf w} &amp;\quad\mbox{margin}(\mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\mathbf w^T\mathbf x_n &gt; 0\\
&amp;\quad\mbox{margin}(\mathbf w)=\min\limits_{n=1,\ldots,N}\mbox{distance}(\mathbf x_n, \mathbf w)。
\end{aligned}
\end{equation*}</p>

<p>$y_n\mathbf w^T\mathbf x_n &gt; 0$表示点被正确的分类，$\mbox{margin}(\mathbf w)$的定义可以保证判别界在两类的“中间”，不会偏向任何一边。</p>

<h2 id="section-1">支持向量机</h2>

<p>与线性感知器的定义不同，定义$b= w_0, \mathbf w=\left[ w_1, \ldots,  w_d\right]^T$，$\mathbf x=\left[ x_1, \ldots,  x_d\right]^T$。点到判别界的距离可表示为
\begin{equation*}
\mbox{distance}(\mathbf x, b, \mathbf w)=\frac{1}{\lVert\mathbf w\rVert}\left\lvert\mathbf w^T\mathbf x + b\right\rvert，
\end{equation*}
因此可得最大间隔
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad\mbox{margin}(b, \mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 0\\
&amp;\quad\mbox{margin}(b, \mathbf w)=\min\limits_{n=1,\ldots,N}\frac{1}{\lVert\mathbf w\rVert}y_n\left(\mathbf w^T\mathbf x_n + b\right)。
\end{aligned}
\end{equation*}
通过系数缩放，总可以做到$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$，最大间隔问题可以转化为等价问题
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad{1\over\lVert\mathbf w\rVert}\\
\mbox{subject to}&amp; \min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1。
\end{aligned}
\end{equation*}
上述条件$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$可以用等价的条件$y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1$代替。如果仅仅是$y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 1$而无法取“$=$”，那么不等式两边可以除以大于$1$的系数，使得条件仍然成立，$\mathbf w$除以大于$1$的系数后，${1\over\lVert\mathbf w\rVert}$会更大。因此，标准形式的最优化是
\begin{equation}
\begin{aligned}
\min\limits_{b, \mathbf w} &amp;\quad{1\over 2}\mathbf w^T\mathbf w\\
\mbox{subject to}&amp; \quad y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1\mbox{ for all }n。
\end{aligned}
\label{eq:linear-svm-model}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png" alt="支持向量" /></a><div class="caption">Figure 2:  支持向量 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png">PNG</a>]</div></div></div>

<p>通过求解可以发现，只有部分点对解有作用，如上图边界上方框内的点，这些点称为<strong>候选</strong>支持向量（support vector）。支持向量机就是利用支持向量学习“最胖”判别界。</p>

<h2 id="section-2">二次规划求解</h2>

<p>求解最佳判别界的是一个二次规划问题（QP，quadratic programming）。二次规划的标准形式为
\begin{equation}
\begin{aligned}
\mbox{optimal} &amp;\quad\mathbf u\leftarrow QP(\mathbf Q, \mathbf p, \mathbf A, \mathbf c) \\
\min\limits_{\mathbf u} &amp;\quad{1\over 2}\mathbf u^T\mathbf Q\mathbf u + \mathbf p^T\mathbf u\\
\mbox{subject to}&amp;\quad\mathbf a_n^T\mathbf u\geq c_n\mbox{ for }n=1,2,\ldots,N。
\end{aligned}
\label{eq:qp-standard-format}
\end{equation}
支持向量机利用二次规划求解时参数为
\begin{equation*}
\mathbf u =<br />
\begin{bmatrix}
b\\
\mathbf w
\end{bmatrix}
，
\end{equation*}
目标函数系数为
\begin{equation*}
\mathbf Q = 
\begin{bmatrix}
0 &amp;\mathbf 0_d^T\\
\mathbf 0_d &amp;\mathbf I_d
\end{bmatrix};
\mathbf p=\mathbf 0_{d+1}
，
\end{equation*}
约束条件系数为
\begin{equation*}
\mathbf a_n^T=y_n\left[1\quad\mathbf x_n^T\right];c_n=1。
\end{equation*}</p>

<p>这里所讲的支持向量机叫做线性hard-margin支持向量机，其中hard-margin是指所有的点能被正确分类。</p>

<p>利用$\mathbf z_n=\Phi\left(\mathbf x_n\right)$，用$\mathbf z_n$替代$\mathbf x_n$，可以得到非线性的支持向量机。</p>

<h2 id="section-3">最大边界的价值</h2>

<p>正则化（regularization）通过约束条件$\mathbf w^T\mathbf w\leq C$最小化$E_{in}$，支持向量机通过$E_{in}=0$等约束条件最小化$\mathbf w^T\mathbf w$。因此，支持向量机也可以看作是一种<strong>特殊的正则化方法</strong>。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png" alt="无法打碎3个点的情况" /></a><div class="caption">Figure 3:  无法打碎3个点的情况 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png">PNG</a>]</div></div></div>

<p>对于最大边界算法$\mathcal A_{\rho}$，脚标表示分界线的宽度要大于$\rho$。当$\rho = 0$时就是PLA，它可以打碎2维平面的3个输入点；若$\rho＝1.126$，如上图所示，无法打碎3个点。</p>

<p>如果加入了$\rho$条件的限制，可能的二分类情况少了，可认为<strong>VC维更小了，有更好的泛化性能</strong>。对于算法的VC维，有数据依赖的VC维比没有依赖的要低，$d_{VC}\left(\mathcal A_\rho\right)&lt;d_{VC}(\mathcal H)$，有数据依赖意味着加入了跟多的约束条件。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png" alt="单位圆上的数据" /></a><div class="caption">Figure 4:  单位圆上的数据 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png">PNG</a>]</div></div></div>

<p>当$\mathcal X$是上图所示半径为$R$的单位圆上点时，
\begin{equation}
d_{VC}\left(\mathcal A_\rho\right)\leq\min\left({R^2\over \rho^2},d\right)+1\leq d+1。
\end{equation}</p>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png" alt="各种判别界的对比" /></a><div class="caption">Figure 5:  各种判别界的对比 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png">PNG</a>]</div></div></div>

<p>几种情况的判别界对比如上图所示，large-margin的判别界简单且数量较少，有特征变换$\Phi$的判别界较复杂但是数量较多。</p>

<p>较少的判别界对$d_{VC}$和泛化有利，复杂的判别界可能得到更好的$E_{in}$。非线性支持向量机同时具备大边界的判别界和采用多种特征变换$\Phi$，因此判别界较少而且比较复杂，兼顾了这两方面的优点。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：噪声与误差</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-noise-and-error" />
      <pubdate>2014-12-28T02:56:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-noise-and-error</guid>
      <content:encoded>&lt;![CDATA[<p>如果数据存在噪声，对机器学习可学习理论推导有何影响？</p>

<h2 id="section">噪声起源</h2>

<p>对于$f:\mathcal X\rightarrow \mathcal Y$，噪声即可能来源于$\mathcal X$，也可能来源于$\mathcal Y$。以信用卡申办信息为例说明（$\mathcal X$为客户资料，$\mathcal Y$为是否办理信用卡）：</p>

<ul>
  <li>$y$的噪声：良好纪录的用户，被标记为Bad；</li>
  <li>$y$的噪声：同样资料的用户，标记却不同（不同专家评价有差异）；</li>
  <li>$\mathbf x$的噪声：不准确的客户资料。</li>
</ul>

<h2 id="target-distribution">目标分布<sup id="fnref:target-distribution"><a href="#fn:target-distribution" class="footnote">1</a></sup></h2>

<p>由于噪声的存在，弹珠模型的弹珠不再是确定的颜色，而是会随时变色，但是通过记录抽出瞬间弹珠的颜色，仍可以对罐中弹珠颜色概率做估计。$y=f(\mathbf x) + \mbox{noise}$，$y$用<strong>目标分布</strong>表示
\begin{equation}
y\sim P(y|\mathbf x)。
\end{equation}
对于满足$(\mathbf x,y)\sim P(\mathbf x,y)$的噪声数据，VC界仍成立，通过$E_{in}$仍然可以估计$E_{out}$。
如果$P(\circ|\mathbf x)=0.7$，$P(\times|\mathbf x)=0.3$，<strong>理想最小目标函数</strong>（ideal mini-target）为$f(\mathbf x)=\circ$，在最佳选择下噪声水平是$0.3$。对于确定的（无噪声）的目标函数$f$，也可用目标分布（target distribution）表示
\[
P(y|\mathbf x)=
\left\{
\begin{aligned}
1&amp;\quad\mbox{for }y=f(\mathbf x)\\
0&amp;\quad\mbox{for }y\neq f(\mathbf x)。
\end{aligned}
\right.
\]</p>

<p>机器学习的目标就是预测常见数据（w.r.t. $P(\mathbf x)$）的理想最小目标函数（w.r.t. $P(y|\mathbf x)$）。</p>

<p>由于噪声的存在，并且$\mathcal D$也是通过采样得到的，从数据$\mathcal D$无法得出目标函数$f$是线性还是非线性等特征。</p>

<h2 id="section-1">误差度量</h2>

<p>回顾out-of-sample误差度量\[E_{out} = \varepsilon_{\mathbf x\sim P}[[g(\mathbf x)\neq f(\mathbf x)]]，\]它是基于每个点的误差度量（pointwise error measure），$[[\mbox{prediction}\neq\mbox{targe}]]$度量的分类错误称为<strong>0/1错误</strong>。基于每个点的in-sample误差度量为\[E_{in} = {1\over N}\sum_{n=1}^Nerr\left(g\left(\mathbf x_n\right),f\left(\mathbf x_n\right)\right)。\]</p>

<p>令$\tilde y=g(\mathbf x)，y=f(\mathbf x)$，0/1误差和<strong>平方误差</strong>分别定义为\[err(\tilde y,y)=[[\tilde y\neq y]]\qquad err(\tilde y,y)=(\tilde y- y)^2。\]</p>

<p>误差是如何影响学习算法的呢？</p>

<p>下面示例展示了不同度量方式下误差的差异：</p>

<p><img src="/assets/images/2014-12-27-machine-learning-noise-and-error-ideal-mini-target-example.png" alt="误差度量的影响" /></p>

<p>$P(y|\mathbf x)$和$err$确定了理想最小目标函数$f(\mathbf x)$。在0/1误差和平方误差度量方式下，理想最小目标函数分别为
\[
f(\mathbf x)=\arg\max_{y\in\mathcal Y}P(y|\mathbf x)\qquad f(\mathbf x)=\sum_{y\in\mathcal Y}yP(y|\mathbf x)。
\]
若采用平方误差度度量方式，上例中$f(\mathbf x)=1.9$。</p>

<p>根据具体应用场景，选择相应的错误（误差）衡量方法，对机器学习至关重要。</p>

<p>VC理论的推导不一定依赖于目标函数$f(\mathbf x)$，只需目标分布。VC理论对很多假设集$\mathcal H$和误差衡量$err$都有效，比如将定义稍作修改可将VC理论推广到回归分析可得到类似的结果。</p>

<h2 id="section-2">误差度量范例</h2>

<p>以指纹识别为例，介绍误差度量。$+1$表示合法用户，$-1$表示入侵者。指纹识别犯的两种错误：</p>

<ol>
  <li>false reject：合法用户识别为非法；</li>
  <li>false accept：非法用户识别为合法。</li>
</ol>

<p>在不同的应用场景下，两种错误导致的损失不一样。在设计算法的时候，须考虑将误差的度量方式。设计算法的时候，不仅要考虑误差定义的合理性，还要考虑算法$\mathcal A$是否容易优化（比如：闭式解或者凸目标函数）。误差的度量是算法的关键。</p>

<p>对于错分的不同损失，可定义如下的加权误差度量方式
\[
E_{in}^w(h)={1\over N}\sum_{n=1}^N
\left\{
\begin{aligned}
&amp;1 &amp;\quad\mbox{if }y_n=+1\\
&amp;1000 &amp;\quad\mbox{if }y_n=-1
\end{aligned}
\right\}
\cdot \left[\left[y_n\neq h(\mathbf x_n)\right]\right]。
\]
对于线性可分数据的PLA，这种定义并无影响。对于pocket算法，假设将$-1$样本复制$1000$倍，在新的数据上仍用$E_{in}^{0/1}$度量误差，就和在原数据上用$E_{in}^{w}$度量一致。</p>

<p>实际使用中，不会真的复制数据，采取虚拟复制（virtual copying）的策略。</p>

<blockquote>
  <h4 id="pocket">加权pocket算法</h4>
  <hr />

  <ol>
    <li>以$1000$倍的概率检查$-1$样本犯的错误；       </li>
    <li>当$\mathbf w_{t+1}$犯的错误$E_{in}^w$比$\hat{\mathbf w}$小时，用$\hat{\mathbf w}$更新$\mathbf w_{t+1}$。</li>
  </ol>
</blockquote>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-4">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:target-distribution">
      <p>不太明白这节的意思…… <a href="#fnref:target-distribution" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>Map-Reduce Essential</title>
      <link href="http://qianjiye.de/2014/12/map-reduce-essential" />
      <pubdate>2014-12-25T02:08:25+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/map-reduce-essential</guid>
      <content:encoded>&lt;![CDATA[<h2 id="map-reduce">为什么需要Map-Reduce？</h2>

<h3 id="cluster">集群（cluster）</h3>

<p>在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？</p>

<p>即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！</p>

<p>需要这样一个集群……</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-cluster-architecture.png"><img src="/assets/images/2014-12-24-map-reduce-cluster-architecture.png" alt="集群架构" /></a><div class="caption">Figure 1:  集群架构 [<a href="/assets/images/2014-12-24-map-reduce-cluster-architecture.png">PNG</a>]</div></div></div>

<h3 id="map-reduce-1">Map-Reduce解决集群带来的挑战</h3>

<h4 id="node-failures">一、节点故障（node failures）</h4>

<p>如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题：</p>

<ul>
  <li>如何存储数据，即使节点故障时仍可用？</li>
  <li>若正在进行大规模计算，如果节点发生故障该如何处理？</li>
</ul>

<p>Map-Reduce在多个节点冗余存储，保证数据持久存储和获取。</p>

<h4 id="network-bottleneck">二、网络瓶颈（network bottleneck）</h4>

<p>Map-Reduce的计算靠近数据端，减少数据移动。</p>

<h4 id="section">三、分布式程序编写困难</h4>

<p>Map-Reduce简单的编程模型，隐藏了复杂的细节。</p>

<h2 id="map-reduce-2">Map-Reduce简介</h2>

<h3 id="redundant-storage-infrastructure">冗余存储架构（redundant storage infrastructure）</h3>

<p>冗余存储架构采用分布式文件系统（distributed file system），例如：Google GFS、Hadoop HDFS。典型的应用是处理大文件，一次存储多次读取追加更新。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png"><img src="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png" alt="数据分块存储" /></a><div class="caption">Figure 2:  数据分块存储 [<a href="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png">PNG</a>]</div></div></div>

<p>数据分块（chuck）存储在多台服务器。如上图所示，一个大文件分割成C0～C5共6块，每块在多台服务器存储备份。每台存储服务器也做计算用，使得计算靠近存储端。</p>

<h3 id="computational-model">计算模型（computational model）</h3>

<blockquote>
  <h4 id="map-reduce-3">Map-Reduce计算模型</h4>
  <hr />
  <p>输入：key-value对的集合     <br />
程序实现以下两个模块：</p>

  <ol>
    <li>Map(k,v) —&gt; &lt;k’, v’&gt;*
      <ul>
        <li>输入一个key-value对，输出多个key-value对；</li>
        <li>对所有的(k,v)对，只有一个Map函数。 </li>
      </ul>
    </li>
    <li>Reduce(k’, &lt;v’&gt;*) —&gt; &lt;k’, v”&gt;
      <ul>
        <li>所有的具有相同k’的v’都被reduce到一起；</li>
        <li>对同一个k’，只有一个Reduce函数。</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Map-Reduce的计算模型分为Map和Reduce两步，Map分布式处理任务，Reduce合并任务。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-computational-model.png"><img src="/assets/images/2014-12-24-map-reduce-computational-model.png" alt="Map-Reduce单词计数实例" /></a><div class="caption">Figure 3:  Map-Reduce单词计数实例 [<a href="/assets/images/2014-12-24-map-reduce-computational-model.png">PNG</a>]</div></div></div>

<p>上图展示了用Map-Reduce统计超大规模文件中单词出现次数，红色横线将不同节点的实现分割开。对于Map节点，所有相同单词都输出到同一个节点，比如the都在第二个节点。为了保证效率，Map-Reduce都采用的是顺序读取。</p>

<h3 id="scheduling-and-data-flow">调度与数据流（scheduling and data flow）</h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-diagram.png"><img src="/assets/images/2014-12-24-map-reduce-diagram.png" alt="Map-Reduce结构" /></a><div class="caption">Figure 4:  Map-Reduce结构 [<a href="/assets/images/2014-12-24-map-reduce-diagram.png">PNG</a>]</div></div></div>

<p>Map-Reduce的数据流：</p>

<ul>
  <li>输入输出存储在分布式文件系统；</li>
  <li>中间结果存储在本地文件系统；</li>
  <li>输出通常再输入到另一个Map-Reduce任务。</li>
</ul>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-parallel.png"><img src="/assets/images/2014-12-24-map-reduce-parallel.png" alt="Map-Reduce的并行实现" /></a><div class="caption">Figure 5:  Map-Reduce的并行实现 [<a href="/assets/images/2014-12-24-map-reduce-parallel.png">PNG</a>]</div></div></div>

<p>上图是Map-Reduce分布式系统的并行实现，Partition Function部分采用Hash算法，将相同key的value映射到同一节点。</p>

<p>Map-Reduce环境的主要任务：</p>

<ul>
  <li>分割输入数据；</li>
  <li>多机之间程序调度；</li>
  <li>执行按key分组操作；</li>
  <li>处理节点故障；</li>
  <li>处理多机间通信。</li>
</ul>

<p>Map-Reduce的实现分为Master节点、Map节点和Reduce节点，Master节点的任务：</p>

<ul>
  <li>管理每个任务状态：空闲（idle，等待处理）、处理中（in-progress）、completed（结束）；</li>
  <li>将空闲任务安排到可用节点；</li>
  <li>当Map任务结束，向Master发送其R中间文件（存放在本地文件系统中）的位置和大小，每个reducer一个中间文件；</li>
  <li>Master推送信息到Reducer；</li>
  <li>Master周期性ping检测节点是否出故障。</li>
</ul>

<p>Map-Reduce系统有M个Map任务和R个Reduce任务，M比集群中的节点数目大得多，R通常比M小。</p>

<h2 id="map-reduce-4">Map-Reduce的改进</h2>

<h4 id="section-1">一、合并操作</h4>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-mapper-combiner.png"><img src="/assets/images/2014-12-24-map-reduce-mapper-combiner.png" alt="合并操作" /></a><div class="caption">Figure 6:  合并操作 [<a href="/assets/images/2014-12-24-map-reduce-mapper-combiner.png">PNG</a>]</div></div></div>

<p>通常在一个Map任务中会产生多个相同key的(k,v)对，在Map节点合并这些相同的key可有效降低网络流量，如上图所示。合并函数通常与Reduce函数相同。</p>

<p>合并时需要注意Reduce函数是否支持在Map节点的合并操作，也就是合并操作会不会改变Reduce的结果。</p>

<h4 id="section-2">二、改写分割函数</h4>

<p>例如：系统采用的默认分割函数<code>hash(key) mod R</code>可以改写为<code>hash(hostname(URL)) mod R</code>，使同一个主机的url输出到相同的文件。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：VC维</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-the-vc-dimension" />
      <pubdate>2014-12-24T05:02:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-the-vc-dimension</guid>
      <content:encoded>&lt;![CDATA[<p>本节的主要内容来自Hsuan-Tien Lin的机器学习基石课程<a href="#lin_ml_vcd_2014">[1]</a>。</p>

<p>Learning happens, if finite $d_{VC}$, large $N$, and low $E_{in}$.</p>

<h2 id="section">学习的可行性</h2>

<p>当$N\geq 2,k\geq 3$时，成长函数满足约束条件
\begin{equation}
m_{\mathcal H}(N)\leq B(N,k)=\sum_{i=0}^{k-1}\binom{N}{i}\leq N^{k-1}，
\end{equation}
由此可见，断点$k$是判断成长函数大小的重要条件。当$k\geq 3$时，才能使用上界约束$N^{k-1}$。</p>

<p>对于$\forall g=\mathcal A(\mathcal D)\in\mathcal H$和足够大的数据集$\mathcal D$，当$k\geq 3$时，
\begin{equation}
\begin{aligned}
P_{\mathcal D}\left[\lvert E_{in}(g)-E_{out}(g)\rvert&gt;\epsilon\right] \leq &amp; P_{\mathcal D}\left[\exists h\in\mathcal H\mbox{ s.t. }\lvert E_{in}(h)-E_{out}(h)\rvert&gt;\epsilon\right] \\
\leq &amp; 4m_{\mathcal H}(2N)\exp\left(-{1\over 8}\epsilon^2N\right)\\
\leq &amp; 4(2N)^{k-1}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{aligned}
\end{equation}</p>

<p>由此可知，学习是可行的，前提是满足下列条件：</p>

<ol>
  <li>好的假设集$\mathcal H$：$m_{\mathcal H}(N)$的断点是$k$；</li>
  <li>好的数据集$\mathcal D$：$N$足够大；</li>
  <li>好的演算法$\mathcal A$：$\mathcal A$能够选到一个$g$使得$E_{in}(g)$很小。</li>
</ol>

<p>其中，前两个条件可能得到$E_{out}\approx E_{in}$。</p>

<h2 id="vc">VC维</h2>

<p>$\mathcal H$的VC维$d_{VC}是指$满足$m_{\mathcal H}(N)=2^N$的最大$N$，VC维也满足：</p>

<ul>
  <li>$\mathcal H$可以打碎的最多输入数据个数；</li>
  <li>$d_{VC}=\mbox{‘minmum }k\mbox{‘} - 1$。</li>
</ul>

<p>若$N\leq d_{VC}$，$\mathcal H$能够打碎某些$N$个点的数据子集；若$k &gt; d_{VC}$，$k$必是$\mathcal H$的断点；如果$N\geq 2, d_{VC}\geq 2$， 显然有
\begin{equation}
m_{\mathcal H}(N)\leq N^{d_{VC}}。
\end{equation}</p>

<p>1维空间的正射线和正区间的$d_{VC}$分别是$1$和$2$；2维空间感知器的$d_{VC}=3$；2维空间凸包的$d_{VC}=\infty$。</p>

<p>有限$d_{VC}$的$\mathcal H$就是好的假设集。若$d_{VC}$有限，存在$g$使得$E_{out}(g)\approx E_{in}(g)$，并且不需要受学习算法$\mathcal A$、输入数据分布$P$和目标函数$f$的制约。</p>

<blockquote>
  <h4 id="section-1">练习题</h4>
  <hr />
  <p>若存在$N$个点的数据集不能被$\mathcal H$打碎，仅仅根据这条信息，可以得到关于$d_{VC}(H)$的什么结论？       </p>

  <p>［A］$d_{VC}(\mathcal H)&gt;N$；［B］$d_{VC}(\mathcal H)=N$；［C］$d_{VC}(\mathcal H)&lt;N$；［D］无法得出以上任何结论。</p>

  <p>答案：［D］。</p>
</blockquote>

<h2 id="vc-1">感知器的VC维</h2>

<p>如何证明$d$维空间感知器的VC维$d_{VC}=d+1$？若能证明$d_{VC}\geq d+1$且$d_{VC}\leq d+1$，那么就可以得到$d_{VC}=d+1$。</p>

<blockquote>
  <h4 id="dvcgeq-d1">练习题：以下哪个表明$d_{VC}\geq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［A］。</p>
</blockquote>

<p>上面练习表明，只要在$d$维空间找到一组$d+1$个点的数据集能被感知器打碎（$d+1$个点的所有二分法都能实现），那么就证明了$d_{VC}\geq d+1$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png" alt="d维空间的一组数据点" /></a><div class="caption">Figure 1:  d维空间的一组数据点 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png">PNG</a>]</div></div></div>

<p>这组数据点如上图所示，红色框中的数据点表述了上方$2$维空间3个点的坐标。最左边1列灰色的$1$，表示感知器的偏移常量。</p>

<p>易知，$\mathbf X$是$(d+1)\times (d+1)$的方正，且所有列线性无关（秩为$d+1$）。因此，$\mathbf X$可逆，且是$d+1$维线性空间的基。若$\mathbf y$表示任意一种二分类结果（$\mathbf X$可被打碎），那么这样的二分类方式$\mathbf w=\mathbf X^{-1}\mathbf y$总存在，因此$d_{VC}\geq d+1$。</p>

<blockquote>
  <h4 id="dvcleq-d1">练习题：以下哪个表明$d_{VC}\leq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［D］。</p>
</blockquote>

<p>上面练习表明，若$d$维空间任意$d+2$个点的数据集均不能被感知器打碎，那么就证明了$d_{VC}\leq d+1$。</p>

<p>若$d$维空间中任取$d+2$个点，都存在一种不能实现的二分类方法，那么这$d+2$个点就不能被打碎。</p>

<p>补上常数项$1$。因为$d+2$个$d+1$维的向量必定线性相关，任意一个均可用其它$d+1$个表示，那么
\[
\mathbf x_{d+2} = a_1\mathbf x_{1} + a_2\mathbf x_{2} + \ldots a_{d+1}\mathbf x_{d+1}，
\]
其中，$a_i(i=1,2,\dots,d+1)$不全为$0$。上式两边同时乘以$\mathbf w^T$可得
\[
\mathbf w^T\mathbf x_{d+2} = a_1\mathbf w^T\mathbf x_{1} + a_2\mathbf w^T\mathbf x_{2} + \ldots a_{d+1}\mathbf w^T\mathbf x_{d+1}。
\]
若要所有二分法都可行，上式不论右边$\mathbf x_i(i=1,2,\dots,d+1)$取何值，左边$\mathbf w^T\mathbf x_{d+2}$既可取正又可取负。采用反证法，找到一组左边$\mathbf x_i(i=1,2,\dots,d+1)$的取值，使得$\mathbf w^T\mathbf x_{d+2}$只能取正（或者负）。</p>

<p>假设这$d+2$个点能被打碎，一定存在一种分类情况使得$\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)$的符号与$a_i(i=1,2,\dots,d+1)$的符号相同，那么$a_i\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)&gt;0$，这样$\mathbf w^T\mathbf x_{d+2}$就只能取正。那么，这$d+2$个点不能被打碎，与假设矛盾。因此，$d$维空间中，任意$d+2$个点都存在不可能二分类的情况，也就是$d$维空间中的任何$d+2$个输入都不能被打碎，那么$d_{VC}\leq d+1$。</p>

<h2 id="vc-2">理解VC维</h2>

<p>VC维可以理解为假设集$\mathcal H$的自由度，它衡量了$\mathcal H$的分类能力。$d_{VC}$可以直观的认为是$\mathcal H$可调节参数的个数（但不总是这样）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png" alt="VC维相当于自由度" /></a><div class="caption">Figure 2:  VC维相当于自由度 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png">PNG</a>]</div></div></div>

<p>如上图所示，正射线只有一个可以调节参数，自由度是$1$，$d_{VC}=1$；正区间有两个可以调节参数，自由度是$2$，$d_{VC}=2$。感知器的参数向量$\mathbf w = (w_0,w_1,\ldots,w_d)$是$d+1$维（有$d+1$个可自由调节参数）和它的$d_{VC}$一致。</p>

<p>过原点的感知器的$d_{VC}=d$，因为少了一个自由度。</p>

<p>对于机器学习是否可行，有两个判断指标：（1）$E_{out}(g)\approx E_{in}(g)$？（2）$E_{in}(g)$是否足够小。$d_{VC}$（或$M$）可以作为这两个条件的评价指标。若$d_{VC}$较小，更大概率保证$E_{out}(g)\approx E_{in}(g)$，但$\mathcal H$的能力较弱，可选择的假设较少，难以使$E_{in}(g)$较小；若$d_{VC}$较大，$\mathcal H$的能力较强，可选择的假设较多，更容易使$E_{in}(g)$较小，但$E_{out}(g)\approx E_{in}(g)$的概率偏低。</p>

<h2 id="vc-3">应用VC维</h2>

<p>对于$\forall g=\mathcal A(\mathcal D)\in \mathcal H$和大的数据集$\mathcal D$，当$d_{VC}\geq 2$时，坏事儿发生概率的$VC$界为
\begin{equation}
P_{\mathcal D}\left[\left\lvert E_{in}(g)-E_{out}(g)\right\rvert&gt;\epsilon\right]\leq 4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{equation}
令$\delta=4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)$，可得$\Omega(N,\mathcal H,\delta)=\epsilon=\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}$，那么可得$E_{out}(g)$的上界
\begin{equation}
E_{out}(g)\leq E_{in}(g)+\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}。
\end{equation}</p>

<p>$\Omega(N,\mathcal H,\delta)$用于度量模型的复杂度，揭示了限定$E_{out}(g)$和$E_{in}(g)$差异的方法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png" alt="VC维与误差的关系" /></a><div class="caption">Figure 3:  VC维与误差的关系 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png">PNG</a>]</div></div></div>

<p>上图展示了VC维和误差的关系，随着VC维$d_{VC}$的增加，模型越来越复杂，但是误差并非越来越小，$E_{out}(g)$和$E_{in}(g)$的差异却越来越大，发生坏事儿的概率变大了。由此可见，强大的$\mathcal H$（$d_{VC}$大）不总是好事儿，要选择合适的$d_{VC}^*$。</p>

<p>给定指标$\epsilon=0.1,\delta=0.1,d_{VC}=3$，可以计算大约需要$N\approx 30000$的数据集能满足要求。</p>

<p>理论上需要$N\approx 10000d_{VC}$才能满足要求，实际上通常只需$N\approx 10d_{VC}$，这是因为在推导$VC$界的时候，不等式不断放大，得到的是一个很宽松的上界。</p>

<p>$d_{VC}$虽是一个宽松的值，但可认为对所有模型都<strong>同样一致</strong>宽松，因此在模型之间比较时，仍有重要作用。</p>

<h2 id="dvc">$d_{VC}$容易计算吗？</h2>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="lin_ml_vcd_2014">[1]H.-T. Lin, “Lecture 7: The VC Dimension.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
