<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-03-19T04:57:20+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>统计学习：线性回归</title>
      <link href="http://qianjiye.de/2015/03/statistical-learning-linear-regression" />
      <pubdate>2015-03-13T20:06:43+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/statistical-learning-linear-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">应用场景</h2>

<p>线性回归是研究其它一些统计学习方法的起点，其它方法可看作线性回归的扩展。</p>

<p>在<a href="http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv">Advertising数据集</a>，某种商品销售额（sales）可视为电视（TV）、广播（radio）和报纸（newspaper）广告预算的函数。通过这些数据，能否解决如下问题：</p>

<ol>
  <li>广告预算和销量有关吗？</li>
  <li>如果相关，这种相关性有多强？也就是给定广告预算，能否准确预测销售额？</li>
  <li>各种类型的广告对销售额都有影响么吗？</li>
  <li>如果准确评估每种广告对销售额的影响？</li>
  <li>如何准确预测未来销售额？</li>
  <li>广告和销售额是线性关系吗？</li>
  <li>广告媒体之间有<strong>协同效应</strong>（synergy effect）吗？协同效应在统计学中称为<strong>相关性</strong>（interaction effect）。例如：在电视和广播分别投放\$50000的广告和将这\$100000投放到其中一种媒体，效果一样吗？</li>
</ol>

<h2 id="section-1">回归模型</h2>

<p>数学上，回归问题可表示为
\begin{equation}
Y\approx \beta_0 + \beta_1 X，
\end{equation}
$X$可以指代电视，$Y$表示销售额。$\beta_0 $和$\beta_1$分别表示<strong>截距</strong>（intercept）和<strong>斜率</strong>（slope）。模型参数用训练数据上的最小二乘法估计可得
\begin{equation}
\begin{aligned}
\hat\beta_1&amp;={\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\over\sum_{i=1}^n(x_i-\bar x)^2}\\
\hat\beta_0&amp;=\bar y-\hat\beta_1\bar x，
\end{aligned}
\label{eq:lsm-coefficients}
\end{equation}
销售额可以预测
\[
\hat y=\hat\beta_0+\hat\beta_1x，
\]
符号$\hat{\;}$表示估计值。第$i$个样本的<strong>残差</strong>（residual）记为$e_i=y_i-\hat y_i$，$n$个样本的<strong>残差平方和</strong>（RSS，residual sum of squares）为
\begin{equation}
RSS=e_i^2+e_2^2+\ldots+e_n^2。
\label{eq:rss}
\end{equation}</p>

<p>假设$X$和$Y$的真实关系为
\begin{equation}
Y=\beta_0+\beta_1X+\epsilon，
\label{eq:population-regression-line}
\end{equation}
截断$\beta_0$表示当$X=0$是$Y$的取值，斜率$\beta_1$表示$X$增加一个单位$Y$的平均增加量，误差$\epsilon$代表了这个简单模型缺失的其它所有量。$X$和$Y$的真实关系可能不是线性的，还可能有其它量导致$Y$的改变，也可能有测量误差。通常假定误差项与$X$独立。</p>

<p>模型\eqref{eq:population-regression-line}定义了<strong>总体回归直线</strong>（population regression line），它是$X$和$Y$真实关系的最佳线性近似。最小二乘法估计的系数\eqref{eq:lsm-coefficients}确定了<strong>最小二乘直线</strong>（least squares line）。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png"><img src="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png" alt="线性回归模型" /></a><div class="caption">图 1:  线性回归模型 [<a href="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png">PNG</a>]</div></div></div>

<p>上图左中，红色直线表示真实关系$f(X)=2+3X$，也就是总体回归线，深蓝色直线表示最小二乘直线，它是基于数据的最小二乘估计；上图右的浅蓝色直线是10次最小二乘法估计的结果，每次估计采用的不同数据集，但是10次估计的平均值很接近总体回归线。</p>

<p>每次实验的100组数据通过模型$Y=2+3X+\epsilon$随机生成，$\epsilon$由零均值的正态分布产生。在实际应用中，只能通过数据得到最小二乘直线，无法观察到总体回归直线。</p>

<p>在一个数据集上通过\eqref{eq:lsm-coefficients}无法得到参数的准确估计，但是通过在大量数据集上估计结果的平均，最小二乘直线就是总体回归直线的无偏（unbiased）估计。</p>

<p>参数估计估计的精度可以通过<strong>标准误差</strong>（SE，standard error）衡量，</p>

<p>\begin{equation}
\begin{aligned}
SE\left(\hat\beta_0\right)^2&amp;=\sigma^2\left[{1\over n}+{\bar x^2\over\sum_{i=1}^n(x_i-\bar x)^2}\right]\\
SE\left(\hat\beta_1\right)^2&amp;={\sigma^2\over\sum_{i=1}^n(x_i-\bar x)^2}，
\end{aligned}
\end{equation}</p>

<p>$\sigma$表示均方差（standard deviation），$\sigma^2=Var(\epsilon)$。</p>

<p>标准误差揭示了参数估计值和真实值之间的平均差异。公式严格成立的条件是每个样本的误差$\epsilon_i$和<strong>公共方差</strong>（common variance）无关。即使这个条件不成立，这个公式也可作为评估的很好近似。当$x_i$越分散时，$SE\left(\hat\beta_1\right)^2$越小；当$\bar x=0$时，$SE\left(\hat\beta_0\right)^2$也更小，此时$\hat\beta_0=\bar y$；随着样本数$n$的增加，$SE\left(\hat\beta_0\right)^2$也越来越小。通常$\sigma^2$未知，但可以通过数据集上的<strong>标准化残差</strong>（RSE，residual standard error）估计，</p>

<p>\begin{equation}
RSE=\sqrt{RSS\over n-2}。
\label{eq:rse}
\end{equation}</p>

<p>标准误差可以用于计算<strong>置信区间</strong>（confidence interval）。$95\%$的置信区间表示该区间有$95\%$的概率包括参数的真实值。对于线性回归的参数估计，$95\%$的置信区间记为<sup id="fnref:confidence-interval"><a href="#fn:confidence-interval" class="footnote">1</a></sup>：</p>

<p>\begin{equation}
\hat\beta_0\pm 2\cdot SE\left(\hat\beta_0\right)，\qquad
\hat\beta_1\pm 2\cdot SE\left(\hat\beta_1\right)。
\label{eq:lr-confidence-interval}
\end{equation}</p>

<p>标准误差也可用于系数的<strong>假设检验</strong>（hypothesis test）。最常用的两种假设检验是<strong>零假设检验</strong>（null hypothesis）和<strong>替代假设检验</strong>（alternative hypothesis），
\[
\begin{aligned}
H_0&amp;:\mbox{There is no relationship between }X\mbox{ and }Y\\
H_a&amp;:\mbox{There is some relationship between }X\mbox{ and }Y，
\end{aligned}
\]
数学上表示为
\[
H_0:\beta_1=0，\qquad H_a:\beta_1\neq0。
\]
若$\beta_1=0$，那么$X$和$Y$无关。进行零假设检验时，需要估计$\hat\beta_1$是否离0足够远，这需要通过$\hat\beta_1$的精度$SE\left(\hat\beta_1\right)$来衡量。如果$SE\left(\hat\beta_1\right)$很小，较小的$\hat\beta_1$可以足够确信$\beta_1\neq 0$；如果$SE\left(\hat\beta_1\right)$很大，$\hat\beta_1$的绝对值要很大才能拒绝零假设。在实际应用中，通过<strong>$t$统计量</strong>（$t$-statistic）</p>

<p>\begin{equation}
t={\hat\beta_1-0\over SE\left(\hat\beta_1\right)}
\label{eq:t-statistic}
\end{equation}</p>

<p>测量$\hat\beta_1$远离$0$的标准差（standard deviation）。如果$X$和$Y$真的无关，\eqref{eq:t-statistic}是$n-2$自由度的<strong>$t$分布</strong>（$t$-distribution）。当$n$大约大于$30$时，$t$分布与正态分布很相似。<strong>$p$值</strong>（$p$-value）表示$\beta_1=0$时观测值大于等于$|t|$的概率。较小的$p$值表示预测（predictor）与响应（response）之间有关联。若$p$值很小时，拒绝零假设（reject the null hypothesis）表示$X$与$Y$有关系。拒绝零假设的典型$p$值是$5\%$或$1\%$，当$n=30$时，相应的$t$统计量分别为$2$和$2.75$。<sup id="fnref:t-and-p-means"><a href="#fn:t-and-p-means" class="footnote">2</a></sup></p>

<div class="highlight"><pre><code>Call:
lm(formula = Price ~ AGST + WinterRain + HarvestRain + Age + 
    FrancePop, data = wine)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.48179 -0.24662 -0.00726  0.22012  0.51987 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -4.504e-01  1.019e+01  -0.044 0.965202    
AGST         6.012e-01  1.030e-01   5.836 1.27e-05 ***
WinterRain   1.043e-03  5.310e-04   1.963 0.064416 .  
HarvestRain -3.958e-03  8.751e-04  -4.523 0.000233 ***
Age          5.847e-04  7.900e-02   0.007 0.994172    
FrancePop   -4.953e-05  1.667e-04  -0.297 0.769578    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.3019 on 19 degrees of freedom
Multiple R-squared:  0.8294,    Adjusted R-squared:  0.7845 
F-statistic: 18.47 on 5 and 19 DF,  p-value: 1.044e-06
</code></pre></div>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-3">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:confidence-interval">
      <p>Approximately for several reasons. Equation relies on the assumption that the errors are Gaussian. Also, the factor of $2$ in front of the $SE\left(\hat\beta_1\right)$ term will vary slightly depending on the number of observations $n$ in the linear regression. To be precise, rather than the number $2$, Equation should contain the $97.5\%$ quantile of a $t$-distribution with $n−2$ degrees of freedom.  <a href="#fnref:confidence-interval" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t-and-p-means">
      <p>如何理解$t$分布、$p$值？ <a href="#fnref:t-and-p-means" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>模式发掘（2）：高效的模式挖掘方法</title>
      <link href="http://qianjiye.de/2015/03/efficient-pattern-mining-methods" />
      <pubdate>2015-03-07T23:11:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/efficient-pattern-mining-methods</guid>
      <content:encoded>&lt;![CDATA[<h2 id="apriori">Apriori性质</h2>

<p><strong>向下闭包</strong>（downward closure）特性是指频繁项集的所有非空子集也必须是频繁的，这也称为<strong>Apriori性质</strong>。若$\{beer, diaper, nuts\}$是频繁的，那么$\{beer, diaper\}$也是，每个包含$\{beer, diaper, nuts\}$的事务必包含$\{beer, diaper\}$。</p>

<p>如果项集$S$的存在非频繁的子集，那么$S$必不是频繁的。如果项集是非频繁的，它的超集（superset）必是非频繁的，这称为<strong>Apriori剪枝原理</strong>（Apriori pruning principle）。基于这个原理，主要有三种可扩展的（scalable）频繁模式挖掘方法：Apriori、Eclat和FPgrowth。</p>

<h2 id="apriori-1">Apriori算法</h2>

<p>Apriori算法是第一个基于候选集生成与测试的频繁集挖掘方法。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png"><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png" alt="Apriori算法" /></a><div class="caption">图 1:  Apriori算法 [<a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png">PNG</a>]</div></div></div>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png"><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png" alt="Apriori算法示例" /></a><div class="caption">图 2:  Apriori算法示例 [<a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png">PNG</a>]</div></div></div>

<p>Airport算法生成候选项集的集合分两步：</p>

<ol>
  <li>自连接（self-join）$F_k$生成候选项集的集合；</li>
  <li>剪枝，删除候选项集的集合中的非频繁模式。</li>
</ol>

<p>若频繁模式的集合$F_3=\{abc,abd,acd,ace,bcd\}$，自连接可得候选模式的集合$C’_4=\{abcd, acde\}$，但是$ade\notin F_3$，因此剪枝后可得$C_4=\{abcd\}$。</p>

<p>基于SQL语句生成候选项集的集合：</p>

<p><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-SQL.png" alt="SQL implementation" /></p>

<h2 id="section">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-1">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>模式发掘（1）：基本概念</title>
      <link href="http://qianjiye.de/2015/03/pattern-discovery-basic-concepts" />
      <pubdate>2015-03-07T19:11:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/pattern-discovery-basic-concepts</guid>
      <content:encoded>&lt;![CDATA[<p><strong>模式</strong>（pattern）是数据集中频繁出现或强相关的条目、子序列或子结构组成的集合，它代表了数据集的本质特征和重要属性。<strong>模式发掘</strong>（pattern discovery）就是从大规模数据集中发现模式。</p>

<h2 id="section">模式发掘的重要性</h2>

<p>模式发掘的典型应用场景包括：</p>

<ul>
  <li>哪些商品常被一起购买？</li>
  <li>买了iPad之后还会买哪些商品？</li>
  <li>哪些代码片段可能包括复制粘贴错误（copy-and-paste bug）？</li>
  <li>语料库中哪些词串可能构成短语？</li>
</ul>

<p>模式发掘是从数据集中发现内在规则，在数据挖掘中有广泛应用：</p>

<ul>
  <li>关联分析、相关分析以及因果分析；</li>
  <li>挖掘序列模式、结构（例如：子图）模式；</li>
  <li>分析时空数据、多媒体、时间序列和留数据中的模式；</li>
  <li>基于判别模式分析的分类；</li>
  <li>基于模式子空间的聚类。</li>
</ul>

<p>模式发现广泛应用于购物篮分析（market basket analysis）、交叉营销（cross-marketing）、目录设计（catalog design）、销售活动分析（sale campaign analysis）、Web日志分析（Web log analysis）和生物序列分析（biological sequence analysis）。</p>

<h2 id="a-hrefagrawal1993mar1700361700721aa-hrefhan2007frequent2a">频繁模式与关联规则<a href="#Agrawal:1993:MAR:170036.170072">[1]</a><a href="#han2007frequent">[2]</a></h2>

<p>一个或多个项组成的集合称为<strong>项集</strong>（itemset），$k$-项集记为$ X=\{ x_1,\ldots,x_k\}$。<strong>绝对支持度</strong>（absolute support）是指项集$  X$在事务中出现的次数，<strong>相对支持度</strong>（relative support）是指包含项集$  X$的事务所占比率。如果项集$X$的支持度大于某个$minsup$阈值$\sigma$，则称$X$是<strong>频繁模式</strong>。</p>

<p><img src="/assets/images/2015-03-07-pattern-discovery-basic-concepts-itemset-table.png" alt="项集表格" /></p>

<p>令$minsup=50\%$，那么：</p>

<ul>
  <li>频繁的$1$-项集包括：$Beer: 3(60\%), Nuts: 3(60\%), Diaper: 4(80\%), Eggs:3(60\%)$；</li>
  <li>频繁的$2$-项集包括：$\{Beer, Diaper\}:3(60\%)$。</li>
</ul>

<p><strong>关联规则</strong>（association rule）记为$  X\rightarrow  Y(s,c)$，支持度$s$为包含项集$  X\cup  Y$事务出现的概率，<strong>置信度</strong>（confidence）$c$为包含项集$  X$的事务中包含项集$  X\cup  Y$的事务出现的概率，$c={\sup(  X\cup  Y)\over\sup(  X)}$。</p>

<p><strong>关联规则挖掘</strong>是找出大于最小支持度和置信度的所有关联规则$  X\rightarrow  Y$。令$minconf=50\%$，关联规则为
\[
Beer\rightarrow Diapper(60\%, 100\%),~Diaper\rightarrow Beer(60\%,75\%)。
\]</p>

<h2 id="section-1">闭模式与最大模式</h2>

<p>在频繁模式和关联规则挖掘时，一个长的模式包含很多子模式。</p>

<p>若模式（项集）$  X$是频繁的，并且不存在与$  X$有相同支持度的超模式（super-pattern）$  Y\supset  X$，则称$  X$为<strong>闭模式</strong>（closed pattern）<a href="#Pasquier1999discovering">[3]</a>。闭模式是频繁模式的无损压缩，虽降低了模式数量，但没有损失支持度信息。</p>

<p>若模式$X$是频繁的，并且不存在频繁的超模式$Y\supset X$，则称$X$为<strong>最大模式</strong>（max-pattern）<a href="#roberto667815">[4]</a>。最大模式不关注子模式的真实支持度，它是频繁模式的有损压缩，只知道子模式是频繁的，但不知道子模式的真实支持度。</p>

<p>令事务集$TDB_1$的事务为$T_1:\{a_1,\ldots,a_{50}\},T_2:\{a_1,\ldots,a_{100}\}$并且$minsup=1$，那么$TDB_1$只有两个闭模式$\{a_1,\ldots,a_{50}\}:2,\;\{a_1,\ldots,a_{100}\}:1$和唯一的最大模式$\{a_1,\ldots,a_{100}\}:1$。</p>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="Agrawal:1993:MAR:170036.170072">[1]R. Agrawal, T. Imieliński, and A. Swami, “Mining Association Rules Between Sets of Items in Large Databases,” <i>Sigmod Record</i>, vol. 22, no. 2, pp. 207–216, Jun. 1993.</span>

[<a href="http://doi.acm.org/10.1145/170036.170072">Online</a>]

</li>
<li><span id="han2007frequent">[2]J. Han, H. Cheng, D. Xin, and X. Yan, “Frequent pattern mining: current status and future directions,” <i>Data Mining and Knowledge Discovery</i>, vol. 15, no. 1, pp. 55–86, 2007.</span>

</li>
<li><span id="Pasquier1999discovering">[3]N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, “Discovering Frequent Closed Itemsets for Association Rules,” in <i>International Conference on Database Theory</i>, 1999, pp. 398–416.</span>

</li>
<li><span id="roberto667815">[4]R. J. Bayardo, “Efficiently Mining Long Patterns from Databases,” <i>Sigmod Record</i>, vol. 27, no. 2, pp. 85–93, 1998.</span>

</li></ol>

<h3 id="section-3">脚注</h3>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>R Essential</title>
      <link href="http://qianjiye.de/2015/03/r-essential" />
      <pubdate>2015-03-06T02:36:23+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/r-essential</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">基本操作</h2>

<h4 id="section-1">工作环境</h4>

<table>
  <thead>
    <tr>
      <th>命令</th>
      <th>功能</th>
      <th>例子</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>?"*"</code></td>
      <td>查看*的帮组文档</td>
      <td><code>?"data.frame"</code></td>
    </tr>
    <tr>
      <td><code>??"*"</code></td>
      <td>在帮助文档中搜索*</td>
      <td><code>??"network"</code></td>
    </tr>
    <tr>
      <td><code>ls()</code></td>
      <td>查看可用对象</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>rm()</code></td>
      <td>移除对象</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>getwd()</code></td>
      <td>查看工作目录</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>setwd(dir)</code></td>
      <td>设置工作目录</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><code class="language-R"><span class="c1">#删除当前环境中所有对象</span>
<span class="kp">rm</span><span class="p">(</span><span class="kt">list</span> <span class="o">=</span> <span class="kp">ls</span><span class="p">())</span></code></pre></div>

<h2 id="section-2">基本类型</h2>

<p>R不能直接存取内存，通过称之为<strong>对象</strong>（object）的特殊数据结构存取内存。这些对象通过符号（symbol）或变量（variable）访问，符号本身也是对象。R的<code>typeof</code>函数返回对象的类型<sup id="fnref:R-type"><a href="#fn:R-type" class="footnote">1</a></sup>。<a href="http://cran.r-project.org/doc/manuals/r-release/R-lang.html#Objects">R的对象类型</a>包括<code>NULL</code>、<code>closure</code>、<code>integer</code>、<code>complex</code>等。</p>

<p><code>mode</code>函数返回对象的模式（mode），主要是为了和S语言的其它实现兼容。<code>storage.mode</code>函数返回参数的存储模式（storage mode），通常用于调用C或FORTRAN时，确定数据类型。在S语言中，整数和实数向量都是<code>numeric</code>模式，但存储模式不同。</p>

<h3 id="section-3">基本类型</h3>

<p><strong>向量</strong>（vector）是最基本数据结构，R包含6种基本的原子向量（atomic vector）：</p>

<table>
  <thead>
    <tr>
      <th>typeof</th>
      <th>mode</th>
      <th>storage.mode</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>logical</td>
      <td>logical</td>
      <td>logical</td>
    </tr>
    <tr>
      <td>integer</td>
      <td>numeric</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>double</td>
      <td>numeric</td>
      <td>double</td>
    </tr>
    <tr>
      <td>complex</td>
      <td>complex</td>
      <td>complex</td>
    </tr>
    <tr>
      <td>character</td>
      <td>character</td>
      <td>character</td>
    </tr>
    <tr>
      <td>raw</td>
      <td>raw</td>
      <td>raw</td>
    </tr>
  </tbody>
</table>

<p>单个数和字符串也是长度为1的向量，例如4.2和“four point two”。原子向量中的每个元素类型必须相同。向量的长度可为0。</p>

<p><strong>列表</strong>（list）称为通用向量（generic vector），每个元素可以是不同数据类型。虽然列表也是向量，但通常说的向量是不包括列表的原子向量。</p>

<p>R语言由3种不同类型的对象构成，它们是calls、expressions和names，这些对象的模式分别为<code>call</code>、<code>expression</code>和<code>name</code>，可通过<code>as.list</code>和<code>as.call</code>在列表和<strong>语言对象</strong>之间转换。<strong>符号</strong>(symbol)指向R的对象，R对象的名字通常是一个符号。符号能够通过函数<code>as.name</code>和<code>quote</code>创建。符号的模式为<code>name</code>，存储模式和类型都为<code>symbol</code>，可通过<code>as.character</code>和<code>as.name</code>与字符串转换。</p>

<p>R的expression对象包含一个或多个语句（statement）。R的expression对象只有在显示调用<code>eval</code>时才被执行。expression对象与列表相似，访问元素的方法与列表一样。</p>

<p><strong>函数对象</strong>包含三个元素：参数列表、函数体和环境。参数列表用逗号分割，<code>…</code>可包含任意个数的参数。函数体通常是包含在大括号中的R语句。环境在函数创建时激活。这三部分可通过函数<code>formals</code>、<code>body</code>和<code>environment</code>操作。<code>as.list</code>和<code>as.function</code>函数可进行列表和函数的转换。</p>

<p>NULL是特殊的对象，与长度为0的向量不同，R中只有唯一的NULL对象。</p>

<h3 id="section-4">对象属性</h3>

<p>除NULL外的所有对象都可包含一个或多个属性。属性以成对列表（pairlist）方式存储，所有元素都有名字。通过<code>attributes</code>查看属性列表，通过<code>attr</code>获取单个属性。一些特殊的属性有特定的存取函数，例如因子的<code>levels</code>函数，数组的<code>dim</code>和<code>dimnames</code>函数。属性被用于实现R的类结构。If an object has a class attribute then that attribute will be examined during evaluation.</p>

<h2 id="section-5">数据结构</h2>

<p>R的数据结构包括数组（array）和列表（list）。数组的所有元素类型必须一致，列表元素的类型可以不同。向量（vector）是一维素组，矩阵（matrix）是二维数组。数据框（data frame）是特殊的列表。</p>

<p>数据的索引（indexing）有<code>[]</code>、<code>[[]]</code>和<code>$</code>三种方法，形如<code>x[i]</code>、<code>x[i, j]</code>、<code>x[[i]]</code>、<code>x[[i, j]]</code>、<code>x$a</code>和<code>x$”a”</code>。</p>

<ul>
  <li>对向量和矩阵，<code>[[]]</code>会抛弃所有<code>names</code>和<code>dimnames</code>属性，<code>[]</code>则不会；</li>
  <li>对列表，<code>[[]]</code>返回列表，<code>[]</code>返回列表的元素；</li>
  <li><code>[[]]</code>只能对单个元素索引，<code>[]</code>可以通过向量索引多个元素；</li>
  <li><code>$</code>只能用于列表索引，索引参数不能被计算（如果索引参数需要计算，采用<code>x[[expr]]</code>），索引的对象不存在时返回<code>NULL</code>。</li>
</ul>

<p>数据的索引从1开始，索引前加负号<code>-</code>表示删除这些索引的元素。</p>

<h3 id="section-6">数据分割与合并</h3>

<table>
  <thead>
    <tr>
      <th>命令</th>
      <th>功能</th>
      <th>备注</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>rbind</code></td>
      <td>行合并</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>cbind</code></td>
      <td>列合并</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>merge</code></td>
      <td>列合并</td>
      <td>用于data frame</td>
    </tr>
    <tr>
      <td><code>subset</code></td>
      <td>抽取子集</td>
      <td><code>subset(airquality, Temp &gt; 80 &amp; is.na(Solar.R))</code></td>
    </tr>
    <tr>
      <td><code>append</code></td>
      <td>插入元素</td>
      <td>用于向量</td>
    </tr>
    <tr>
      <td><code>stack</code></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><code>split</code></td>
      <td>数据分组</td>
      <td>用于向量</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><code class="language-R">airquality<span class="o">$</span>Temp2 <span class="o">&lt;-</span> airquality<span class="o">$</span>Temp <span class="c1">#增加Temp2列</span>
airquality<span class="o">$</span>Temp <span class="o">&lt;-</span> <span class="kc">NULL</span> <span class="c1">#删除Temp列</span>

Top5 <span class="o">=</span> <span class="kp">subset</span><span class="p">(</span>mvt<span class="p">,</span> LocationDescription <span class="o">%in%</span> TopLocations<span class="p">)</span>
<span class="c1"># 删除多余无关的levels（Top5在执行了subset之后会包含整个集合的levels）</span>
Top5<span class="o">$</span>LocationDescription <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>Top5<span class="o">$</span>LocationDescription<span class="p">)</span>

CPS <span class="o">=</span> <span class="kp">merge</span><span class="p">(</span>CPS<span class="p">,</span> MetroAreaMap<span class="p">,</span> by.x<span class="o">=</span><span class="s">&quot;MetroAreaCode&quot;</span><span class="p">,</span> by.y<span class="o">=</span><span class="s">&quot;Code&quot;</span><span class="p">,</span> all.x<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span></code></pre></div>

<h2 id="section-7">运算符和优先级</h2>

<table>
  <thead>
    <tr>
      <th>Operator</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>[</code> <code>[[</code></td>
      <td>Indexing</td>
    </tr>
    <tr>
      <td><code>::</code> <code>:::</code></td>
      <td>Access variables in a name space</td>
    </tr>
    <tr>
      <td><code>$</code> <code>@</code></td>
      <td>Component extraction, slot extraction</td>
    </tr>
    <tr>
      <td><code>^</code></td>
      <td>Exponentiation (right to left)</td>
    </tr>
    <tr>
      <td><code>-</code> <code>+</code></td>
      <td>Unary minus and plus</td>
    </tr>
    <tr>
      <td><code>:</code></td>
      <td>Sequence creation</td>
    </tr>
    <tr>
      <td><code>%any%</code></td>
      <td>Special operators</td>
    </tr>
    <tr>
      <td><code>*</code> <code>/</code></td>
      <td>Multiplication, division</td>
    </tr>
    <tr>
      <td><code>+</code> <code>-</code></td>
      <td>Addition, subtraction</td>
    </tr>
    <tr>
      <td><code>==</code> <code>!=</code> <code>&lt;</code> <code>&gt;</code> <code>&lt;=</code> <code>&gt;=</code></td>
      <td>Comparison</td>
    </tr>
    <tr>
      <td><code>!</code></td>
      <td>Logical negation</td>
    </tr>
    <tr>
      <td><code>&amp;</code> <code>&amp;&amp;</code></td>
      <td>Logical “and”, short-circuit “and”</td>
    </tr>
    <tr>
      <td><code>|</code> <code>||</code></td>
      <td>Logical “or”, short-circuit “or”</td>
    </tr>
    <tr>
      <td><code>~</code></td>
      <td>Formula</td>
    </tr>
    <tr>
      <td><code>-&gt;</code> <code>-&gt;&gt;</code></td>
      <td>Rightward assignment</td>
    </tr>
    <tr>
      <td><code>=</code></td>
      <td>Assignment (right to left)</td>
    </tr>
    <tr>
      <td><code>&lt;-</code> <code>&lt;&lt;-</code></td>
      <td>Assignment (right to left)</td>
    </tr>
    <tr>
      <td><code>?</code></td>
      <td>Help</td>
    </tr>
  </tbody>
</table>

<p><code>%any%</code>运算符：</p>

<ul>
  <li><code>%%</code>：求模，<code>7%%4=3</code>；</li>
  <li><code>%/%</code>：整数除，<code>7%/%4=1</code>；</li>
  <li><code>%*%</code>：矩阵乘法；</li>
  <li><code>%o%</code>：外积；</li>
  <li><code>%x%</code>：Kronecker乘积；</li>
  <li><code>%in%</code>：匹配运算。</li>
</ul>

<h2 id="section-8">常用函数</h2>

<h3 id="section-9">查看数据统计特性</h3>

<h4 id="str"><code>str</code>：查看对象结构</h4>

<p>显示对象各属性。</p>

<h4 id="summary"><code>summary</code>：查看对象统计特性</h4>

<p>显示最小值、分位数、最大值等数据，可以通过<code>boxplot</code>直观的显示。</p>

<h4 id="tablefactor"><code>table</code>：按factor类型统计</h4>

<div class="highlight"><pre><code class="language-R"><span class="c1">#将Ozone（行）和Month（列）作为factor类型，统计同时满足两个条件样本的个数，以表格的形式显示。</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span> <span class="kp">table</span><span class="p">(</span>Ozone<span class="p">,</span> Month<span class="p">))</span> 

<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> Month<span class="p">,</span> useNA <span class="o">=</span> <span class="s">&quot;ifany&quot;</span><span class="p">))</span> <span class="c1">#若有NA则显示</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> Month<span class="p">,</span> useNA <span class="o">=</span> <span class="s">&quot;always&quot;</span><span class="p">))</span><span class="c1">#总是显示NA</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> <span class="kp">addNA</span><span class="p">(</span>Month<span class="p">)))</span>           <span class="c1">#增加一列NA</span></code></pre></div>

<h4 id="hist"><code>hist</code>：直方图</h4>

<div class="highlight"><pre><code class="language-R">hist<span class="p">(</span><span class="kp">sqrt</span><span class="p">(</span>islands<span class="p">),</span> breaks <span class="o">=</span> <span class="m">12</span><span class="p">,</span> col <span class="o">=</span> <span class="s">&quot;lightblue&quot;</span><span class="p">,</span> border <span class="o">=</span> <span class="s">&quot;pink&quot;</span><span class="p">)</span></code></pre></div>

<h4 id="boxplotbox-and-whisker"><code>boxplot</code>：盒须图（box-and-whisker）</h4>

<div class="highlight"><pre><code class="language-R">boxplot<span class="p">(</span>count <span class="o">~</span> spray<span class="p">,</span> data <span class="o">=</span> InsectSprays<span class="p">)</span> <span class="c1"># 横轴为spray，纵轴为count</span></code></pre></div>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-05-r-essential-boxplot.png"><img src="/assets/images/2015-03-05-r-essential-boxplot.png" alt="boxplot图示" /></a><div class="caption">图 1:  boxplot图示 [<a href="/assets/images/2015-03-05-r-essential-boxplot.png">PNG</a>]</div></div></div>

<p><a href="http://msenux.redwoods.edu/math/R/boxplot.php"><code>boxplot</code>绘制如上图</a>：</p>

<ul>
  <li>box中的粗横线表示中位数（median）；</li>
  <li>box的下边和上边分别表示第一和第三四分位数（quartile），二者之间的距离称为四分位数间距（IQR，inter-quartile range），分位数用<code>quantile</code>函数计算，IQR用<code>IQR</code>函数计算；</li>
  <li>最下和最上的横线分别表示最小和最大值；</li>
  <li>小圆圈表示离群点（outlier）。</li>
</ul>

<p>令$STEP = range\times IQR$（$range$可通过<code>boxplot</code>设置），区间$[first\; quartile - STEP, third\; quartile + STEP]$之外的是离群点，大于$first\; quartile - STEP$的最小值为图中的最小值（minimum），小于$third\; quartile + STEP$的最大值为图中的最大值（maximum），当<code>boxplot</code>的参数<code>range=0</code>时，不探测离群点，显示实际的最大和最小值。</p>

<h3 id="section-10">批处理函数</h3>

<h4 id="tapply"><code>tapply</code>：应用函数操作分组数据</h4>

<p><img src="/assets/images/2015-03-05-r-essential-tapplay.png" alt="tapply" /></p>

<div class="highlight"><pre><code class="language-R"><span class="c1"># 将warpbreaks[,-1]强制转换为factor类型；</span>
<span class="c1"># 通过warpbreaks[,-1]对warpbreaks$breaks分类；</span>
<span class="c1"># 将每类的warpbreaks$breaks作为函数sum的输入；</span>
<span class="c1"># ……</span>
<span class="kp">tapply</span><span class="p">(</span>warpbreaks<span class="o">$</span>breaks<span class="p">,</span> warpbreaks<span class="p">[,</span><span class="m">-1</span><span class="p">],</span> <span class="kp">sum</span><span class="p">)</span></code></pre></div>

<h3 id="section-11">其它</h3>

<div class="highlight"><pre><code class="language-R">DateConvert <span class="o">=</span> <span class="kp">as.Date</span><span class="p">(</span><span class="kp">strptime</span><span class="p">(</span>mvt<span class="o">$</span>Date<span class="p">,</span> <span class="s">&quot;%m/%d/%y %H:%M&quot;</span><span class="p">))</span> <span class="c1">#日期转换</span>
mvt<span class="o">$</span>Month <span class="o">=</span> <span class="kp">months</span><span class="p">(</span>DateConvert<span class="p">)</span>
mvt<span class="o">$</span>Weekday <span class="o">=</span> <span class="kp">weekdays</span><span class="p">(</span>DateConvert<span class="p">)</span></code></pre></div>

<h2 id="section-12">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-13">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:R-type">
      <p>Note that in the C code underlying R, all objects are pointers to a structure with typedef <code>SEXPREC</code>; the different R data types are represented in C by <code>SEXPTYPE</code>, which determines how the information in the various parts of the structure is used. <a href="#fnref:R-type" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>图像分类（3）：最优化</title>
      <link href="http://qianjiye.de/2015/02/image-classification-optimization" />
      <pubdate>2015-02-13T22:53:50+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/image-classification-optimization</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">理解损失函数</h2>

<p>图像分类的两个关键步骤包括：（1）通过<strong>评分函数</strong>将图像映射到类别评分；（2）通过<strong>损失函数</strong>度量评分。多分类支持向量机采用的是线性评分函数$f\left(\mathbf x_i,\mathbf W\right)=\mathbf W\mathbf x_i$，需要最小化的损失函数为
\[
L={1\over N}\sum_i\sum_{j\neq y_i}
\max\left(
0, f\left(\mathbf x_i,\mathbf W\right)_j-f\left(\mathbf x_i,\mathbf W\right)_{y_i}+1
\right)
+\alpha R(\mathbf W)。
\]</p>

<p>图像分类的第三个关键步骤：（3）通过最优化求解最小化损失函数的参数$\mathbf W$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg"><img src="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg" alt="损失函数图" /></a><div class="caption">图 1:  损失函数图 [<a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg">JPG</a>]</div></div></div>

<p>将$\mathbf W$视为空间中的一个点，损失函数（不含正则化项）可以用上图表示。$\mathbf W$、$\mathbf W_1$和$\mathbf W_2$是随机产生的矩阵。上图左表示损失函数$L(\mathbf W+a\mathbf W_1)$，横轴为$a$，纵轴为$L$；上图中和右表示损失函数$L(\mathbf W+a\mathbf W_1+b\mathbf W_2)$，横轴和纵轴分别为$a$和$b$，越红损失$L$越大，越蓝损失$L$越小。上图左和中，只计算一张图片的损失；上图右的碗状图是100张图片损失的平均值，相当于100张上图中的平均图。</p>

<p>每张图$i$的损失为（不含正则化项）
\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+1
\right)，
\]
它是分段线性（piecewise-linear）函数。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png"><img src="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png" alt="1维损失函数" /></a><div class="caption">图 2:  1维损失函数 [<a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png">PNG</a>]</div></div></div>

<p>三张图片$\mathbf x_0$、$\mathbf x_1$和$\mathbf x_2$分别属于类0、1和2，它们的损失计算如下
\[
\begin{aligned}
L_0 &amp;= \max\left(0, \mathbf w_1^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)\\
L_1 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)\\
L_2 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)+
\max\left(0, \mathbf w_1^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)\\
L &amp;= {1\over 3}\left(L_0+L_1+L_2\right)，
\end{aligned}
\]
如上图所示，横轴表示权值，纵轴表示损失。</p>

<p>多分类SVM的代价函数是凸函数的一个范例，当采用神经网络的评分函数$f$时，代价函数就是非凸的。损失函数不可微分（non-differentiable），因此梯度未定义，通常使用次梯度（subgradient）代替梯度。本文不区分梯度和次梯度。</p>

<p>损失函数可以评估任意权值$\mathbf W$，最优化的目标是找出最小化损失函数的权值$\mathbf W$。神经网络的优化不能方便地使用凸函数的优化工具。</p>

<h2 id="section-1">随机方法</h2>

<h3 id="section-2">随机搜索</h3>

<p>比较糟糕的优化策略是随机搜索（random search）。由于验证参数$\mathbf W$比较简单，随机搜索通过尝试不同的随机权值$\mathbf W$，从中选择最优的。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span> <span class="c"># Python assigns the highest possible float value</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span> <span class="c"># generate random parameters</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the loss over the entire training set</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span> <span class="c"># keep track of the best solution</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="n">bestW</span> <span class="o">=</span> <span class="n">W</span>
  <span class="k">print</span> <span class="s">&#39;in attempt </span><span class="si">%d</span><span class="s"> the loss was </span><span class="si">%f</span><span class="s">, best </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，随机搜索可以做到约15.5%的精度，优于随机猜想的10%。找到最优的$\mathbf W$很困难甚至不可行，但是找到更好一些的$\mathbf W$就不难么困难了。基于这个思路，优化算法可以从随机的$\mathbf W$开始，不断更新权值，使得每次更新都提升一点性能。随机搜索如同徒步者带上眼罩向山脚走。针对CIFAR-10数据集，这山有30730维，山上的每一点都相当于特定的损失值。</p>

<h3 id="section-3">随机局部搜索</h3>

<p>从随机初始化的$\mathbf W$开始，若增加扰动$\delta\mathbf W$，损失在$\mathbf W+\delta\mathbf W$比在$\mathbf W$时更低，那么更新$\mathbf W\leftarrow\mathbf W+\delta\mathbf W$。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># generate random starting W</span>
<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.0001</span>
  <span class="n">Wtry</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">Xtr_cols</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Wtry</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">Wtry</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
  <span class="k">print</span> <span class="s">&#39;iter </span><span class="si">%d</span><span class="s"> loss is </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，精度提高到了21.4%。但这仍然是低效耗时的算法。</p>

<h2 id="section-4">梯度下降法</h2>

<p>随机搜索对寻找最佳的权值改变方向没有帮助。沿着最佳的方向改变权值，在数学上可以保证损失最速下降（steepest descend）。这个方向和梯度（gradient）相关。</p>

<p>一维函数的斜率（slope）是函数值在某点的瞬时（instantaneous）变化率。梯度是斜率在多维空间函数的推广，它是每维斜率构成的向量，通常也称为导数（derivative）。一维函数的导数为
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x)\over h}，
\]
将其推广到多变量函数时称为偏导数（partial derivative）。梯度就是每一维偏导数组成的向量，有两种计算方法：</p>

<ul>
  <li>数值梯度（numerical gradient）：近似计算，较慢，但容易；</li>
  <li>解析梯度（analytic gradient）：计算快，但是容易出错（error-prone）。</li>
</ul>

<h3 id="section-5">数值梯度</h3>

<p>梯度的数值计算方法如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">  a naive implementation of numerical gradient of f at x </span>
<span class="sd">  - f should be a function that takes a single argument</span>
<span class="sd">  - x is the point (numpy array) to evaluate the gradient at</span>
<span class="sd">  &quot;&quot;&quot;</span> 

  <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evaluate function value at original point</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>

  <span class="c"># iterate over all indexes in x</span>
  <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;readwrite&#39;</span><span class="p">])</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

    <span class="c"># evaluate function at x+h</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c"># increment by h</span>
    <span class="n">fxh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evalute f(x + h)</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="c"># restore to previous value (very important!)</span>

    <span class="c"># compute the partial derivative</span>
    <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span> <span class="c"># the slope</span>
    <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c"># step to next dimension</span>

  <span class="k">return</span> <span class="n">grad</span></code></pre></div>
<p>上述代码计算损失函数在$\mathbf x$每个维度的偏导数。在实际应用中通常使用中心差分公式（centered difference formula）
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x-h)\over 2h}。
\]</p>

<p>梯度只表明了最快增长的方向，还需要在这个方向前进的步长（也就是学习率）。步长是神经网络需要设定的重要超参数。</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># to use the generic code above we want a function that takes a single argument</span>
<span class="c"># (the weights in our case) so we close over X_train and Y_train</span>
<span class="k">def</span> <span class="nf">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># random weight vector</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">CIFAR10_loss_fun</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the gradient</span>

<span class="n">loss_original</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c"># the original loss</span>
<span class="k">print</span> <span class="s">&#39;original loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_original</span><span class="p">,</span> <span class="p">)</span>

<span class="c"># lets see the effect of multiple step sizes</span>
<span class="k">for</span> <span class="n">step_size_log</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">step_size_log</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">df</span> <span class="c"># new position in the weight space</span>
  <span class="n">loss_new</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>
  <span class="k">print</span> <span class="s">&#39;for step size </span><span class="si">%f</span><span class="s"> new loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">loss_new</span><span class="p">)</span>

<span class="c"># prints:</span>
<span class="c"># original loss: 2.200718</span>
<span class="c"># for step size 1.000000e-10 new loss: 2.200652</span>
<span class="c"># for step size 1.000000e-09 new loss: 2.200057</span>
<span class="c"># for step size 1.000000e-08 new loss: 2.194116</span>
<span class="c"># for step size 1.000000e-07 new loss: 2.135493</span>
<span class="c"># for step size 1.000000e-06 new loss: 1.647802</span>
<span class="c"># for step size 1.000000e-05 new loss: 2.844355</span>
<span class="c"># for step size 1.000000e-04 new loss: 25.558142</span>
<span class="c"># for step size 1.000000e-03 new loss: 254.086573</span>
<span class="c"># for step size 1.000000e-02 new loss: 2539.370888</span>
<span class="c"># for step size 1.000000e-01 new loss: 25392.214036</span></code></pre></div>
<p>上述代码中，<code>step_size</code>表示步长（学习率），步长小损失函数减小慢，但步长太大损失函数不降反增。当损失函数有30730个参数时，每次更新参数需要计算30731次损失函数，计算复杂度非常高。</p>

<h3 id="section-6">解析梯度</h3>

<p>数值梯度虽简单但耗时，解析梯度计算高效但易错。在实际应用中，采用解析梯度时，通过比较它与数值梯度确定梯度计算是否正确，这称为<strong>梯度校验</strong>（gradient check）。</p>

<p>对每个数据，多分类SVM的损失函数为</p>

<p>\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta
\right)，
\]</p>

<p>对$\mathbf w_{y_i}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{y_i}}L_i=
-\left(\sum_{j\neq y_i}\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\right)\mathbf x_i，
\]</p>

<p>对$\mathbf w_{j}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{j}}L_i=
\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\mathbf x_i。
\]</p>

<p>利用梯度更新参数的方法称为<strong>梯度下降法</strong>（gradient descent），通常的形式：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>梯度下降法是到目前为止最常用的优化神经网络损失函数的方法。</p>

<p>对大数据集上的应用，在整个数据集上计算损失函数的梯度非常耗时，通常从中抽取从中抽取一部分数据计算梯度，这称为mini-batch梯度下降法，例如采用256个样本计算梯度：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Minibatch Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">data_batch</span> <span class="o">=</span> <span class="n">sample_training_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c"># sample 256 examples</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>在实际中，采用mini-batch的方法能提高参数更新频率，更快收敛。若mini-batch梯度下降法只采用一个样本，称为随机梯度下降法（SGD，stochastic gradient descent）或在线（on-line）梯度下降法。但这不太常用，在实际中由于向量化代码的优化，计算100个样本的梯度比1个样本的梯度计算100次高效。虽然人们使用SGD这个称谓，实际通常指的是mini-batch的梯度下降法（MGD／BGD，minbatch/batch gradient descent）。min-batch的样本数量虽是超参数，但很少采用验证法确定，通常根据内存大小来决定，或者直接设定为100左右的值。</p>

<h2 id="bp">BP梯度</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit2.png" alt="BP梯度计算" /></a><div class="caption">图 3:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png">PNG</a>]</div></div></div>

<p>对函数$f(x,y,z)=(x+y)z$，令$q=x+y$，那么根据链式法则（chain rule）有${\partial f\over \partial x}={\partial f\over \partial q}{\partial q\over \partial x}$。梯度${\partial f\over \partial x}$从右到左反向计算如上图“电路”所示，通过相邻节点的局部梯度链式相乘得到，每个节点用门（gate）表示。当${\partial f\over \partial q}=-4$和${\partial q\over \partial x}=1$时，${\partial f\over \partial x}=-4\times 1 = -4$。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit3.png" alt="BP梯度计算" /></a><div class="caption">图 4:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png">PNG</a>]</div></div></div>

<p>当
$
f(\mathbf w,\mathbf x)={1\over 1 + e^{-\left(w_0x_0+w_1x_1+w_2\right)}}
$
时，链式计算如上如所示。链式法则的依据是复合函数的求导法则。门可以是任何可导函数（differentiable function），通常选择导数容易计算的函数作为门，如sigmoid函数$\sigma(x)={1\over 1 + e^{-x}}$。将${df\over dx}$简记为<code>dx</code>，${d\sigma(x)\over dx}=(1-\sigma(x))\sigma(x)$，上图的计算过程简化如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="c"># assume some random weights and data</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>

<span class="c"># forward pass</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">))</span> <span class="c"># sigmoid function</span>

<span class="c"># backward pass through the neuron (backpropagation)</span>
<span class="n">ddot</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">f</span> <span class="c"># gradient on dot variable, using the sigmoid gradient derivation</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into x</span>
<span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into w</span>
<span class="c"># we&#39;re done! we have the gradients on the inputs to the circuit</span></code></pre></div>

<p>对于复杂的函数，一次求导很复杂，如果采用链式法则，可以降低计算复杂度。对函数
\[
f(x,y)={x+\sigma(y)\over\sigma(x)+(x+y)^2}，
\]
前向计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># example values</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="c"># forward pass</span>
<span class="n">sigy</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c"># sigmoid in numerator   #(1)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sigy</span> <span class="c"># numerator                               #(2)</span>
<span class="n">sigx</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c"># sigmoid in denominator #(3)</span>
<span class="n">xpy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                                              <span class="c">#(4)</span>
<span class="n">xpysqr</span> <span class="o">=</span> <span class="n">xpy</span><span class="o">**</span><span class="mi">2</span>                                          <span class="c">#(5)</span>
<span class="n">den</span> <span class="o">=</span> <span class="n">sigx</span> <span class="o">+</span> <span class="n">xpysqr</span> <span class="c"># denominator                        #(6)</span>
<span class="n">invden</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">den</span>                                       <span class="c">#(7)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">num</span> <span class="o">*</span> <span class="n">invden</span> <span class="c"># done!                                 #(8)</span></code></pre></div>

<p>反向梯度计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># backprop f = num * invden</span>
<span class="n">dnum</span> <span class="o">=</span> <span class="n">invden</span> <span class="c"># gradient on numerator                             #(8)</span>
<span class="n">dinvden</span> <span class="o">=</span> <span class="n">num</span>                                                     <span class="c">#(8)</span>
<span class="c"># backprop invden = 1.0 / den </span>
<span class="n">dden</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">den</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dinvden</span>                                <span class="c">#(7)</span>
<span class="c"># backprop den = sigx + xpysqr</span>
<span class="n">dsigx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                                <span class="c">#(6)</span>
<span class="n">dxpysqr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                              <span class="c">#(6)</span>
<span class="c"># backprop xpysqr = xpy**2</span>
<span class="n">dxpy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">xpy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpysqr</span>                                        <span class="c">#(5)</span>
<span class="c"># backprop xpy = x + y</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="c"># backprop sigx = 1.0 / (1 + math.exp(-x))</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigx</span> <span class="c"># Notice += !! See notes below  #(3)</span>
<span class="c"># backprop num = x + sigy</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                  <span class="c">#(2)</span>
<span class="n">dsigy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                <span class="c">#(2)</span>
<span class="c"># backprop sigy = 1.0 / (1 + math.exp(-y))</span>
<span class="n">dy</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigy</span>                                 <span class="c">#(1)</span>
<span class="c"># done! phew</span></code></pre></div>

<h4 id="section-7">注意事项：</h4>

<ol>
  <li>将前向计算结果缓存起来，供后向计算使用，如<code>xpy</code>；</li>
  <li>当变量的导数分成几部分计算时，结果要加起来，采用<code>+=</code>。</li>
</ol>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit4.png" alt="BP梯度计算" /></a><div class="caption">图 5:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png">PNG</a>]</div></div></div>

<p>神经网络中常使用的三种门是$+,\times,\max$，计算法则如上图。</p>

<p>对于线性分类器$\mathbf w^\top\mathbf x_i$，采用乘法门，输入数据的大小会影响权值梯度。输入数据对梯度影响很大，因此数据预处理能极大影响梯度。如果输入数据增大1000倍，权值的梯度也会增大1000倍，此时可以降低学习率补偿数据的影响。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>特征学习模型总结</title>
      <link href="http://qianjiye.de/2015/02/summary-of-extraction-models" />
      <pubdate>2015-02-13T17:48:53+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/summary-of-extraction-models</guid>
      <content:encoded>&lt;![CDATA[<p>特征学习／提取模型（extraction model）：除了得到最终的线性模型外，将特征变换$\Phi$作为隐含的学习变量。</p>

<p>特征学习模型是拥有众多成员的大家族：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>隐藏变量</th>
      <th>线性模型</th>
      <th>extraction技术</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>神经网络／深度学习</td>
      <td>$w_{ij}^{(\ell)}$</td>
      <td>$w_{ij}^{(L)}$</td>
      <td>梯度下降法＋BP<br />自编码器（非监督学习）</td>
    </tr>
    <tr>
      <td>RBF网络</td>
      <td>中心$\boldsymbol\mu_m$</td>
      <td>$\beta_m$</td>
      <td>k均值聚类（非监督学习）</td>
    </tr>
    <tr>
      <td>矩阵分解<sup id="fnref:v-equals-w"><a href="#fn:v-equals-w" class="footnote">1</a></sup></td>
      <td>用户特征$\mathbf v_n$</td>
      <td>电影特征$\mathbf w_m$</td>
      <td>梯度下降法<br />交替最小二乘法</td>
    </tr>
    <tr>
      <td>Ada/Gradient Boosting</td>
      <td>假设$g_t$</td>
      <td>投票权重$\alpha_t$</td>
      <td>函数梯度下降法</td>
    </tr>
    <tr>
      <td>k最近邻算法</td>
      <td>邻居$\mathbf x_n$</td>
      <td>$y_n$</td>
      <td>lazy learning</td>
    </tr>
  </tbody>
</table>

<p>特征学习模型的优劣：</p>

<table>
  <thead>
    <tr>
      <th>优势</th>
      <th>坏处</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>容易：减轻了人工提取特征的负担</td>
      <td>困难：通常是非凸优化</td>
    </tr>
    <tr>
      <td>强大：如果有足够多的隐含变量</td>
      <td>过拟合：需要正则化或验证</td>
    </tr>
  </tbody>
</table>

<p>因此，使用特征学习模型要当心！</p>

<h2 id="section">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-1">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:v-equals-w">
      <p>$\mathbf v_n$和$\mathbf w_m$实际上是对称的（等价的）。 <a href="#fnref:v-equals-w" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>矩阵分解</title>
      <link href="http://qianjiye.de/2015/02/matrix-factorization" />
      <pubdate>2015-02-12T18:33:54+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/matrix-factorization</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">线性网络</h2>

<p>对于推荐系统（recommender system）数据集，用户对第$m$部电影的评分表示为
\[
\mathcal D_m=
\left\{\left(
\tilde x_n=(n), y_n=r_{nm}
\right)
\mbox{ user }n\mbox{ rated movie }m
\right\}，
\]
$\tilde x_n=(n)$表示抽象的（abstract）特征，代表用户编号（ID），不代表任何数值上的意义，这类的特征称为<strong>类别特征</strong>（categorical feature）。</p>

<p>常见的类别特征有ID、血型、程序语言……然而，许多机器学习算法都对数值特征友好，比如线性模型、扩展的（extended）线性模型（神经网络等）……决策树可以处理类别特征。若要将类别特征用于对数值特征友好的模型，需要将类别特征转换或编码（transform/encode）为数值特征。二值编码（binary vector encoding）是比较简单的征编码方式，对于血型有
\[
A=[1\;0\;0\;0]^\top,\quad
B=[0\;1\;0\;0]^\top,\quad
AB=[0\;0\;1\;0]^\top,\quad
O=[0\;0\;0\;1]^\top。
\]
编码之后第$m$部电影的的数据记为
\[
\mathcal D_m=
\left\{\left(
\tilde x_n=\mbox{BinaryVectorEncoding}(n), y_n=r_{nm}
\right)
\mbox{ user }n\mbox{ rated movie }m
\right\}，
\]
所有用户的数据可以统一记为
\[
\mathcal D=
\left\{\left(
\tilde x_n=\mbox{BinaryVectorEncoding}(n), 
y_n=\left[r_{n1}\;?\;?\;r_{n4}\;r_{n5}\;\ldots\;r_{nM}\right]^\top
\right)
\right\}，
\]
“$?$”表示用户没有对电影做出评价。若要学到每个用户的爱好，需要先学到每个用户的特征（年龄、电影类型的偏好等）。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-12-matrix-factorization-NN-for-features.png"><img src="/assets/images/2015-02-12-matrix-factorization-NN-for-features.png" alt="利用神经网络提取特征" /></a><div class="caption">图 1:  利用神经网络提取特征 [<a href="/assets/images/2015-02-12-matrix-factorization-NN-for-features.png">PNG</a>]</div></div></div>

<p>利用不含$x_0^{(\ell)}$神经元<sup id="fnref:why-no-1-neuron"><a href="#fn:why-no-1-neuron" class="footnote">1</a></sup>的$N-\tilde d-M$神经网络（自编码器）提取特征，如上图所示，$\mathbf x$是只有一个非0元的二值向量。中间的$\tanh$转换必要么？</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-12-matrix-factorization-linear-neuron-for-features.png"><img src="/assets/images/2015-02-12-matrix-factorization-linear-neuron-for-features.png" alt="采用线性神经元提取特征" /></a><div class="caption">图 2:  采用线性神经元提取特征 [<a href="/assets/images/2015-02-12-matrix-factorization-linear-neuron-for-features.png">PNG</a>]</div></div></div>

<p>对于$\mathbf x$，只会有一个非0分量进入$\tanh$层的神经元，即使不进行$\tanh$转换（采用线性模型，如上图所示）也能找到特征的恰当描述方式，这样的神经网络称为<strong>线性网络</strong>（linear network）。第一层用$N\times\tilde d$的矩阵$\mathbf V^\top$表示，第二层用$\tilde d\times M$的矩阵$\mathbf W$表示，线性网络可以表示为
\begin{equation}
h(\mathbf x)=\mathbf W^\top\mathbf V\mathbf x。
\end{equation}
对每个用户而言，由于$\mathbf x_n$只有一个元素为1，相当于抽取矩阵的一列
\begin{equation*}
h(\mathbf x_n)=\mathbf W^\top\mathbf v_n，
\end{equation*}
$\mathbf v_n$表示$\mathbf V$的第$n$列，相当于对第$n$个用户进行了特征转换。</p>

<p>对于推荐系统，线性网络学习$\mathbf V$和$\mathbf W$。</p>

<h2 id="section-1">矩阵分解</h2>

<p>令$\Phi(\mathbf x)=\mathbf V\mathbf x$，第$m$部电影的评分只是线性模型$h_m(\mathbf x)=\mathbf w_m^\top\Phi(\mathbf x)$。对于$\mathcal D_m$，期望有
\[
r_{nm}=y_n\approx\mathbf w_m^\top\mathbf v_n，
\]
需要最小化目标函数
\begin{equation}
E_{in}\left(\left\{\mathbf w_m\right\},\left\{\mathbf v_n\right\}\right)
={1\over \sum_{m=1}^M\lvert\mathcal D_m\rvert}\sum_{\mbox{user }n\mbox{ rated movie }m}
\left(r_{nm}-\mathbf w_m^\top\mathbf v_n\right)^2，
\end{equation}
同时学到转换方式$\left\{\mathbf v_n\right\}$和线性模型$\left\{\mathbf w_m\right\}$。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-12-matrix-factorization-matrix-factorization.png"><img src="/assets/images/2015-02-12-matrix-factorization-matrix-factorization.png" alt="矩阵分解的形式" /></a><div class="caption">图 3:  矩阵分解的形式 [<a href="/assets/images/2015-02-12-matrix-factorization-matrix-factorization.png">PNG</a>]</div></div></div>

<p>当$r_{nm}=\mathbf w_m^\top\mathbf v_n=\mathbf v_n^\top\mathbf w_m$时，所有评价可记为矩阵形式$\mathbf R\approx\mathbf V^\top\mathbf W$，将含有缺失值的$\mathbf R$矩阵分解为两个矩阵的乘积，如上图所示，就得到了用户的特征以及特征的线性组合方式。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-12-matrix-factorization-recommender.png"><img src="/assets/images/2015-02-12-matrix-factorization-recommender.png" alt="推荐系统" /></a><div class="caption">图 4:  推荐系统 [<a href="/assets/images/2015-02-12-matrix-factorization-recommender.png">PNG</a>]</div></div></div>

<p>通过用户的评分$\mathbf R$，学到了$\mathbf v_n$表示用户的特征（喜好），也学到了$\mathbf w_m$表电影具备哪些元素，如上图所示。当有新用户$N+1$时，$\mathbf v_{N+1}$初始化为$\mathbf v_{N+1}={1\over N}\sum_{n=1}^N\mathbf v_n$，新用户$N+1$的评分$r_{(N+1)m}$最高的电影是有最高平均评分的电影。</p>

<p>类似的矩阵分解模型可以用于提取其它抽象特征。线性自编码器可看作一种特殊的矩阵分解方法：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th><a href="/2015/02/deep-learning/#linear-autoencoder">线性自编码器</a> $\mathbf X\approx\mathbf W\left(\mathbf W^\top\mathbf X\right)$</th>
      <th>矩阵分解 $\mathbf R\approx \mathbf V^\top\mathbf W$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>motivation</td>
      <td>特殊的$d-\tilde d-d$的线性神经网络</td>
      <td>$N-\tilde d-M$的线性神经网络</td>
    </tr>
    <tr>
      <td>误差度量</td>
      <td>所有数据$x_{ni}$上的平方误差</td>
      <td>已知数据$r_{nm}$上的平方误差</td>
    </tr>
    <tr>
      <td>求解方法</td>
      <td>$\mathbf X^\top\mathbf X$特征向量上的全局最优解</td>
      <td>交替最小二乘法求取的局部最优解</td>
    </tr>
    <tr>
      <td>功能</td>
      <td>获取数据低维特征表示</td>
      <td>提取隐含特征</td>
    </tr>
  </tbody>
</table>

<h2 id="section-2">交替最小二乘法</h2>

<p>通过最优化
\[
\min_{\mathbf W,\mathbf V}E_{in}\left(\left\{\mathbf w_m\right\},\left\{\mathbf v_n\right\}\right)
\propto\sum_{m=1}^M\left(
\sum_{(\mathbf x_n,r_{nm})\in\mathcal D_m}
\left(r_{nm}-\mathbf w_m^\top\mathbf v_n\right)^2
\right)，
\]
学到参数矩阵$\mathbf W,\mathbf V$。同时学习两组变量很困难，可以采取类似<a href="/2015/02/radial-basis-function-network/#k-means">k均值算法</a>的<strong>交替最小二乘法</strong>（alternating least squares algorithm）：</p>

<ul>
  <li>当固定$\mathbf v_n$时，相当于在第$m$部电影的数据$\mathcal D_m$上最小化$E_{in}$得到$\mathbf w_m$，通过对每部电影的线性回归（不含$w_0$）实现；</li>
  <li>由于用户特征向量和电影特征向量的对称性（二者地位一样），当固定$\mathbf w_m$时，相当于对每个用户的线性回归（不含$v_0$）。</li>
</ul>

<blockquote>
  <h4 id="section-3">交替最小二乘法</h4>
  <hr />
  <p>随机初始化$\tilde d$维向量$\{\mathbf w_m\}，\{\mathbf v_n\}$；</p>

  <p>交替最小化$E_{in}$直到收敛：</p>

  <ul>
    <li>最优化$\mathbf w_1,\mathbf w_2,\ldots,\mathbf w_M$：对电影$m$在数据集$\{(\mathbf v_n,r_{nm})\}$做线性回归；</li>
    <li>最优化$\mathbf v_1,\mathbf v_2,\ldots,\mathbf v_N$：对用户$n$在数据集$\{(\mathbf w_m,r_{nm})\}$做线性回归。</li>
  </ul>
</blockquote>

<p>每次交替最小化都会使$E_{in}$减小，收敛是有保障的。采用交替最小二乘法，类似于用户和电影之间的“探戈”（tango）。</p>

<h2 id="section-4">随机梯度下降法</h2>

<p>随机梯度下降法（SGD，stochastic gradient descent）容易实现，每轮迭代高效，容易扩展到平方误差之外的其它误差度量方式。</p>

<p>单个数据产生的误差为
\[
err(\mbox{user }n, \mbox{movie }m, \mbox{rating }r_{nm})
=\left(r_{nm}-\mathbf w_m^\top\mathbf v_n\right)^2，
\]
那么可得偏微分
\[
\begin{aligned}
\nabla_{\mathbf v_n} &amp;err(\mbox{user }n, \mbox{movie }m, \mbox{rating }r_{nm})
=-2\left(r_{nm}-\mathbf w_m^\top\mathbf v_n\right)\mathbf w_m\\
\nabla_{\mathbf w_m} &amp;err(\mbox{user }n, \mbox{movie }m, \mbox{rating }r_{nm})
=-2\left(r_{nm}-\mathbf w_m^\top\mathbf v_n\right)\mathbf v_n。
\end{aligned}
\]
每次跟新量$\propto -(\mbox{residual})(\mbox{the other feature vector})$，余数（residual）为$r_{nm}-\mathbf w_m^T\mathbf v_n$，也就是用余数倍另一个量更新当前量。</p>

<blockquote>
  <h4 id="section-5">基于随机梯度下降法的矩阵分解</h4>
  <hr />
  <p>随机初始化$\tilde d$维向量$\{\mathbf w_m\}，\{\mathbf v_n\}$<sup id="fnref:all-zeros"><a href="#fn:all-zeros" class="footnote">2</a></sup>；</p>

  <p>对$t=0,1,\ldots,T$迭代：</p>

  <ol>
    <li>对所有已知的$r_{nm}$，随机抽取$(n,m)$；</li>
    <li>计算余数$\tilde r_{nm}=r_{nm}-\mathbf w_m^T\mathbf v_n$；</li>
    <li>随机梯度更新：
\[
\begin{aligned}
\mathbf v_n^{new}&amp;\leftarrow\mathbf v_n^{old}+\eta\tilde r_{nm}\mathbf w_m^{old}\\
\mathbf w_m^{new}&amp;\leftarrow\mathbf w_m^{old}+\eta\tilde r_{nm}\mathbf v_n^{old}。
\end{aligned}
\]</li>
  </ol>
</blockquote>

<p>随机梯度下降法在大规模矩阵分解中很常用。</p>

<blockquote>
  <h4 id="kddcup-2011-track-1-world-champion-solution-by-ntu">KDDCup 2011 Track 1: World Champion Solution by NTU</h4>
  <hr />
  <p><strong>问题</strong>：训练数据时间上要早于测试数据。也就是训练和测试数据分布不同，有偏采样（sampling bias）。</p>

  <p><strong>对策</strong>：强化时间晚的数据。SGD在最后$T’$轮迭代只选用感兴趣的$T’$个数据。</p>

  <p><strong>结果</strong>：consistent improvements of test performance。</p>
</blockquote>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:why-no-1-neuron">
      <p>让网络看上去简单些而已，加入$x_0^{(\ell)}$可以得到一个稍微有些不同的模型。 <a href="#fnref:why-no-1-neuron" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:all-zeros">
      <p>若都初始化为0，$\{\mathbf w_m\}$和$\{\mathbf v_n\}$始终为0，$E_{in}$永远也不会减少。 <a href="#fnref:all-zeros" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>径向基函数网络</title>
      <link href="http://qianjiye.de/2015/02/radial-basis-function-network" />
      <pubdate>2015-02-11T23:59:57+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/radial-basis-function-network</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">径向基函数网络</h2>

<p><a href="/2015/01/kernel-svm/#mjx-eqn-eqrbf-svm">高斯核SVM</a>，可以实现无限维特征空间的最大边界分类器，它的实现方式是利用$\alpha_n$实现以支持向量$\mathbf x_n$为中心高斯函数的线性组合。</p>

<p>高斯核通常也称为径向基函数（RBF，radial basis function）。radial表示函数值只依赖于$\mathbf x$与“中心”$\mathbf x_n$之间的距离；basis function表示用来组合的系数$\alpha_n，y_n$。</p>

<p>令$g_n(\mathbf x)=y_n\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_n\right\rVert^2\right)$，它表示基于$\mathbf x$与$\mathbf x_n$之间距离权重的加权投票（$+1$或$-1$的票）。高斯核SVM可以表示为
\[
g_{SVM}(\mathbf x)=\mbox{sign}\left(\sum_{SV}\alpha_ng_n(\mathbf x)+b\right)，
\]
它是径向基假设的线性融合。</p>

<p><strong>径向基函数网络</strong>就是径向基假设的线性融合，它也是一种神经网络。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-11-radial-basis-function-network-NN-vs-RBFN.png"><img src="/assets/images/2015-02-11-radial-basis-function-network-NN-vs-RBFN.png" alt="神经网络与径向基函数网络" /></a><div class="caption">图 1:  神经网络与径向基函数网络 [<a href="/assets/images/2015-02-11-radial-basis-function-network-NN-vs-RBFN.png">PNG</a>]</div></div></div>

<p>神经网络与径向基函数网络如上图所示：</p>

<ul>
  <li>隐藏层：神经网络采用“内积＋$\tanh$”，径向基函数网络采用“距离＋高斯函数”；</li>
  <li>输出层都是同样的线性融合模型。</li>
</ul>

<p>径向基函数网络的假设为
\begin{equation}
h(\mathbf x)=\mbox{Output}\left(
\sum_{m=1}^{M}\beta_m\mbox{RBF}(\mathbf x,\boldsymbol\mu_m)+b
\right)，
\end{equation}
高斯函数只是一种径向基函数$\mbox{RBF}$，$\beta_m$是投票权值，$\mbox{Output}$形式由不同的回归或分类问题确定。$\boldsymbol\mu_m$和$\beta_m$（带符号）是两个关键参数。高斯核SVM是一种径向基函数网络：径向基函数$\mbox{RBF}$采用高斯函数；对二分类问题输出是$\pm 1$；$M=\#SV$；$\boldsymbol\mu_m$是支持向量$\mathbf x_m$；$\beta_m$是从对偶SVM推导的$\alpha_my_m$。</p>

<p>给定径向基函数$\mbox{RBF}$和输出，径向基函数网络需要学习的参数是$\mu_m$和$\beta_m$。</p>

<p>核和RBF是两种不同的相似度度量方法。核描述基于$\mathcal Z$空间内积的相似性，需要满足<a href="/2015/01/kernel-svm">Mercer条件</a>。RBF通过$\mathcal X$空间的距离度量相似性，这种相似性与距离是单调递减（monotonically non-increasing）关系。一般采用度量$\mathbf x$和$\mathbf x’$相似性的函数还可以是<sup id="fnref:edit-distance"><a href="#fn:edit-distance" class="footnote">1</a></sup>
\[
\mbox{Neuron}(\mathbf x,\mathbf x’)=\tanh\left(\gamma\mathbf x^\top\mathbf x’+1\right)，\quad\mbox{DNASim}(\mathbf x,\mathbf x’)=\mbox{EditDistance}(\mathbf x,\mathbf x’)。
\]
神经网络的神经元也是在比较输入向量与权值向量之间的相似性。</p>

<p>RBF网络展示了通过度量到中心的距离也是一种很好的特征转换。 相似性是一种很好的特征转换方法，相似性不一定与距离有关，也不一定与核有关。</p>

<h2 id="section-1">学习算法</h2>

<p>当$M=N$和$\boldsymbol\mu_m=\mathbf x_m$时，称为完全RBF网络。它的物理含义是每个点$\mathbf x_m$对周围的数据$\mathbf x$有权重$\beta_m$的影响，假设对二分类问题均匀影响（uniform influence）周围的数据，$\beta_m=1\cdot y_m$，那么
\begin{equation}
g_{uniform}(\mathbf x)=\mbox{sign}\left(\sum_{m=1}^Ny_m\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_m\right\rVert^2\right)\right)，
\end{equation}
这就是每个数据通过相似度对新数据的投票融合。这和<a href="/2015/01/image-classification-knn-based-introduction/#kNN">k最近邻算法</a>思想很相似。离$\mathbf x$最近的$\mathbf x_m$让$\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_m\right\rVert^2\right)$取得最大值。高斯函数衰减很快，最大的投票很可能会左右投票结果，不需要让所有数据投票，只用离$\mathbf x$最近数据投票，用选择法（selection）替代融合法（aggregation），
\begin{equation}
g_{nbor}=y_m\mbox{ such that }\mathbf x\mbox{ closest to }\mathbf x_m， 
\end{equation}
这就是最近邻算法。若让k个$\mathbf x_m$参与对$\mathbf x$投票就是k最近邻算法。</p>

<p>如果不直接用$y_m$作为$\beta_m$投票，而更进一步考虑对$\beta_m$进行优化，用完全RBF网络解决基于平方误差的回归问题，
\begin{equation}
h(\mathbf x)=\sum_{m=1}^{M}\beta_m\mbox{RBF}(\mathbf x,\mathbf x_m)。
\end{equation}
这是基于RBF特征转换的线性回归，
\[
\mathbf z_n=[\mbox{RBF}(\mathbf x_n,\mathbf x_1),\mbox{RBF}(\mathbf x_n,\mathbf x_2),\ldots,\mbox{RBF}(\mathbf x_n,\mathbf x_N)]，
\]
最优解为$\boldsymbol\beta=\left(\mathbf Z^\top\mathbf Z\right)^{-1}\mathbf Z^\top\mathbf y$（若$\mathbf Z^\top\mathbf Z$可逆），$\mathbf Z$是$N\times N$的对称方阵，那么就有
\begin{equation}
\boldsymbol\beta=\mathbf Z^{-1}\mathbf y。
\end{equation}
若每个$\mathbf x_n$都不一样，那么$\mathbf Z$总可逆。对于数据$\mathbf x_1$，
\[
g_{RBF}(\mathbf x_1)
=\boldsymbol\beta^\top\mathbf z_1
=\mathbf y^\top\mathbf Z^{-1}\cdot(\mbox{first column of }\mathbf Z)
=\mathbf y^\top\left[1,0,\ldots,0\right]^\top
=y_1，
\]
那么就有
\begin{equation}
g_{RBF}(\mathbf x_n)=y_n，
\end{equation}
对于回归问题$E_{in}(g_{RBF})=0$。在函数逼近（function approximation）领域这叫完美内插（exact interpolation）；对机器学习而言这是过拟合，需要正则化。</p>

<h2 id="section-2">正则化</h2>

<p>对正则化的完全RBF网络的脊回归可得
\begin{equation}
\boldsymbol\beta=\left(\mathbf Z^\top\mathbf Z+\lambda\mathbf I\right)^{-1}\mathbf Z^\top\mathbf y，
\end{equation}
其中$\mathbf Z$可以看作<a href="/2015/01/kernel-svm">高斯核矩阵</a>，$\mathbf K=\mathbf Z$，对比<a href="/2015/01/support-vector-regression/#mjx-eqn-eqanalytic-solution-ridge-regression">脊回归的核模型</a>，二则只相差矩阵$\mathbf Z^\top$，前者是有限$N$维转换的回归，后者是无限维转换的回归。</p>

<p>对于<a href="/2015/01/kernel-svm/#mjx-eqn-eqrbf-svm">SVM核模型</a>，只采用了支持向量作为参考进行距离度量。减少中心数量（也就减少了参与投票的权值），只使用部分代表点（prototype）作为中心，使$M\ll N$，也是一种有效的正则化方法。</p>

<p>如何找出这些有效的代表点呢？</p>

<h2 id="k-means">k均值聚类</h2>

<p>若$\mathbf x_1\approx\mathbf x_2$，不需要$\mbox{RBF}(\mathbf x,\mathbf x_1)$和$\mbox{RBF}(\mathbf x,\mathbf x_2)$都出现在RBF网络中，可以只用一个cluster$\boldsymbol\mu\approx\mathbf x_1\approx\mathbf x_2$替代两个点即可。</p>

<p>聚类是将数据$\{\mathbf x_n\}$分为不相交的集合（类）$S_1,S_2,\ldots,S_M$。当每个集合用$\boldsymbol\mu_m$作为代表时，若$\mathbf x_1$和$\mathbf x_2$都属于$S_m$，当且仅当$\boldsymbol\mu_m\approx\mathbf x_1\approx\mathbf x_2$。聚类的目标函数为
\begin{equation}
\min_{\{S_1,\ldots,S_M;\boldsymbol\mu_1,\ldots,\boldsymbol\mu_M\}}E_{in}(S_1,\ldots,S_M;\boldsymbol\mu_1,\ldots,\boldsymbol\mu_M)
={1\over N}\sum_{n=1}^{N}\sum_{m=1}^M[[\mathbf x_n\in S_m]]\lVert\mathbf x_n-\boldsymbol\mu_m\rVert^2。
\end{equation}
这是混合的组合数值优化问题，很难最优化，但是可以简化为分别交替最优化$S_m$和$\boldsymbol\mu_m$：</p>

<ol>
  <li>最优划分：固定$\boldsymbol\mu_1,\ldots,\boldsymbol\mu_M$；选择每个$\mathbf x_n$所属的唯一类别$S_m$，使$\lVert\mathbf x_n-\boldsymbol\mu_m\rVert$最小。</li>
  <li>最优计算：固定$S_1,\ldots,S_M$；对每个$\boldsymbol\mu_m$，由于
\[
\nabla_{\boldsymbol\mu_m}E_{in}
=-2\sum_{n=1}^N[[\mathbf x_n\in S_m]](\mathbf x_n-\boldsymbol\mu_m)
=-2\left(\left(\sum_{\mathbf x_n\in S_m}\mathbf x_n\right)-\left|S_m\right|\boldsymbol\mu_m\right)=0，
\]
可得最佳$\boldsymbol\mu_m$是属于$S_m$所有$\mathbf x_n$的均值。</li>
</ol>

<p>以上就是<a href="/2014/12/k-means">k均值算法</a>的关键步骤，$k=M$，重复以上2步直至收敛。通常随机选择k个$\mathbf x_n$作为$\boldsymbol\mu_k$的初始值。收敛是指$S_1,\ldots,S_k$不再改变。算法通过交替最小化使$E_{in}$减小，一定能收敛。</p>

<blockquote>
  <h4 id="krbf">基于k均值的RBF网络</h4>
  <hr />

  <ol>
    <li>利用k均值算法，$k=M$，得到$\{\boldsymbol\mu_m\}$；</li>
    <li>进行特征转换
\[
\Phi(\mathbf x)=\left[\mbox{RBF}(\mathbf x,\boldsymbol\mu_1),\mbox{RBF}(\mathbf x,\boldsymbol\mu_2),\ldots,\mbox{RBF}(\mathbf x,\boldsymbol\mu_M)\right]；
\]</li>
    <li>通过$\{(\Phi(\mathbf x_n),y_n)\}$上的线性模型得到参数$\boldsymbol\beta$；</li>
    <li>返回$g_{RBFNET}(\mathbf x)=\mbox{LinearHypothesis}(\boldsymbol\beta, \Phi(\mathbf x))$。</li>
  </ol>
</blockquote>

<p>上述算法采用非监督的k均值作为特征转换，如同自编码器。RBF网络需要确定的的参数是$M$和$\mbox{RBF}$函数，这些可通过验证方法确定。</p>

<p>虽然RBF网络和SVM的核模型与神经网络性能差不多，但在实际中并不常用。</p>

<h2 id="section-3">实战技能</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-11-radial-basis-function-network-k-means.png"><img src="/assets/images/2015-02-11-radial-basis-function-network-k-means.png" alt="k均值算法对参数敏感" /></a><div class="caption">图 2:  k均值算法对参数敏感 [<a href="/assets/images/2015-02-11-radial-basis-function-network-k-means.png">PNG</a>]</div></div></div>

<p>只要选择合适的$k$和恰当的初始化，k均值算法通常表现良好。k均值算法对参数敏感，选择不同的$k$和不同的初始化，结果差异大，如上图所示。对参数交替最优化不能保证得到全局最优。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-11-radial-basis-function-network-k-means-and-RBFN.jpg"><img src="/assets/images/2015-02-11-radial-basis-function-network-k-means-and-RBFN.jpg" alt="基于k均值的RBF网络" /></a><div class="caption">图 3:  基于k均值的RBF网络 [<a href="/assets/images/2015-02-11-radial-basis-function-network-k-means-and-RBFN.jpg">JPG</a>]</div></div></div>

<p>上图是利用了k均值的RBF网络，蓝色和红色表示两类的标签，阴影渐变区域表示k均值聚类的结果，黑色线条展示分类结果。上图左完全没办法分出两类，输出都是同一类的结果。如果k均值的效果较好，RBF网络效果也更好。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-11-radial-basis-function-network-full-RBFN.jpg"><img src="/assets/images/2015-02-11-radial-basis-function-network-full-RBFN.jpg" alt="完全RBF网络" /></a><div class="caption">图 4:  完全RBF网络 [<a href="/assets/images/2015-02-11-radial-basis-function-network-full-RBFN.jpg">JPG</a>]</div></div></div>

<p>上图左和右分别展示了两种完全RBF网络的效果，所有的数据都参与分类计算，由于计算量很大，在实际中很少使用。或者说这类算法还需要借助其它方式，克服计算复杂度。</p>

<h2 id="section-4">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-5">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:edit-distance">
      <p>$\mbox{EditDistance}$表示一个字符串如何通过最小的修改变为另一个字符串。 <a href="#fnref:edit-distance" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>深度学习</title>
      <link href="http://qianjiye.de/2015/02/deep-learning" />
      <pubdate>2015-02-04T19:31:41+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/deep-learning</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">深度神经网络</h2>

<p>确定神经网络的结构是非常核心也是非常困难的问题。</p>

<p>需要多少神经元？网络多少层？神经元之间如何连接？……这些选择一方面凭主观意愿，另一方面通过验证的方法确定。</p>

<p>浅层（shallow）神经网络只有少量的隐层，深度（deep）神经网络有很多层，它们之间的对比如下：</p>

<table>
  <thead>
    <tr>
      <th>浅层神经网络</th>
      <th>深度神经网络</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>✅训练较高效</td>
      <td>❌训练极具挑战性</td>
    </tr>
    <tr>
      <td>✅结构简单</td>
      <td>❌结构复杂</td>
    </tr>
    <tr>
      <td>✅非常强大</td>
      <td>✅非常强大</td>
    </tr>
    <tr>
      <td> </td>
      <td>✅能提取有意义的特征</td>
    </tr>
  </tbody>
</table>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png"><img src="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png" alt="深度学习提取有意义的特征" /></a><div class="caption">图 1:  深度学习提取有意义的特征 [<a href="/assets/images/2015-02-04-deep-learning-meaningfulness-of-DL.png">PNG</a>]</div></div></div>

<p>若只有一层，它需要完成的任务就很复杂。层数增多，每层或层与层之间功能简单，如上图所示，只需从输入中提取简单的笔画，若还有下一层，继续从简单笔画中找出更复杂的笔画。层与层之间，实现从简单到复杂的特征转换。若原始特征是raw feature（每个特征物理意义有限，比如图像的每个像素），最终需要完成复杂的分类动作，深度学习处理这样的问题更自然。</p>

<h4 id="section-1">一、深度学习的挑战与关键技术</h4>

<ul>
  <li>决定网络结构很困难。
    <ul>
      <li>验证能帮忙但并非易事；</li>
      <li>借助领域知识（domain knowledge），比如用于图像的convolutional NNet，像素物理位置有意义，相邻像素连接到下层同一神经元可描述更高阶的特征，相距太远的像素不连接在一起。</li>
    </ul>
  </li>
  <li>模型复杂度很高。
    <ul>
      <li>数据足够多时模型复杂度不是问题，但计算复杂度会更高；</li>
      <li>利用正则化（在某些神经元坏掉时，dropout能使网络很好工作；输入出问题时，denoising能使网络很好工作）。</li>
    </ul>
  </li>
  <li>最优化很困难。局部极小值比神经网络更容易出现。
    <ul>
      <li>精心初始化，避免陷入糟糕的局部极小值（利用pre-training）。</li>
    </ul>
  </li>
  <li>计算复杂度很高，尤其是面对大数据的时候。
    <ul>
      <li>利用新的硬件软件架构，比如mini-batch+GPU。</li>
    </ul>
  </li>
</ul>

<p>其中，正则化和初始化是尤为关键的两项技术。</p>

<blockquote>
  <h4 id="section-2">二、深度学习两步走策略</h4>
  <hr />

  <ol>
    <li>预训练：对$\ell=1,\dots,L$，假设$\mathbf w_*^{(1)},\ldots\mathbf w_*^{(\ell-1)}$已知，逐层训练权值$\left\{w_{ij}^{(\ell)}\right\}$。</li>
    <li>训练：利用BP算法在预训练的基础上调整（fine-tune）权值$\left\{w_{ij}^{(\ell)}\right\}$。</li>
  </ol>
</blockquote>

<h2 id="section-3">非线性自编码器：神经网络</h2>

<p>权值的作用是进行特征转换，将数据换成另一种表现形式，也就是编码（encoding）。在预训练深度神经网络时，并不清楚当前层的权重对以后层有何影响。因此，好的权值需要能保持信息（information-preserving），不同层的权值以不同的形式表示信息。保持信息的意思是数据经过编码之后能重建或精确解码（decode accurately）出原来的信息。预训练权值就是保持信息的编码。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png"><img src="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png" alt="自编码器（保持信息的神经网络）" /></a><div class="caption">图 2:  自编码器（保持信息的神经网络） [<a href="/assets/images/2015-02-04-deep-learning-information-preserving-NN.png">PNG</a>]</div></div></div>

<p>上图的$d-\tilde d-d$结构的神经网络力求输出与输入一致，$g(\mathbf x) = \mathbf x$，这种保持信息的神经网络称为<strong>自编码器</strong>（autoencoder），它的目的是做函数到它本身的逼近（approximate identity function）。$w_{ij}^{(1)}$称为编码权值，$w_{ji}^{(2)}$称为解码权值。</p>

<p>自编码器的变换$g(\mathbf x) = \mathbf x$采用了数据集上的隐含结构（hidden structure），其价值在于：</p>

<ul>
  <li>对于监督学习：$\mathbf x$的隐含结构（权值）很好的保持了信息，可用作特征变换$\Phi(\mathbf x)$。——informative representation of data</li>
  <li>对于非监督学习：（1）密度估计（density estimation）：稠密的区域$g(\mathbf x)\approx\mathbf x$效果好<sup id="fnref:why-density-estimate"><a href="#fn:why-density-estimate" class="footnote">1</a></sup>，新的数据若表现好则位于稠密区域；（2）异常检测（outlier detection）：异常点的$g(\mathbf x)\ne\mathbf x$。——typical representation of data</li>
</ul>

<p>自编码器通过学习identity function得到了数据的表现形式。</p>

<p>基本的自编码器就是$d-\tilde d-d$结构的神经网络，误差函数为
$
\sum_{i=1}^d(g_i(\mathbf x)-x_i)^2。
$
这是浅层网络，可以容易利用BP算法训练。通常情况$\tilde d&lt;d$，这是数据的压缩表示。数据集是$\{(\mathbf x_1, \mathbf y_1=\mathbf x_1),(\mathbf x_2,\mathbf y_2=\mathbf x_2),\ldots,(\mathbf x_N, \mathbf y_N=\mathbf x_N)\}$，因此这也被视为非监督学习。有时会用约束条件$w_{ij}^{(1)}=w_{ji}^{(2)}$让网络更简单，也就是进行正则化，这也会让算法更复杂。</p>

<p>在深度学习中，采用基本的自编码器在数据集$\left\{\mathbf x_n^{(\ell-1)}\right\}$上训练（其中$\tilde d=d^{(\ell)}$），将自编码器权值$\left\{w_{ij}^{(1)}\right\}$作为深度神经网络预训练的权值$\left\{w_{ij}^{(\ell)}\right\}$。</p>

<p>许多成功的预训练技术，利用不同的网络结构或正则化机制等方法，实现了更精妙的自编码器。</p>

<h2 id="section-4">去噪自编码器：正则化</h2>

<p>复杂的神经网络模型复杂度高，需要利用正则化避免过拟合。通常采用的正则化技术包括：</p>

<ul>
  <li>选择简单的网络结构；</li>
  <li><a href="/2015/02/neural-network/#regularization-weighted-methods">weight-decay或weight-elimination正则化</a>；</li>
  <li><a href="/2015/02/neural-network/#regularization-early-stopping">尽早停止迭代</a>。</li>
</ul>

<p>深度学习和自编码器采用了不同的正则化方法。</p>

<p>噪声是导致过拟合的重要原因，<a href="/2015/01/hazard-of-overfitting/#overfitting-illustration">数据越少噪声越多时，越容易过拟合</a>。当模型和数据确定时，去噪是避免过拟合的正则化方法。最直接的去噪技术可以采用<a href="/2015/01/hazard-of-overfitting/#data-cleaning">数据清洗或者数据剪枝</a>。能否反其道而行之，加入噪声？✅</p>

<p>对于鲁棒的自编码器，不仅能对原始数据$\mathbf x$有$g(\mathbf x)\approx \mathbf x$，而且对噪声数据$\tilde{\mathbf x}$有$g(\tilde{\mathbf x})\approx \mathbf x$，这就是<strong>去噪自编码器</strong>（denoising autoencoder）的任务。在数据集$\{(\tilde{\mathbf x}_1, \mathbf y_1=\mathbf x_1),(\tilde{\mathbf x}_2,\mathbf y_2=\mathbf x_2),\ldots,(\tilde{\mathbf x}_N, \mathbf y_N=\mathbf x_N)\}$训练自编码器，其中$\tilde{\mathbf x}_n$是对${\mathbf x}_n$加入了人工噪声的数据。在图像处理领域，$g(\tilde{\mathbf x})$可以得到$\tilde{\mathbf x}$的去噪版本。</p>

<p>在有噪声的数据集上训练出正则化的$g$具备抗噪的能力，这种正则化方法在神经网络模型中非常实用<sup id="fnref:data-hinting"><a href="#fn:data-hinting" class="footnote">2</a></sup>。</p>

<h2 id="linear-autoencoder">线性自编码器／主成分分析</h2>

<p>神经网络是复杂的非线性自编码器，能否利用简单高效且不易过拟合的线性自编码器呢？✅</p>

<p>对于第$k$个元素的线性模型
\[
h_k(\mathbf x)=\sum_{j=0}^{\tilde d}w_{jk}^{(2)}\left(
\sum_{i=0}^{d}w_{ij}^{(1)}x_i
\right)，
\]
加入限制条件：</p>

<ul>
  <li>去除掉常数项$x_0$，使$i$和$k$取值范围相同；</li>
  <li>加入正则化约束$w_{ij}^{(1)}=w_{ji}^{(2)}=w_{ij}$，$\mathbf W=[w_{ij}]$是$d\times\tilde d$的权值矩阵；</li>
  <li>$\tilde d&lt;d$；</li>
</ul>

<p>可得
\[
h_k(\mathbf x)=\sum_{j=0}^{\tilde d}w_{kj}\left(
\sum_{i=1}^{d}w_{ij}x_i
\right)。
\]</p>

<p>因此，线性自编码器的假设（hypothesis）可以表示为
\begin{equation}
h(\mathbf x)=\mathbf W\mathbf W^\top\mathbf x。
\end{equation}
好的自编码器就是对$\mathbf W$做最优化
\[
\mathbf W=\arg\min_{\mathbf W} E_{in}(\mathbf h)=E_{in}(\mathbf W)=
{1\over N}\sum_{n=1}^N\left\lVert\mathbf x_n-\mathbf W\mathbf W^\top\mathbf x_n\right\rVert^2，
\]
这是$w_{ij}$的4次多项式，不易得到解析解。</p>

<p>$\mathbf W\mathbf W^\top$是半正定矩阵，进行特征值分解$\mathbf W\mathbf W^\top=\mathbf V\boldsymbol\Gamma\mathbf V^\top$：</p>

<ul>
  <li>$d\times d$的矩阵$\mathbf V$是正交的（orthogonal）$\mathbf V\mathbf V^\top=\mathbf V^\top\mathbf V=\mathbf I_d$；</li>
  <li>$d\times d$的对角（diagonal）矩阵$\boldsymbol\Gamma$非零对角线元素最多$\tilde d$个。</li>
</ul>

<p>由此可得$\mathbf W\mathbf W^\top\mathbf x_n=\mathbf V\boldsymbol\Gamma\mathbf V^\top\mathbf x_n$：</p>

<ul>
  <li>$\mathbf V^\top\mathbf x_n$表示对$\mathbf x_n$的坐标转换，几何上看是一种旋转或镜射（rotate or reflect），$\mathbf x_n$的长度不会改变；</li>
  <li>$\boldsymbol\Gamma(\cdots)$中$\boldsymbol\Gamma$对角线有$d-\tilde d$个0元素，其余的非0元素进行尺度缩放；</li>
  <li>$\mathbf V(\cdots)$表示还原到原始坐标系。</li>
</ul>

<p>由于$\mathbf x_n=\mathbf V\mathbf I\mathbf V^\top\mathbf x_n$，最优化问题可变为
\[
\min_{\mathbf V}\min_{\boldsymbol\Gamma}{1\over N}\sum_{n=1}^N\left\lVert\mathbf V\mathbf I\mathbf V^\top\mathbf x_n-\mathbf V\boldsymbol\Gamma\mathbf V^\top\mathbf x_n\right\rVert^2，
\]
可先对$\boldsymbol\Gamma$最优化再对$\mathbf V$最优化，并且$\mathbf V$不影响向量在变换过程中的长度，可省去$\mathbf V$。最优化的形式可记为$\min_{\boldsymbol\Gamma}\sum\lVert(\mathbf I-\boldsymbol\Gamma)(\cdots)\rVert^2$，这需要$\mathbf I-\boldsymbol\Gamma$的0元素越多越好，也就是$\boldsymbol\Gamma$的对角线1越多越好。由于$\boldsymbol\Gamma$的秩不超过$\tilde d$，$\boldsymbol\Gamma$的对角线最多$\tilde d$个1，$\boldsymbol\Gamma$的最佳形式为
\[
\boldsymbol\Gamma=
\left[
\begin{aligned}
\mathbf I_{\tilde d}&amp;\quad 0\\
0&amp;\quad 0
\end{aligned}
\right]。
\]
接下来对$\mathbf V$优化
\[
\min_{\mathbf V}
\sum_{n=1}^N
\left\lVert
\left[
\begin{aligned}
0&amp;\quad 0\\
0&amp;\quad \mathbf I_{d-\tilde d}
\end{aligned}
\right]
\mathbf V^\top\mathbf x_n
\right\rVert^2，
\]
上式的意思是保留$\mathbf V^\top\mathbf x_n$的$d-\tilde d$个维度实现最小化，这可以通过最大化实现
\[
\max_{\mathbf V}
\sum_{n=1}^N
\left\lVert
\left[
\begin{aligned}
\mathbf I_{\tilde d}&amp;\quad 0\\
0&amp;\quad 0
\end{aligned}
\right]
\mathbf V^\top\mathbf x_n
\right\rVert^2。
\]
当$\tilde d=1$时，只有$\mathbf V^\top$的第一行$\mathbf v^\top$参与优化，也就是
\[
\max_{\mathbf v}\sum_{n=1}^N\mathbf v^\top\mathbf x_n\mathbf x_n^\top\mathbf v\quad\mbox{ s.t. }\mathbf v^\top\mathbf v=1，
\]
最佳的$\mathbf v$满足$\sum_{n=1}^N\mathbf x_n\mathbf x_n^\top\mathbf v=\lambda\mathbf v$（$\lambda$是拉格朗日乘子）<sup id="fnref:get-optimal-v"><a href="#fn:get-optimal-v" class="footnote">3</a></sup>，此时最佳目标值为$\lambda$，这个最佳的$\mathbf v$是$\mathbf X^\top\mathbf X$的“最大”（topmost）特征向量。一般地，$\{\mathbf v_j\}_{j=1}^{\tilde d}$是$\mathbf X^\top\mathbf X$“最大”的$\tilde d$个特征向量。</p>

<p>最佳的$\{\mathbf w_j\}$就是$\mathbf X^\top\mathbf X$“最大”的特征向量。线性自编码器就是投影到最符合数据$\{\mathbf x_n\}$的正交模式（orthogonal pattern）$\mathbf w_j$。</p>

<p>线性编码器的目标是最大化$\sum(\mbox{maginitude after projection})^2$；主成分分析（PCA，principal component analysis）的目标是最大化$\sum(\mbox{variance after projection})$，变化量$\mbox{variance}$是相对于平均数差距的平方。线性编码器和主成分分析都是线性降维方法，主成分分析＝数据0均值化 ＋ 线性自编码器，在降维中应用更普遍。</p>

<blockquote>
  <h4 id="section-5">线性编码器／主成分分析</h4>
  <hr />

  <ol>
    <li>令$\bar{\mathbf x}={1\over N}\sum_{n=1}^N\mathbf x_n$，$\mathbf x_n\leftarrow\mathbf x_n-\bar{\mathbf x}$<sup id="fnref:zero-mean-vector"><a href="#fn:zero-mean-vector" class="footnote">4</a></sup>；</li>
    <li>计算$\mathbf X^\top\mathbf X$的最大$\tilde d$个特征向量$\mathbf w_1,\mathbf w_2,\ldots,\mathbf w_{\tilde d}$；</li>
    <li>返回转换后的特征$\Phi(\mathbf x)=\mathbf W(\mathbf x-\bar{\mathbf x})$。</li>
  </ol>
</blockquote>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:why-density-estimate">
      <p>为啥稠密区域效果好？ <a href="#fnref:why-density-estimate" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:data-hinting">
      <p>可视为<a href="/2015/02/neural-network/#data-hinting">data hinting</a>的正则化技术。 <a href="#fnref:data-hinting" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:get-optimal-v">
      <p>利用拉格朗日乘子法求导可得。 <a href="#fnref:get-optimal-v" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:zero-mean-vector">
      <p>在0均值数据集基础上进行线性编码，自然实现了投影方向上变化最大。 <a href="#fnref:zero-mean-vector" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>神经网络</title>
      <link href="http://qianjiye.de/2015/02/neural-network" />
      <pubdate>2015-02-04T16:15:04+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/neural-network</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">感知器融合</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png" alt="感知器的线性融合" /></a><div class="caption">图 1:  感知器的线性融合 [<a href="/assets/images/2015-02-04-neural-network-aggregation-of-perceptrons.png">PNG</a>]</div></div></div>

<p>上图展示了感知器的线性融合结构，其中包含两层的权重$\mathbf w_t$和$\boldsymbol\alpha$，还包含两层的符号函数（sign function）$g_t$和$G$。这样的融合可以表示什么样的分类边界呢？</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-and-xor-operator.png"><img src="/assets/images/2015-02-04-neural-network-and-xor-operator.png" alt="AND和XOR运算" /></a><div class="caption">图 2:  AND和XOR运算 [<a href="/assets/images/2015-02-04-neural-network-and-xor-operator.png">PNG</a>]</div></div></div>

<p>单层感知器可以实现如上图所示的AND运算（+1表示TRUE，-1表示FALSE）
\[
G(\mathbf x)=\mbox{sign}(-1+g_1(\mathbf x)+g_2(\mathbf x))，
\]
也可实现OR和NOT运算。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-enough-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-enough-perceptrons.png" alt="足够多感知器的融合" /></a><div class="caption">图 3:  足够多感知器的融合 [<a href="/assets/images/2015-02-04-neural-network-enough-perceptrons.png">PNG</a>]</div></div></div>

<p>即使感知器的线性组合，分类能力也很强大，如上图所示。只要对足够多的感知器融合，可以在空间中切割出任意凸集（convex set），但$d_{VC}\rightarrow\infty$。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png"><img src="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png" alt="两层感知器的线性融合实现XOR运算" /></a><div class="caption">图 4:  两层感知器的线性融合实现XOR运算 [<a href="/assets/images/2015-02-04-neural-network-xor-multi-layer-perceptrons.png">PNG</a>]</div></div></div>

<p>但是，单层感知器不能实现XOR运算，经过XOR运算的特征转换$\phi(\mathbf x)=(g_1(\mathbf x),g_2(\mathbf x))$后数据线性不可分。线性不可分数据继续特征转换，最终将XOR用AND和OR实现
\[
XOR(g_1,g_2)=OR(AND(-g_1,g_2),AND(g_1,-g_2))，
\]
并且可以用上图所示的两层级连结构表示<sup id="fnref:five-input-xor"><a href="#fn:five-input-xor" class="footnote">1</a></sup>。</p>

<p>由此可见，虽然感知器算法简单，经过线性融合，以及多层级连，可以得到功能强大的分类器。多层感知器就是基本的神经网络结构。</p>

<h2 id="section-1">神经网络</h2>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-simple-NN.png"><img src="/assets/images/2015-02-04-neural-network-simple-NN.png" alt="简单的神经网络模型" /></a><div class="caption">图 5:  简单的神经网络模型 [<a href="/assets/images/2015-02-04-neural-network-simple-NN.png">PNG</a>]</div></div></div>

<p>事实上，对最后一层OUTPUT神经元，除了用感知器外，可以采用其它的<a href="/2015/01/linear-models-for-classification/#linear-models">线性模型</a>。若要分类，OUTPUT采用线性分类模型；若要回归分析，OUTPUT采用线性回归（不做任何处理）；若要soft分类，OUTPUT采用logistic回归。</p>

<p>中间层神经元采用的转换函数（transformation function），除了使用阶梯（符号）函数，也可采用其它转换函数。若所有神经元都采用线性回归，整个网络都是线性运算，用一个线性模型就可以实现。因此，很少用线性回归作为转换函数。阶梯函数是离散的，难以通过最优化求解$\mathbf w$，也很少使用。通常使用的是S形的转换函数
\[
\tanh(s)={\exp(s)-\exp(-s)\over\exp(s)+\exp(-s)}=2\theta(2s)-1，
\]
可以通过<a href="/2015/01/logistic-regression/#mjx-eqn-eqsigmoid-function">logistic函数</a>得到。该函数是阶梯函数的近似，且容易优化，传说和生物神经元也相近。</p>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-common-NN.png"><img src="/assets/images/2015-02-04-neural-network-common-NN.png" alt="常用的神经网络模型" /></a><div class="caption">图 6:  常用的神经网络模型 [<a href="/assets/images/2015-02-04-neural-network-common-NN.png">PNG</a>]</div></div></div>

<p>常用的神经网络采用$\tanh$作为转换函数，输出采用线性回归，如上图所示。$d^{(0)},d^{(1)},\ldots,d^{(L)}$表示每层神经元数目（节点数目），每层的权值为
\[
w_{ij}^{(\ell)}:\left\{
\begin{aligned}
&amp;1\leq\ell\leq L&amp;\mbox{layers}&amp;\\
&amp;0\leq i\leq d^{(\ell-1)}&amp;\mbox{inputs}&amp;\\
&amp;1\leq j\leq d^{(\ell)}&amp;\mbox{outputs}&amp;，
\end{aligned}
\right.
\]
常数+1的神经元相当于偏移项，评分函数为
\begin{equation}
s_j^{(\ell)}=\sum_{i=0}^{d^{(\ell-1)}}w_{ij}^{(\ell)}x_i^{(\ell-1)}，
\end{equation}
转换后的特征为
\begin{equation}
x_j^{(\ell)}=\left\{
\begin{aligned}
&amp;\tanh\left(s_j^{(\ell)}\right)&amp;\mbox{if }\ell&lt;L\\
&amp;s_j^{(\ell)}&amp;\mbox{if }\ell=L
\end{aligned}
\right.
\label{eq:forward-x}
\end{equation}</p>

<p>神经网络将$\mathbf x$当作输入层$\mathbf x^{(0)}$，隐层计算变换后的特征$\mathbf x^{(\ell)}$，输出层计算预测结果$x_1^{(L)}$。</p>

<p>神经网络隐层相当于模式（特征）提取（pattern extraction），进行$\mathbf x^{(\ell)}$和权值向量的模式匹配（利用基于内积的余弦相似度），每个神经元提取一种特征，权值向量纪录了从数据中学到的模式。</p>

<h2 id="bp">BP算法</h2>

<p>如何通过最小化$E_{in}\left(\left\{w_{ij}^{(\ell)}\right\}\right)$学到权值$\left\{w_{ij}^{(\ell)}\right\}$？</p>

<p>如果只有一个隐层，神经网络相当于感知器的融合，可以通过GradientBoost方法一个接一个的确定隐层的神经元。如果有多个隐层，问题就变得复杂了。</p>

<p>每个数据的误差记为
\[
e_n=(y_n-\mbox{NNet}(\mathbf x_n))^2，
\]
若能计算$\partial e_n\over \partial w_{ij}^{(\ell)}$，就可以通过梯度下降法求解。对输出层
\[
e_n=\left(y_n-s_1^{(L)}\right)^2=\left(y_n-\sum_{i=0}^{d^{(L-1)}}w_{i1}^{(L)}x_i^{(L-1)}\right)^2，
\]
对输出层和其它层分别求偏微分
\[
\begin{aligned}
&amp;{\partial e_n\over\partial w_{i1}^{(L)}}
={\partial e_n\over\partial s_{1}^{(L)}}\cdot{\partial s_{1}^{(L)}\over\partial w_{i1}^{(L)}}
=-2\left(y_n-s_1^{(L)}\right)\cdot x_i^{(L-1)}，\\
&amp;{\partial e_n\over\partial w_{ij}^{(\ell)}}
={\partial e_n\over\partial s_{j}^{(\ell)}}\cdot{\partial s_{j}^{(\ell)}\over\partial w_{ij}^{(\ell)}}
=x_i^{(\ell-1)}\delta_j^{(\ell)}。
\end{aligned}
\]
对输出层，令<sup id="fnref:tanh_output"><a href="#fn:tanh_output" class="footnote">2</a></sup>
\begin{equation}
\delta_1^{(L)}=-2\left(y_n-s_1^{(L)}\right)。
\label{eq:backpropagation-delta-L}
\end{equation}
对其它层
\begin{equation}
\delta_j^{(\ell)}
={\partial e_n\over\partial s_{j}^{(\ell)}}
=\sum_{k=1}^{d^{(\ell+1)}}{\partial e_n\over\partial s_{k}^{(\ell+1)}}{\partial s_{k}^{(\ell+1)}\over\partial x_{j}^{(\ell)}}{\partial x_{j}^{(\ell)}\over\partial s_{j}^{(\ell)}}
=\tanh’\left(s_j^{(\ell)}\right)\sum_{k=1}^{d^{(\ell+1)}}\delta_k^{(\ell+1)}w_{jk}^{(\ell+1)}，
\label{eq:backpropagation-delta2}
\end{equation}
也就是前一层的$\delta_j^{(\ell)}$，可以通过后一层$\delta_k^{(\ell+1)}$回推计算，<a href="http://www.wolframalpha.com/input/?i=d%2Fdx+tanh">其中</a>
\[
\tanh’(s)=1-\tanh^2(s)=\left({2\over \exp(s) + \exp(-s)}\right)^2，
\]
那么
\begin{equation}
\delta_j^{(\ell)}
=\left(1-\left(x_j^{(\ell)}\right)^2\right)\sum_{k=1}^{d^{(\ell+1)}}\delta_k^{(\ell+1)}w_{jk}^{(\ell+1)},\quad(j\geq 1)。
\label{eq:backpropagation-delta}
\end{equation}</p>

<blockquote>
  <h4 id="bpbackpropagation">BP（backpropagation）算法</h4>
  <hr />
  <p>用小的随机值初始化所有的$w_{ij}^{(\ell)}$；</p>

  <p>对于$t=1,2,\ldots,T$，循环执行： </p>

  <ol>
    <li>随机化：随机选取$n\in\{1,2,\ldots,N\}$；</li>
    <li>前向传播：从$\mathbf x^{(0)}=\mathbf x_n$开始，利用\eqref{eq:forward-x}计算所有$x_i^{(\ell)}$；</li>
    <li>误差回传：对$\mathbf x^{(0)}=\mathbf x_n$，利用\eqref{eq:backpropagation-delta-L}和\eqref{eq:backpropagation-delta}计算所有$\delta_j^{(\ell)}$；</li>
    <li>梯度下降：$w_{ij}^{(\ell)}\leftarrow w_{ij}^{(\ell)}-\eta x_i^{(\ell-1)}\delta_j^{(\ell)}$；</li>
  </ol>

  <p>返回$g_{\mbox{NNET}}(\mathbf x)=\left(\ldots\tanh\left(\sum_jw_{jk}^{(2)}\cdot\tanh\left(\sum_iw_{ij}^{(1)}x_i\right)\right)\right)$。</p>

  <p>在实际应用中，第1步至第3步可以先执行多次后，再用$x_i^{(\ell-1)}\delta_j^{(\ell)}$的平均值执行第4步的更新，这就是mini-batch的方法。</p>
</blockquote>

<p>神经网络通过最小化
\[
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^Nerr\left(\left(\ldots\tanh\left(\sum_jw_{jk}^{(2)}\cdot\tanh\left(\sum_iw_{ij}^{(1)}x_i\right)\right)\right),y_n\right)
\]
计算权值。通常多隐层神经网络的误差函数是非凸的（non-convex），难以达到全局最小值（global minimum），梯度下降法通过BP算法也仅仅得到局部极小值（local minimum）。不同的初始化$w_{ij}^{(\ell)}$，会得到不同的局部极值：</p>

<ul>
  <li>BP算法对权重初始值敏感；</li>
  <li>若权值太大，会落到$\tanh$的saturate区域（梯度很小），每次按梯度更新很小；</li>
  <li>用小的随机值初始化权值$w_{ij}^{(\ell)}$。</li>
</ul>

<p>若初始化$w_{ij}^{(\ell)}＝0$，由于$x_0^{(\ell)}=1$，除了${\partial e_n\over \partial w_{01}^{(L)}}\neq 0$外，其它的导数都为0；若初始化$w_{ij}^{(\ell)}＝1$，那么$w_{ij}^{(1)}=w_{i(j+1)}^{(1)}$。因此，$w_{ij}^{(\ell)}$不能初始化为0和1。</p>

<p>虽然神经网络很难最优化，但在实际中很有用。</p>

<h2 id="regularization">正则化</h2>

<p>若用形如$\tanh$的转换函数，神经网络的$d_{VC}=O(VD)$，$V$为神经元数量，$D$为神经元之间的权值数量。当神经元数目足够时，可以做任意逼近，也更容易导致过拟合。为了避免过拟合，需要采取正则化方法。</p>

<h4 id="regularization-weighted-methods">一、weight-elimination正则化</h4>

<p>常用的方法是基于$L_2$的weight-decay正则化，$\Omega(\mathbf w)=\sum\left(w_{ij}^{(\ell)}\right)^2$。这种正则化对权值的压缩（shrink）力度和权值大小“成比例”，大的权值压缩厉害，小的权值压缩较小。</p>

<p>如果通过正则化使权值部分为0（稀疏），就能有效减小$d_{VC}$，常用的方法是$L_1$正则化，$\Omega(\mathbf w)=\sum\left\lvert w_{ij}^{(\ell)}\right\rvert$，但是不可微。采用weight-elimination正则化（放缩的$L_2$正则化），大的权值中等幅度的压缩（median shrink），小的权值也中等幅度的压缩，小的权值就会接近0，具有权值稀疏化的效果。weight-elimination正则化采用
\begin{equation}
\Omega(\mathbf w)=\sum{\left(w_{ij}^{(\ell)}\right)^2\over 1+\left(w_{ij}^{(\ell)}\right)^2}，
\end{equation}
那么
\[
{\partial\Omega(\mathbf w)\over \partial w_{ij}^{(\ell)}}
={2w_{ij}^{(\ell)}\over\left(1+\left(w_{ij}^{(\ell)}\right)^2\right)^2}。
\]</p>

<h4 id="regularization-early-stopping">二、尽早停止迭代</h4>

<div class="image_line" id="figure-7"><div class="image_card"><a href="/assets/images/2015-02-04-neural-network-early-stopping.png"><img src="/assets/images/2015-02-04-neural-network-early-stopping.png" alt="迭代次数对误差的影响" /></a><div class="caption">图 7:  迭代次数对误差的影响 [<a href="/assets/images/2015-02-04-neural-network-early-stopping.png">PNG</a>]</div></div></div>

<p>迭代的次数$t$越多，选择过的$\mathbf w$也就越多，有效的$d_{VC}$也越大。小的$t$使得$d_{VC}$也较小。尽早停止（early stopping）迭代，通过如上图右的最佳$t^*$，获得如上图左的最佳$d_{VC}^*$，克服过拟合。通过验证（validation）确定停止迭代的参数$t$。</p>

<p>所有和梯度有关的优化算法，都可利用尽早停止迭代的机制，实现某种正则化。</p>

<h2 id="section-2">程序示例</h2>

<div class="highlight"><pre><code class="language-R">nnet_train <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>X<span class="p">,</span> Y<span class="p">,</span> struct<span class="p">,</span> r<span class="p">,</span> eta<span class="p">,</span> TT<span class="p">)</span> <span class="p">{</span>
  <span class="c1"># X -- a matrix with each feature row </span>
  <span class="c1"># Y -- a matrix </span>
  <span class="c1"># struct -- a vector with neuron number of each layer, including the input layer</span>
  <span class="c1"># r -- initialization range</span>
  <span class="c1"># eta -- learning rate</span>
  <span class="c1"># TT -- iteration times</span>
  
  X <span class="o">&lt;-</span> <span class="kp">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> X<span class="p">)</span>
  num_data <span class="o">&lt;-</span> <span class="kp">nrow</span><span class="p">(</span>X<span class="p">)</span>
  num_layer <span class="o">&lt;-</span> <span class="kp">length</span><span class="p">(</span>struct<span class="p">)</span> 
  nnetwork <span class="o">&lt;-</span> Xs <span class="o">&lt;-</span> Deltas <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">()</span>
  
  <span class="c1"># initialization</span>
  <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> <span class="p">(</span>num_layer <span class="o">-</span> <span class="m">1</span><span class="p">))</span> <span class="p">{</span>
    nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kt">matrix</span><span class="p">(</span>runif<span class="p">((</span>struct<span class="p">[</span>i<span class="p">]</span> <span class="o">+</span> <span class="m">1</span><span class="p">)</span> <span class="o">*</span> struct<span class="p">[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">],</span> <span class="o">-</span>r<span class="p">,</span> r<span class="p">),</span>
                            struct<span class="p">[</span>i<span class="p">]</span> <span class="o">+</span> <span class="m">1</span><span class="p">,</span> struct<span class="p">[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">])</span>
  <span class="p">}</span>
  
  <span class="kr">for</span> <span class="p">(</span>t <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> TT<span class="p">)</span> <span class="p">{</span>
    n <span class="o">&lt;-</span> <span class="kp">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span>num_data<span class="p">,</span> <span class="m">1</span><span class="p">)</span>
    
    <span class="c1"># forward</span>
    Xs<span class="p">[[</span><span class="m">1</span><span class="p">]]</span> <span class="o">&lt;-</span> <span class="kp">t</span><span class="p">(</span>X<span class="p">[</span>n<span class="p">,,</span>drop <span class="o">=</span> <span class="bp">F</span><span class="p">])</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">2</span> <span class="o">:</span> num_layer<span class="p">)</span> <span class="p">{</span>
      xx <span class="o">&lt;-</span> <span class="kp">tanh</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span>Xs<span class="p">[[</span>i <span class="o">-</span> <span class="m">1</span><span class="p">]])</span> <span class="o">%*%</span> nnetwork<span class="p">[[</span>i <span class="o">-</span> <span class="m">1</span><span class="p">]]))</span>
      <span class="kp">ifelse</span><span class="p">(</span>i <span class="o">!=</span> num_layer<span class="p">,</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kp">rbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> xx<span class="p">),</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> xx<span class="p">)</span>
    <span class="p">}</span>
    
    <span class="c1"># backward</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> num_layer <span class="o">:</span> <span class="m">2</span><span class="p">)</span> <span class="p">{</span>
      <span class="kp">ifelse</span><span class="p">(</span>i <span class="o">==</span> num_layer<span class="p">,</span>
             Deltas<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> 
               <span class="m">-2</span> <span class="o">*</span> <span class="p">(</span><span class="kp">t</span><span class="p">(</span>Y<span class="p">[</span>n<span class="p">,,</span>drop<span class="o">=</span><span class="bp">F</span><span class="p">])</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]])</span> <span class="o">*</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">^</span> <span class="m">2</span><span class="p">),</span>
             Deltas<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> 
               <span class="p">((</span>nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">%*%</span> Deltas<span class="p">[[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">]])</span> <span class="o">*</span>
                  <span class="p">(</span><span class="m">1</span> <span class="o">-</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">^</span> <span class="m">2</span><span class="p">))[</span><span class="m">2</span><span class="o">:</span><span class="kp">nrow</span><span class="p">(</span>nnetwork<span class="p">[[</span>i<span class="p">]]),,</span>drop<span class="o">=</span><span class="bp">F</span><span class="p">])</span>
    <span class="p">}</span>
    
    <span class="c1"># update weight</span>
    <span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span> <span class="o">:</span> <span class="p">(</span>num_layer <span class="o">-</span> <span class="m">1</span><span class="p">))</span> <span class="p">{</span>
      nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> nnetwork<span class="p">[[</span>i<span class="p">]]</span> <span class="o">-</span> eta <span class="o">*</span> Xs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">%*%</span> <span class="kp">t</span><span class="p">(</span>Deltas<span class="p">[[</span>i <span class="o">+</span> <span class="m">1</span><span class="p">]])</span>
    <span class="p">}</span>
    
  <span class="p">}</span>
  <span class="kr">return</span><span class="p">(</span>nnetwork<span class="p">)</span>
<span class="p">}</span></code></pre></div>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-4">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:five-input-xor">
      <p>如果用神经网络实现$XOR(x_1,\dots,x_5)$，采用结构为5-D-1，那么<a href="https://class.coursera.org/ntumltwo-001/forum/thread?thread_id=243">最小的D</a>是多少？ <a href="#fnref:five-input-xor" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tanh_output">
      <p>对于二分类问题的神经网络，输出为$\{-1,+1\}$，输出层的神经元也要采用$\tanh$，此时$e_n=\left(y_n-\tanh\left(s_1^{(L)}\right)\right)^2$，那么$\delta_1^{(L)}=-2\left(y_n-x_1^{(L)}\right)\left(1-\left(x_1^{(L)}\right)^2\right)$。 <a href="#fnref:tanh_output" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
