<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-01-17T18:21:32+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>分类器融合（2）：AdaBoost</title>
      <link href="http://qianjiye.de/2015/01/adaptive-boosting" />
      <pubdate>2015-01-17T13:02:33+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/adaptive-boosting</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">基于样本加权的误差度量</h2>

<p>bootstrapping重采样数据集$\mathcal D=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),(\mathbf x_3,y_3),(\mathbf x_4,y_4)\}$，可能得到$\tilde{\mathcal D}_t=\{(\mathbf x_1,y_1),(\mathbf x_1,y_1),(\mathbf x_2,y_2),(\mathbf x_4,y_4)\}$，那么$\tilde{\mathcal D}_t$上的in-sample误差是$E_{in}^{0/1}(h)={1\over 4}\sum_{(\mathbf x,y)\in\tilde{\mathcal D}_t}[[y\neq h(\mathbf x)]]$，令$\mathbf u^{(t)}=[2,1,0,1]^T$，该误差也可以直接用$\mathcal D$上的加权误差$E_{in}$表示，$E_{in}^{\mathbf u^{(t)}}(h)={1\over 4}\sum_{n=1}^4u_n^{(t)}\cdot[[y_n\neq h(\mathbf x_n)]]$。这就是bagging通过最小化bootstrap-weighted误差得到不同$g_t$的方法。</p>

<p>通常需要最小化的加权误差为
\[
E_{in}^{\mathbf u}(h)={1\over N}\sum_{n=1}^Nu_n\cdot err(y_n, h(\mathbf x_n))，
\]
把$\mathbf u$放回算法并不困难。对于SVM，利用对偶QP最小化误差$E_{in}^{\mathbf u}\varpropto C\sum_{n=1}^Nu_n\widehat{err}_{SVM}$，可以通过调整原方法的上界为$0\leq \alpha_n\leq Cu_n$来实现；对于logistic回归，利用SGD最小化误差$E_{in}^{\mathbf u}\varpropto C\sum_{n=1}^Nu_n\widehat{err}_{CE}$，可以通过按不同倍率$u_n$的概率采样$(\mathbf x_n,y_n)$来实现。</p>

<p>这里是基于不同样本点加权的误差度量方式，与<a href="/2014/12/machine-learning-noise-and-error/#class-weighted-error">基于不同类别加权的误差度量方式</a>的加权对象不同。如何将$\mathbf u$放回原算法是这类加权算法要处理的重要问题。</p>

<h2 id="section-1">权重调整策略</h2>

<p>如果算法会根据$\mathbf u$决定$g$，那么怎样改变$\mathbf u$使得$g$越不一样越好？越不一样的$g$，通过聚合（aggregation）机制，越有可能得到更好的结果。</p>

<p>通过$u_n^{(t)}$得到$g_t$，$u_n^{(t+1)}$得到$g_{t+1}$，
\[
\left\{
\begin{aligned}
g_t&amp;\leftarrow\arg\min_{h\in\mathcal H}\left(\sum_{n=1}^Nu_n^{(t)}[[y_n\neq h(\mathbf x_n)]]\right)\\
g_{t+1}&amp;\leftarrow\arg\min_{h\in\mathcal H}\left(\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq h(\mathbf x_n)]]\right)。
\end{aligned}
\right.
\]
如果先选定$g_t$（当作$h$），调整权重$u_n^{(t+1)}$使得$g_t$效果非常差，$g_t$以及与$g_t$相似的假设都不会被当作$g_{t+1}$，这样就能选择到一个与$g_t$很不一样的$g_{t+1}$。这就是获得不一样$g$的基本思想。理想的情况就是构造$\mathbf u_n^{(t+1)}$，使得$g_t$的表现就像随机猜想一样
\[
\frac{\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t+1)}}={1\over 2}，
\]
也就是期望
\[
\frac{\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t+1)}}=\frac{\clubsuit_{t+1}}{\clubsuit_{t+1}+\spadesuit_{t+1}}={1\over 2}，
\]
其中
\[
\clubsuit_{t+1}=\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]\qquad\spadesuit_{t+1}=\sum_{n=1}^Nu_n^{(t+1)}[[y_n= g_t(\mathbf x_n)]]。
\]</p>

<p>假设犯错误的样本点有$1126$个，正确的样本点有$6211$个，对于错分的样本点就可以用$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot {6211\over 7337}$更新，对于正确分类的样本点就可以用$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot {1126\over 7337}$更新。更新权重$\mathbf u^{(t+1)}$时，设错误率为
\begin{equation}
\epsilon_t=\frac{\sum_{n=1}^Nu_n^{(t)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t)}}，
\label{eq:epsilon-t}
\end{equation}
错误的点原来的权重乘以系数$\varpropto(1-\epsilon_t)$，正确的点原来的权重乘以系数$\varpropto\epsilon_t$。</p>

<p>通常的做法是定义缩放因子
\begin{equation}
\blacklozenge_t=\sqrt{1-\epsilon_t\over\epsilon_t}，
\label{eq:blacklozenge-t}
\end{equation}
其中$\epsilon_t$按\eqref{eq:epsilon-t}计算，权重更新方法为
\[
\mbox{incorrect}\leftarrow\mbox{incorrect}\cdot\blacklozenge_t\qquad\mbox{correct}\leftarrow\mbox{correct }/\blacklozenge_t。
\]
当$\epsilon\leq{1\over 2}$时，$\blacklozenge_t\geq 1$，放大错误的作用，缩小正确的影响，更关注错分的样本。</p>

<h2 id="adaboost">AdaBoost</h2>

<p>AdaBoost<sup id="fnref:pi-jiang-method"><a href="#fn:pi-jiang-method" class="footnote">1</a></sup> ＝ 弱的基础学习算法$\mathcal A$（学生）＋最优的权重调整因子$\blacklozenge_t$（老师）＋神奇的线性聚合$\alpha_t$（班级集体智慧）。</p>

<blockquote>
  <h4 id="adaboostadaptive-boosting">AdaBoost（<em>ada</em>ptive <em>boost</em>ing）算法</h4>
  <hr />

  <p>首先，初始化$\mathbf u^{(1)}=\left[{1\over N},{1\over N},\ldots,{1\over N}\right]$；</p>

  <p>其次，对于$t=1,2,\ldots,T$执行以下步骤：</p>

  <ol>
    <li>利用$\mathcal A(\mathcal D, \mathbf u^{(t)})$得到$g_t$，其中$\mathcal A$最小化$\mathbf u^{(t)}$加权的0/1误差；</li>
    <li>将$\mathbf u^{(t)}$更新为$\mathbf u^{(t＋1)}$：
\[
u_n^{(t+1)}\leftarrow\left\{
\begin{aligned}
u_n^{(t)}\cdot\blacklozenge_t&amp;\quad\mbox{if }[[y_n\neq g_t(\mathbf x_n)]]\\
u_n^{(t)}/\blacklozenge_t&amp;\quad\mbox{if }[[y_n= g_t(\mathbf x_n)]]，
\end{aligned}
\right.
\]
其中$\blacklozenge_t$按\eqref{eq:blacklozenge-t}计算；</li>
    <li>计算系数$\alpha_t=\ln(\blacklozenge_t)$；</li>
  </ol>

  <p>最后，返回$G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T\alpha_tg_t(\mathbf x)\right)$。</p>
</blockquote>

<p>好的$g_t$应该有大的$\alpha_t$。对于$\epsilon_t={1\over 2}$，近似于随机猜想，$\alpha_t=0$；对于$\epsilon_t=0$，完全正确的分类器，$\alpha_t=\infty$。</p>

<p>AdaBoost的VC界是
\[
E_{out}(G)\leq E_{in}(G)+O\left(\sqrt{O\left(d_{VC}(\mathcal H)\cdot T\log T\right)\cdot{\log N\over N}}\right)，
\]
其中$d_{VC}(\mathcal H)$是为了$g_t$所要付出的代价。当$g_t$的性能优于随机猜想$\left(\epsilon_t\leq\epsilon&lt;{1\over 2}\right)$时，经过$T=O(\log N)$轮迭代就可以达到$E_{in}(G)=0$。由于总共的$d_{VC}=O\left(d_{VC}(\mathcal H)\cdot T\log T\right)$随$T$增长缓慢，不等式右边第二项也可以做到很小。</p>

<p>AdaBoost是一种提升算法（boosting）的实现，从boosting的角度，若$\mathcal A$略优于随机猜想$\left(\epsilon_t\leq\epsilon&lt;{1\over 2}\right)$，AdaBoost＋$\mathcal A$可以达到很强大的性能（$E_{in}(G)=0$，$E_{out}$很小）。</p>

<h2 id="adaboost-stump">AdaBoost-Stump</h2>

<p>对于一个decision stump分类器
\[
h_{s,i,\theta}(\mathbf x)=s\cdot\mbox{sign}(\mathbf x_i-\theta)，
\]
$i$表示特征维，$\theta$表示阈值，$s$控制方向，在2D空间该分类器就是水平或竖直线，该算法优化的时间复杂度为$O(d\cdot N\log N)$。decision stump能够高效的最小化$E_{in}^\mathbf u$，但是自身的性能却很弱。
将decision stump作为基础分类器，可以组合出功能强大的<strong>AdaBoost-Stump</strong>。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png"><img src="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png" alt="AdaBoost-Stump示例" /></a><div class="caption">Figure 1:  AdaBoost-Stump示例 [<a href="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png">PNG</a>]</div></div></div>

<p>上图展示了基于decision stump构造的AdaBoost-Stump，当$t=5$时就能完美的分类。AdaBoost-Stump能够比核SVM更高效地得到非线性分类器。</p>

<p>世界上第一个实时人脸识别程序就是基于AdaBoost-Stump。从$24\times 24$规格的162336张候选图片中挑选关键图片，在此基础上进行线性聚合（linear aggregation）<sup id="fnref:how-to-do-AdaBoost-face"><a href="#fn:how-to-do-AdaBoost-face" class="footnote">2</a></sup>。为了提高速度，人脸识别采用的$G$会尽早排除非人脸。</p>

<p>AdaBoost-Stump能够有效的进行特征选择和聚合。</p>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-3">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:pi-jiang-method">
      <p>也可称为“皮匠法”，意为“三个臭皮匠，胜过诸葛亮”。 <a href="#fnref:pi-jiang-method" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-to-do-AdaBoost-face">
      <p>这里如何构造人脸识别程序的呢？ <a href="#fnref:how-to-do-AdaBoost-face" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>分类器融合（1）：混合与自助聚合</title>
      <link href="http://qianjiye.de/2015/01/blending-and-bagging" />
      <pubdate>2015-01-15T15:12:32+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/blending-and-bagging</guid>
      <content:encoded>&lt;![CDATA[<p><strong>混合</strong>（blending）是将不同的假设用均匀、线性或非线性的方式组合起来；如果先从boostrapped数据上学习到各种不同的假设，然后再混合，就称为<strong>自助聚合</strong>（bagging, bootstrap aggregating）。</p>

<h2 id="section">聚合法简介</h2>

<p>如何将特征和假设集组合起来，让机器学习的性能更好呢？</p>

<p>假设$T$个👬朋友$g_1,\ldots,g_T$给出参考意见$g_t(\mathbf x)$预测股票是否会涨，如何决策呢？</p>

<ol>
  <li>校验法（validation）：听从最懂股票那个朋友的意见，
\begin{equation*}
G(\mathbf x)=g_{t_*}(\mathbf x)，\qquad t_*=\arg\min_{t\in\{1,2,\ldots,T\}}E_{val}\left(g_t^-\right)，
\end{equation*}
$E_{val}\left(g_{t}^-\right)$表示$g^-$是在一个较小数据集上得到的结果，选择完成之后在全验证集上得到完整的$g$。</li>
  <li>投票法（vote）：一人一票的均匀投票，听从多数人的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T1\cdot g_t(\mathbf x)\right)。
\label{eq:uniform-blending-hypothesis}
\end{equation}</li>
  <li>加权投票法：每个人的票数不一样，听从多数票的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T\alpha_t\cdot g_t(\mathbf x)\right),\qquad\alpha_t\geq 0。
\label{eq:linear-blending-hypothesis}
\end{equation}
当$\alpha_t=[[E_{val}\left(g_{t}^-\right)\mbox{ smallest}]]$时，与方法1一样；当$\alpha_t=1$时，与方法2一样。</li>
  <li>有条件的组合：比如科技股听从擅长这方面的朋友，传统行业股票听从那些……
\begin{equation*}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T q_t(\mathbf x)\cdot g_t(\mathbf x)\right),\qquad q_t(\mathbf x)\geq 0，
\end{equation*}
当$q_t(\mathbf x)=\alpha_t$时，与方法3一样，也就是包含了前面所有情况。</li>
</ol>

<p><strong>聚合法</strong>的目的是综合多个假设（可能是比较弱的）让效果更好。上述方法中，2～4称为<strong>聚合模型</strong>（aggregation model）。</p>

<p>对于上述1的校验（validation）方法，如果用$E_{in}(g_t)$代替$E_{val}(g_t)$进行选择，最终可能会付出很大VC维的代价<sup id="fnref:why-large-dvc"><a href="#fn:why-large-dvc" class="footnote">1</a></sup>。这种方法需要一个强大优秀的$g_t^-$，否则也只是差中择优。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png"><img src="/assets/images/2015-01-15-blending-and-bagging-vote-method.png" alt="［左］：水平垂直线投票；［右］PLA投票" /></a><div class="caption">Figure 1:  ［左］：水平垂直线投票；［右］PLA投票 [<a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png">PNG</a>]</div></div></div>

<p>上图展示了利用聚合法的投票机制，灰色的判别界表示参与投票的分类器。上图左中，水平线将平面分割成了9块区域，如果按“垂直、水平左，水平右”的顺序投票，每个区域中的投票结果如图中绿色标注，最终结果相当于组合成了黑色的判别界。上图左展示的投票方法，相当于特征变换（feature transform）的效果<sup id="fnref:why-feateure-tansform"><a href="#fn:why-feateure-tansform" class="footnote">2</a></sup>；上图右展示了感知器的投票结果，相当于正则化的效果。通过上图可看出，合理的集合方法能提升性能。</p>

<h2 id="section-1">均匀混合</h2>

<p>均匀混合（uniform blending）也就是投票(voting)方法\eqref{eq:uniform-blending-hypothesis}。如果每个$g_t$都相同，结果等价于任意的$g_t$；如果每个$g_t$千差万别，就是少数服从多数。该方法可以直接推广到多类的情况，
\[
G(\mathbf x)=\arg\max_{1\leq k\leq K}\sum_{t=1}^T[[g_t(\mathbf x)=k]]。
\]</p>

<p>均匀混合解决回归问题的方法是
\[
G(\mathbf x) = {1\over T}\sum_{t=1}^Tg_t(\mathbf x)。
\]
如果每个$g_t$都相同，结果等价于任意的$g_t$；对于不同的$g_t$，可能得到比单一判别更精确的结果。</p>

<p>由此可见，对于多个不同的假设，即使采用简单的混合法则，也可以得到比单一假设更好的结果。</p>

<p>对于均匀混合的回归，
\[
\begin{aligned}
avg\left(\left(g_t(\mathbf x)-f(\mathbf x)\right)^2\right)
=&amp;avg\left(g_t^2-2g_t^2f+f^2\right)\\
=&amp;avg\left(g_t^2\right)-2Gf+f^2\\
=&amp;avg\left(g_t^2\right)-G^2+(G-f)^2\\
=&amp;avg\left(g_t^2\right)-2G^2 + G^2 +(G-f)^2\\
=&amp;avg\left(g_t^2-2g_tG + G^2\right) +(G-f)^2\\
=&amp;avg\left(\left(g_t-G\right)^2\right) +(G-f)^2。
\end{aligned}
\]</p>

<p>若对产生$\mathbf x$分布的所有点都进行上述运算，然后取期望可得
\[
avg\left(E_{out}\left(g_t\right)\right)=avg(\varepsilon(g_t-G)^2)+E_{out}(G)\geq E_{out}(G)，
\]
也就是说，均匀混合方法的效果会比选择其中一个好。</p>

<p>从$P^N$采集大小为$N$的$T$个数据集，利用上述公式，衡量演算法$\mathcal A$的表现。通过$\mathcal A(\mathcal D_t)$获得$g_t$，对其平均
\[
\bar g=\lim_{T\rightarrow\infty}G=\lim_{T\rightarrow\infty}{1\over T}\sum_{t=1}^Tg_t=\varepsilon_{\mathcal D}\mathcal A(\mathcal D)，
\]
算法$\mathcal A$的性能期望为
\begin{equation}
avg(E_{out}(g_t))=avg(\varepsilon(g_t-\bar g)^2)+E_{out}(\bar g)。
\label{eq:bias-variance-decomposition}
\end{equation}</p>

<p>上述公式中：</p>

<ul>
  <li>$avg(E_{out}(g_t))$：算法$\mathcal A$的期望性能；</li>
  <li>$E_{out}(\bar g)$：算法共识（consensus）的性能（$\bar g$就是从$\mathcal D_t\sim P^N$期望获得的$g_t$），通常称为<strong>bias</strong>；</li>
  <li>$avg(\varepsilon(g_t-\bar g)^2)$：偏离共识的期望，通常称为<strong>variance</strong>。</li>
</ul>

<p>通过bias和variance，将演算法的表现拆分为两部分。均匀混合通过减小variance获得更稳定的性能。</p>

<h2 id="section-2">线性混合</h2>

<p>通过\eqref{eq:linear-blending-hypothesis}，赋予不同假设不同的权重就是<strong>线性混合</strong>（linear blending）。</p>

<p>线性回归的线性混合目标为
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^N\left(y_n-\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)^2，
\]
将$g(\mathbf x)$视为特征变换$\phi(\mathbf x)$，换一种表达形式
\[
\min_{\mathbf w}{1\over N}\sum_{n=1}^{N}\left(y_n-\sum_{i=1}^{\tilde d}w_i\phi_i(\mathbf x_n)\right)^2，
\]
这就类似两阶（two-level）的学习方法。</p>

<p>线性混合＝线性模型＋假设（hypothesis）视为变换＋约束条件，
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^Nerr\left(y_n,\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)。
\]
对于二分类问题
\[
\alpha_tg_t(\mathbf x)=|\alpha_t|(-g_t(\mathbf x))\qquad\mbox{if }\alpha_t&lt;0，
\]
正负对分类器本质上没有差别，实际上有“线性混合＝线性模型＋假设（hypothesis）视为变换”，不需要$\alpha_t$的约束条件。</p>

<p>在实际中，$g_t$通常是用$E_{in}$从各模型中选的最优，$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\dots,g_T\in\mathcal H_T$。如果在这些$g_t$中用$E_{in}$再选最优的，就是best of best，将付出高复杂度$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。如果在这些$g_t$中用$E_{in}$再采用线性混合，就是aggregation of best，将付出<strong>高于</strong>$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。实际上通常采用$E_{val}$替代$E_{in}$，通过最小化$E_{train}$得到$g_t^-$。</p>

<blockquote>
  <h4 id="section-3">线性混合算法</h4>
  <hr />

  <ol>
    <li>从$\mathcal D_{train}$中获取$g_1^-,g_2^-,\ldots,g_T^-$；     </li>
    <li>在$\mathcal D_{val}$中将$(\mathbf x_n,y_n)$转换为$(\mathbf z_n=\phi^-(\mathbf x_n),y_n)$，其中$\phi^-(\mathbf x)＝(g_1^-(\mathbf x),g_2^-(\mathbf x),\ldots,g_T^-(\mathbf x))$；</li>
    <li>计算$\boldsymbol\alpha=\mbox{LinearModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
    <li>返回 $G_{LINB}(\mathbf x)=\mbox{LinearHypothesis}_\boldsymbol\alpha(\phi(\mathbf x))$，其中$\phi(\mathbf x)＝(g_1(\mathbf x),g_2(\mathbf x),\ldots,g_T(\mathbf x))$<sup id="fnref:how-gt-gt-x"><a href="#fn:how-gt-gt-x" class="footnote">3</a></sup>。</li>
  </ol>

</blockquote>

<p><strong>如果将3、4两步换为：</strong></p>

<ul>
  <li>计算$\tilde g=\mbox{AnyModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
  <li>返回$G_{ANYB}(\mathbf x)=\tilde g(\phi(\mathbf x))$。</li>
</ul>

<p>这就是any blending（stacking）的方法。any blending方法强大，可以实现conditional blending，但是也存在过拟合的风险。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png"><img src="/assets/images/2015-01-15-blending-and-bagging-blending-example.png" alt="混合方法使用实例" /></a><div class="caption">Figure 2:  混合方法使用实例 [<a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png">PNG</a>]</div></div></div>

<p>上图的流程中，Val.-Set Blending就是any blending的方法，使得$E_{test}$降低到$456.24$。然后再将这一步得到的上百个$g，G$用近似的$\tilde E_{test}$（$\tilde E_{test}$好可使真正的$E_{test}$更好）进行linear blending。</p>

<p>混合（blending）在实际中很有用，但是模型计算复杂度增大了。</p>

<h2 id="section-4">⾃助聚合</h2>

<p>混合（blending）方法的策略是先学习到$g_t$，然后再聚合（aggregate）。<del>能否在学习$g_t$的同时进行聚合呢？✅</del></p>

<p>对于均匀聚合（uniform aggregation），不同模型参与聚合是关键，获取不同模型的方法包括：</p>

<ul>
  <li>从不同假设集获取模型：$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\ldots,g_T\in\mathcal H_T$；</li>
  <li>采用不同的参数：对于梯度下降法，$\eta=0.001,0.01,\ldots,10$；</li>
  <li>利用算法的随机性：从不同的初始化开始PLA；</li>
  <li>利用数据的随机性：交叉检验采用不同数据集验证$g_v^-$<sup id="fnref:how-to-gv-x"><a href="#fn:how-to-gv-x" class="footnote">4</a></sup>。</li>
</ul>

<p>回顾\eqref{eq:bias-variance-decomposition}，算法的性能被拆分为bias和variance两部分进行评估。共识的结果会比$\mathcal A(\mathcal D)$的单一$g$效果好，但是每次都需要用不同的$\mathcal D_t$获得$g_t$。</p>

<p>能否通过有限的$T$和单一的数据集$\mathcal D$得近似的$\bar g$？✅</p>

<p><strong>bootstrapping</strong>是一种通过重采样（re-sample）$\mathcal D$模拟$\mathcal D_t$的统计学工具。bootstrap得到$\tilde{\mathcal D}_t$的方法：从$\mathcal D$中随机抽取一个点，纪录该点后然后放回，重复该过程直到抽取到$N’$个数据。</p>

<blockquote>
  <h4 id="section-5">自助聚合算法</h4>
  <hr />

  <ol>
    <li>利用bootstrapping技术得到$N’$点的数据集$\tilde{\mathcal D}_t$；</li>
    <li>利用$\mathcal A(\tilde{\mathcal D}_t)$，算法$\mathcal A$在数据集$\tilde{\mathcal D}_t$上得到$g_t$；</li>
    <li>$G=\mbox{Uniform}(g_t)$。</li>
  </ol>
</blockquote>

<p><strong>⾃助聚合</strong>（<strong>b</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>）也称为<strong>打包</strong>（bagging）。像⾃助聚合这种，建立在其它基础算法（base algorithm）$\mathcal A$之上的算法称为meta algorithm。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png"><img src="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png" alt="bagging pocket方法" /></a><div class="caption">Figure 3:  bagging pocket方法 [<a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png">PNG</a>]</div></div></div>

<p>上图是bagging pocket方法的效果，通过bagging得到各不相同的$g_t$，然后通过聚合得到合适的非线性分类器。</p>

<p>如果基础算法（base algorithm）对数据的随机性很敏感，bagging可以工作得相当好。</p>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:why-large-dvc">
      <p>为什么会付出很大VC维的代价？ <a href="#fnref:why-large-dvc" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-feateure-tansform">
      <p>为什么相当于特征变换的效果？ <a href="#fnref:why-feateure-tansform" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-gt-gt-x">
      <p>$g_t^-(\mathbf x)$和$g_t(\mathbf x)$具体如何计算？ <a href="#fnref:how-gt-gt-x" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-to-gv-x">
      <p>这步如何操作？ <a href="#fnref:how-to-gv-x" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>线性回归</title>
      <link href="http://qianjiye.de/2015/01/linear-regression" />
      <pubdate>2015-01-13T20:17:01+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/linear-regression</guid>
      <content:encoded>&lt;![CDATA[<p><img src="/assets/images/2014-10-19-linear_regression_0.png" alt="线性回归" /></p>

<p>本节的主要参考资料是机器学习<a href="#ng_ml_linreg_2014">[1]</a>和机器学习基石<a href="#lin_mlf_linreg_2014">[2]</a>网络课程。</p>

<h2 id="section">线性回归模型</h2>

<p>线性回归的假设集（hypothesis）为
\begin{equation}
h(\mathbf x) = \mathbf w^T\mathbf x，
\label{eq:linear-regression-model}
\end{equation}
$\mathbf x$是含有常数项$x_0 = 1$的$d+1$维向量$\mathbf x =\left[1,x_1,\ldots,x_d\right]^T$，$\mathbf w=\left[w_0,w_1,\ldots,w_d\right]^T$，$d$是特征维数。</p>

<p>目标代价函数采用平方误差和估计
\begin{equation}
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n-y_n\right)^2，
\label{eq:cost-function-linear-regression}
\end{equation}
$N$是样本数。通过机器学习算法求得线性回归的参数
\begin{equation}
\mathbf w_{LIN} = \arg\min_{\mathbf w}E_{in}(\mathbf w)，
\end{equation}
通常方法有正规方程（normal equation）的解析方法和梯度下降法（gradient descent）。</p>

<h2 id="section-1">解析方法</h2>

<p>解析方法得到的最优解也称为<strong>线性回归的最小二乘解</strong>。</p>

<p>代价函数改写为矩阵形式
\[
E_{in}(\mathbf w) 
={1\over N}\lVert\mathbf X\mathbf w-\mathbf y\rVert^2
={1\over N}\left(\mathbf w^T\mathbf X^T\mathbf X\mathbf w-2\mathbf w^T\mathbf X^T\mathbf y+\mathbf y^T\mathbf y\right)，
\]
$\mathbf X$是样本数据矩阵，每行代表一个样本点，每列代表一个特征，第一列的向量$\mathbf 1_N$对应常数偏移。$E_{in}(\mathbf w)$是连续可微的凸函数，当$\nabla E_{in}(\mathbf w)=0$时取得最小值，
\[
\nabla E_{in}(\mathbf w)={2\over N}\left(\mathbf X^T\mathbf X\mathbf w-\mathbf X^T\mathbf y\right)，
\]
取得最小值时
\begin{equation}
\mathbf w_{LIN} = \left(\mathbf X^T\mathbf X\right)^{-1}\mathbf X^T\mathbf y = \mathbf X^\dagger \mathbf y，
\end{equation}
$\mathbf X^\dagger$称为<strong>伪逆</strong>（pseudo-inverse）。通常情况$N\gg d+1$，因此$\left(\mathbf X^T\mathbf X\right)^{-1}$通常都可逆；如果不可逆，解不唯一。</p>

<p>导致$\mathbf X^T\mathbf X$不可逆的原因可能是冗余特征（redundant features）或者特征数目过多（$d$太大而$N$太少），解决的办法：   </p>

<ul>
  <li>对于冗余的线性相关特征，例如$x_1 = 2x_2$，删除线性相关特征；</li>
  <li>对于特征数目过多，例如$N&lt;d$，删除特征或正则化（regularization）<sup id="fnref:how-to-regularize"><a href="#fn:how-to-regularize" class="footnote">1</a></sup>。</li>
</ul>

<p>Matlab的<code>pinv</code>函数可以处理$\mathbf X^T\mathbf X$不可逆的情况<sup id="fnref:pinv-vs-inv"><a href="#fn:pinv-vs-inv" class="footnote">2</a></sup>。</p>

<h3 id="mathbf-wlin">解析方法求解$\mathbf w_{LIN}$是机器学习算法吗？</h3>

<p>✅通过VC维的角度分析，能得到小的$E_{out}\left(\mathbf w_{\small{LIN}}\right)$，就是学习……</p>

<ul>
  <li>能够得到最佳的$E_{in}$；</li>
  <li>$d+1$个变量，有限的$d_{VC}$，因此有好的$E_{out}$；</li>
  <li>事实上，求解伪逆的过程也是迭代逐步最优的过程（高斯消元法）。</li>
</ul>

<p>VC维考察的是个别的$E_{in}$<sup id="fnref:some-E-in"><a href="#fn:some-E-in" class="footnote">3</a></sup>，从$E_{in}$平均误差角度分析
\begin{equation}
\bar E_{in}
=\varepsilon_{\mathcal D\sim P^N}\left\{E_{in}\left(\mathbf w_{LIN}\mbox{ w.r.t }\mathcal D\right)\right\}
=\mbox{noise level}\cdot\left(1-{d+1\over N}\right)，
\label{eq:noise-level-e-in}
\end{equation}
$\mbox{noise level}$表示数据中的噪声，${d+1\over N}$表示比噪声小的比率，数据越多二者差别越小。</p>

<p>$E_{in}\left(\mathbf w_{LIN}\right)$计算方法为
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert\mathbf y-\mathbf X\mathbf X^\dagger\mathbf y\right\rVert^2
={1\over N}\left\lVert\left(\mathbf I-\mathbf X\mathbf X^\dagger\right)\mathbf y\right\rVert^2，
\]
$\mathbf X\mathbf X^\dagger$让$\mathbf y$加帽$\wedge$变成了$\hat{\mathbf y}$，也叫<strong>帽矩阵</strong><sup id="fnref:hat-matrix-properties"><a href="#fn:hat-matrix-properties" class="footnote">4</a></sup>（hat matrix），记为$\mathbf H$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-learning-curve.png"><img src="/assets/images/2015-01-13-linear-regression-learning-curve.png" alt="［左］：图解证明；［右］：学习曲线" /></a><div class="caption">Figure 1:  ［左］：图解证明；［右］：学习曲线 [<a href="/assets/images/2015-01-13-linear-regression-learning-curve.png">PNG</a>]</div></div></div>

<p>由$\hat{\mathbf y}=\mathbf X\mathbf w_{LIN}$可知，$\hat{\mathbf y}$是$\mathbf X$列向量的线性组合，也就是如上图左所示，$\hat{\mathbf y}$位于$\mathbf X$张成的线性空间中。当$\mathbf y-\hat{\mathbf y}$垂直于该生成空间时，$\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2$的值最小。$\mathbf H$将$\mathbf y$投影为$\hat{\mathbf y}$，$\mathbf I-\mathbf H$将$\mathbf y$投影为$\mathbf y-\hat{\mathbf y}$。$\mathbf I-\mathbf H$的迹为$trace(\mathbf I-\mathbf H)=N-(d+1)$，表示自由度从$N$降到$N-(d+1)$。</p>

<p>观测到的数据$\mathbf y$是理想的数据空间$f\left(\mathbf X\right)$叠加一些噪声。$\mathbf y-\hat{\mathbf y}$也可以从噪声投影得到，如上图左所示，
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert(\mathbf I-\mathbf H)\cdot\mbox{noise}\right\rVert^2
={1\over N}(N-(d+1))\lVert\mbox{noise}\rVert^2，
\]
因此可得公式\eqref{eq:noise-level-e-in}的结论。$E_{out}$的证明过程叫复杂，仍然可以得到
\[
\bar E_{out}
=\mbox{noise level}\cdot\left(1+{d+1\over N}\right)。
\]</p>

<p>$\mbox{noise level}$可以用$\sigma^2$表示，从上图右可见，$\bar E_{in}$和$\bar E_{out}$在$N\rightarrow\infty$时都会趋近于$\sigma^2$。期望的泛化误差（generalization error）可以用${2(d+1)\over N}$衡量，这里是平均情况，VC维衡量的是最坏的情况。</p>

<h2 id="section-2">梯度下降法</h2>

<p>梯度下降法就是沿梯度下降方向更新参数，也就是对每个特征的权值$w_i$，不断迭代执行更新
\begin{equation}
w_i := w_i-\alpha{\partial E_{in}(\mathbf w)\over\partial w_i}\quad(i=0,1,\ldots,d)
\end{equation}
直至收敛，其中$\alpha$表示学习速率，梯度计算公式为
\[
{\partial E_{in}(\mathbf w)\over\partial w_i}
={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n - y_n\right)x_{n,i}。
\]</p>

<p>参数须同时更新，也就是当每个$w_i$都更新完成后，才能用新的$\mathbf w$计算$E_{in}(\mathbf w)$，具体可参考<a href="/assets/images/2014-10-19-linear_regression_1.png">Andrew NG的讲义</a><sup id="fnref:andrew-simultaneous-update"><a href="#fn:andrew-simultaneous-update" class="footnote">5</a></sup>。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-curve.png"><img src="/assets/images/2015-01-13-linear-regression-error-curve.png" alt="［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；&lt;br/&gt;［右］：梯度下降路径" /></a><div class="caption">Figure 2:  ［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；<br />［右］：梯度下降路径 [<a href="/assets/images/2015-01-13-linear-regression-error-curve.png">PNG</a>]</div></div></div>

<p>梯度下降法需要将所有特征归一（feature scaling）到统一的尺度（不用归一化$x_0$），比如$-1\le x_i\le 1$，
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\qquad\mbox{or}\qquad
\hat{x}_i = \frac{x_i - x_{mean}}{x_{std}}，
\]
这样有助于提高梯度下降法的速度，如上图左2所示。</p>

<h4 id="alpha">学习率$\alpha$的注意事项：</h4>
<ol>
  <li>在迭代过程中不需调节$\alpha$大小，由于梯度会不断减小，在固定$\alpha$的情况下梯度下降步长也会自动减小，如上图右所示；</li>
  <li>$\alpha$太小收敛慢，太大可能错过极值点而不收敛，甚至可能导致$E_{in}(\mathbf w)$不降反升。</li>
</ol>

<p>线性回归的代价函数$E_{in}(\mathbf w)$不存在局部极值（local optima），极小值就是全局极值<sup id="fnref:no-local-optima"><a href="#fn:no-local-optima" class="footnote">6</a></sup>。</p>

<h2 id="section-3">两种方法对比</h2>

<table>
  <thead>
    <tr>
      <th>梯度下降法</th>
      <th>解析解法</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>迭代实现，可实现在线增量学习</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$d$很大时（$10^6$）工作良好</td>
      <td>$d$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化<sup id="fnref:why-not-scale"><a href="#fn:why-not-scale" class="footnote">7</a></sup></td>
    </tr>
  </tbody>
</table>

<h2 id="section-4">分类问题</h2>

<p>线性分类器和线性回归的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>指标</th>
      <th>线性分类器</th>
      <th>线性回归</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathcal Y$</td>
      <td>$\{-1,+1\}$</td>
      <td>$\mathbb R$</td>
    </tr>
    <tr>
      <td>$h(\mathbf x)$</td>
      <td>$\mbox{sign}\left(\mathbf w^T\mathbf x\right)$</td>
      <td>$\mathbf w^T\mathbf x$</td>
    </tr>
    <tr>
      <td>$err(\hat{y},y)$</td>
      <td>$[[\hat{y}\neq y]]$</td>
      <td>$(\hat{y}-y)^2$</td>
    </tr>
    <tr>
      <td>算法复杂度</td>
      <td>通常是NP-hard</td>
      <td>高效求解方法</td>
    </tr>
  </tbody>
</table>

<p>能否利用线性回归的高效，借助$g(\mathbf x)=\mbox{sign}\left(\mathbf w_{LIN}^T\mathbf x\right)$，用线性回归解决分类问题？✅</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-compare.png"><img src="/assets/images/2015-01-13-linear-regression-error-compare.png" alt="线性分类器和线性回归误差比较" /></a><div class="caption">Figure 3:  线性分类器和线性回归误差比较 [<a href="/assets/images/2015-01-13-linear-regression-error-compare.png">PNG</a>]</div></div></div>

<p>上图展示了两种方法误差的对比，$err_{0/1}\leq err_{sqr}$，平方误差是0/1误差的上限。从VC维的理论可知
\[
\mbox{classification }E_{out}(\mathbf w)
\leq \mbox{classification }E_{in}(\mathbf w)+\sqrt{\cdots}
\leq \mbox{regression }E_{in}(\mathbf w)+\sqrt{\cdots}，
\]
in-sample回归误差也是out-sample分类误差的上限，做好in-sample回归误差也是做好in-sample分类误差的一种方法，in-sample回归误差很小时能保证out-sample分类误差也很小。由此可见，可用线性回归解决分类问题。</p>

<p>也可直接将回归问题视为分类问题，只是用$err_{sqr}$当作$\widehat{err}$作为$err_{0/1}$误差的近似。为分类问题选择稍宽松的误差上界，这样容易求解参数。</p>

<p>在很多时候，用线性回归解决分类问题效果尚可。如果要让效果更好，可将$\mathbf w_{LIN}$当做PLA或pocket算法的初始值$\mathbf w_0$，加速PLA或pocket算法。</p>

<h2 id="section-5">多项式回归</h2>

<p>构造多项式特征，利用线性回归模型解决非线性问题，称为<strong>多项式回归</strong>（polynomial regression）。例如利用$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $，构造新的特征向量$\mathbf x$，带入线性回归模型\eqref{eq:linear-regression-model}求解。从另一个角度看，当特征是多项式时，可直接利用线性模型求解。</p>

<p>当对特征进行高次多项式变换后，取值范围可能急剧变化，需要对多项式特征进行尺度归一化处理<a href="#ng_ml_rlrbv_pe_2014">[3, P. 8]</a>。</p>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_linreg_2014">[1]A. Ng, “Linear Regression with multiple variables.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="lin_mlf_linreg_2014">[2]H.-T. Lin, “Lecture 9: Linear Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_rlrbv_pe_2014">[3]A. Ng, “Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:how-to-regularize">
      <p>如何进行正则化？ <a href="#fnref:how-to-regularize" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pinv-vs-inv">
      <p>Matlab的<code>pinv</code>和<code>inv</code>有何区别？ <a href="#fnref:pinv-vs-inv" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:some-E-in">
      <p>为什么VC维考察的是个别的$E_{in}$？ <a href="#fnref:some-E-in" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:hat-matrix-properties">
      <p>帽矩阵的性质（可从文中图示的角度理解）：（1）$\mathbf H$是对称的；（2）$\mathbf H^2=\mathbf H$；（3）$(\mathbf I-\mathbf H)^2=\mathbf I-\mathbf H$。 <a href="#fnref:hat-matrix-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-simultaneous-update">
      <p>事实上，不同时更新的情况很少发生，因为在更新每个$w_i$前，已经用$\mathbf w$计算过了$E_{in}(\mathbf w)$，更新过程中，不再需要重复计算$E_{in}(\mathbf w)$。 <a href="#fnref:andrew-simultaneous-update" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:no-local-optima">
      <p>这是真的吗？ <a href="#fnref:no-local-optima" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-scale">
      <p>为什么解析方法不需要规范化特征？ <a href="#fnref:why-not-scale" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（6）：支持向量回归</title>
      <link href="http://qianjiye.de/2015/01/svm-support-vector-regression" />
      <pubdate>2015-01-12T18:31:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-support-vector-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">脊回归</h2>

<p>有正则化项的回归称为<strong>脊回归</strong>（ridge regression）。脊回归的核模型有解析解么？</p>

<p>脊回归的最优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\left(y_n-\mathbf w^T\mathbf z_n\right)^2\right)，
\]
根据表示定理可知，存在形如$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$的最优解，将其带入并表示为核形式
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\left(y_n-\sum_{n=1}^N\beta_mK(\mathbf x_n,\mathbf x_m)\right)\right)。
\end{equation}
<strong>脊回归的核模型</strong>就是利用表示定理将脊回归核化。目标函数写为矩阵的形式
\begin{equation}
E_{aug}(\boldsymbol\beta)={\lambda\over N}\boldsymbol\beta^T\mathbf K\boldsymbol\beta + {1\over N}\left(\boldsymbol\beta^T\mathbf K^T\mathbf K\boldsymbol\beta-2\boldsymbol\beta^T\mathbf K^T\mathbf y + \mathbf y^T\mathbf y\right)，
\end{equation}
无约束最优化问题可以通过
\[
\nabla E_{aug}(\boldsymbol\beta)={2\over N}\left(\lambda\mathbf K^T\mathbf I\boldsymbol\beta+\mathbf K^T\mathbf K\boldsymbol\beta-\mathbf K^T\right)={2\over N}\mathbf K^T\left((\lambda\mathbf I+\mathbf K)\boldsymbol\beta-\mathbf y\right)
\]
令$\nabla E_{aug}(\boldsymbol\beta)=0$求解，
\begin{equation}
\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y。
\end{equation}
根据Mercer条件可知$\mathbf K$半正定，并且$\lambda&gt;0$，因此矩阵总可逆。稠密矩阵求逆的时间复杂度为$O\left(N^3\right)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png" alt="［左］：脊回归的线性模型；［右］：脊回归的核模型" /></a><div class="caption">Figure 1:  ［左］：脊回归的线性模型；［右］：脊回归的核模型 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png">PNG</a>]</div></div></div>

<p>上图中的蓝线是脊回归的效果。线性模型与核模型的选择是速度与效率的折中权衡，它们之间的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>线性模型</th>
      <th>核模型</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathbf w = (\lambda\mathbf I+\mathbf X^TX)^{-1}\mathbf X^T\mathbf y$</td>
      <td>$\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y$</td>
    </tr>
    <tr>
      <td>功能受限</td>
      <td>通过$K$实现强大的功能</td>
    </tr>
    <tr>
      <td>训练时间复杂度为$O\left(d^3+d^2N\right)$</td>
      <td>训练时间复杂度为$O\left(N^3\right)$</td>
    </tr>
    <tr>
      <td>预测时间复杂度为$O(d)$，当$N\gg d$时效率高</td>
      <td>预测时间复杂度为$O(N)$，当训练样本大时效率低</td>
    </tr>
  </tbody>
</table>

<p>当参数获得后，回归函数就可以用核表示为
\begin{equation}
g(\mathbf x)=\sum_{n=1}^N\beta_nK\left(\mathbf x_n, \mathbf x\right)。
\end{equation}</p>

<h2 id="section-1">最小二乘支持向量机</h2>

<p><strong>最小二乘支持向量机</strong>（LSSVM，least-squares SVM）就是将脊回归的核模型用于分类。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png" alt="最小二乘与soft-margin支持向量机的对比" /></a><div class="caption">Figure 2:  最小二乘与soft-margin支持向量机的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png">PNG</a>]</div></div></div>

<p>上图可以看出，最小二乘与soft-margin支持向量机的分类面很相似，但是LSSVM的支持向量要多得多，预测速度会很慢。</p>

<p>LSSVM和logistic回归的核模型得到的参数$\boldsymbol\beta$是稠密的，标准支持向量机的参数$\boldsymbol\alpha$是稀疏的。能否得到像标准支持向量机一样稀疏的$\boldsymbol\beta$呢？</p>

<h2 id="tube">tube回归</h2>

<p>定义tube回归的误差为
\begin{equation}
err(y, s) = \max(0, \lvert s-y\rvert-\epsilon)，
\end{equation}
在tube区域内不计误差，在该区域外到tube的距离记为误差。</p>

<div class="image_line" id="tube-error-illustration"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png" alt="tube与平方误差对比" /></a><div class="caption">Figure 3:  tube与平方误差对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png">PNG</a>]</div></div></div>

<p>如上图所示，tube与平方误差很相似，尤其是在$\lvert s-y\rvert$很小的区域内，当$\lvert s-y\rvert$很大时，tube误差不如平方误差变化陡峭，因此受噪声影响更小。</p>

<p>基于tube误差的模型能否得到稀疏的系数呢？</p>

<h2 id="section-2">支持向量回归</h2>

<p>基于$L_2$正则化的tube回归模型为
\begin{equation*}
\min\limits_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽无约束，但$\max$导致不可微；利用表示定理可核化，但无法明确得到稀疏的系数。仿照<a href="/2015/01/svm-kernel-logistic-regression/#mjx-eqn-equniform-soft-margin-svm">无约束形式</a>的soft-margin支持向量机，分离出$b$后改写为
\begin{equation*}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n+b-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽然不可微，但是QP问题；对偶问题可核化，KKT条件能得到系数稀疏。再对比<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">约束形式</a>的soft-margin支持向量机，改写为带约束的优化问题
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad \left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert\leq \epsilon+\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\end{equation*}
约束条件线性化
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi^\vee,\boldsymbol\xi^\wedge}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left(\xi_n^\vee+\xi_n^\wedge\right)\\
\mbox{s.t.}&amp;\quad -\epsilon-\xi_n^\vee\leq y_n - \mathbf w^T\mathbf z_n-b\leq \epsilon+\xi_n^\wedge\\
&amp;\quad\xi_n^\vee\geq 0,\xi_n^\wedge\geq 0\mbox{ for all }n，
\end{aligned}
\end{equation}
这就是标准的<strong>支持向量回归</strong>（SVR，support vector regression）原问题，$\xi_n^\vee$和$\xi_n^\wedge$分别记录tube下届和上界违规，如<a href="#tube-error-illustration">上图左</a>所示的分界线下边和上边标红的线段。通过$C$对正则化和tube违规进行折中，调节参数$\epsilon$可控制tube的高度。该QP模型有$\tilde d+1+2N$个变量，$2N+2N$个约束条件。</p>

<p>将支持向量回归的原问题转成对偶问题，可移除对$\tilde d$的依赖。</p>

<h2 id="section-3">支持向量回归的对偶模型</h2>

<p>通过拉格朗日乘子法的KKT条件${\partial\mathcal L\over\partial\mathbf w}=0$可得
\begin{equation}
\mathbf w = \sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)\mathbf z_n = \sum_{n=1}^N\beta_n\mathbf z_n，
\end{equation}
通过${\partial\mathcal L\over\partial b}=0$可得
\[
\sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)＝0，
\]
互补松弛条件（complementary slackness）为
\begin{equation}
\left\{
\begin{aligned}
\alpha_n^\wedge\left(\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\right)=&amp;0\\
\alpha_n^\vee\left(\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\right)=&amp;0。
\end{aligned}
\right.
\label{eq:complementary-slackness-svm-regession}
\end{equation}</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png" alt="QP原问题与对偶问题的对比" /></a><div class="caption">Figure 4:  QP原问题与对偶问题的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png">PNG</a>]</div></div></div>

<p>上图左上和左下分别表示soft-margin支持向量机的原问题和对偶问题的QP模型；上图右上和右下分别表示支持向量回归的原问题和对偶问题的QP模型。上图中，相同颜色的符号展示了如何从原问题变化到对偶问题。</p>

<p>当数据点位于tube中有$\left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert&lt;\epsilon$，不计误差，$\xi_n^\wedge=\xi_n^\vee=0$，根据互补松弛条件\eqref{eq:complementary-slackness-svm-regession}可知
\[
\left\{
\begin{aligned}
\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\neq &amp;0\\
\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\neq &amp;0，
\end{aligned}
\right.
\]
以及$\alpha_n^\wedge=\alpha_n^\vee＝0$，因此可得$\beta_n=0$。由此可知，支持向量回归问题中$\beta_n\neq 0$的支持向量刚好位于tube的边界上或在tube之外。</p>

<p>参考<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-complementary-slackness">soft-margin支持向量机</a>可得
\begin{equation}
b=
\left\{
\begin{aligned}
y_n-\mathbf w^T\mathbf z_n-\epsilon&amp;\quad(0&lt;\alpha_n^\wedge&lt;C)\\
y_n-\mathbf w^T\mathbf z_n+\epsilon&amp;\quad(0&lt;\alpha_n^\vee&lt;C)。
\end{aligned}
\right.
\end{equation}</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（5）：核logistic回归</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-logistic-regression" />
      <pubdate>2015-01-09T15:01:15+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png" alt="4种形式的支持向量机" /></a><div class="caption">Figure 1:  4种形式的支持向量机 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png">PNG</a>]</div></div></div>

<h2 id="section">支持向量机的正则化形式</h2>

<p>回顾<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">soft-margin支持向量机</a>，当违反边界的时候$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n + b\right)$，当没有违反边界的时候$\xi_n = 0$，边界违法的情况可以统一定义为$\xi_n=\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)$，于是无约束形式的soft-margin支持向量机为
\begin{equation}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)\right)，
\label{eq:uniform-soft-margin-svm}
\end{equation}
可以简写为
\[
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum\widehat{\mbox{err}}\right)。
\]
这是目标函数的正则化形式表示方法，soft-margin可以看作一种特殊的误差$\widehat{\mbox {err}}$度量。$L_2$正则化是对$\mathbf w$长度的约束
\[
\min\limits_{b,\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w + {1\over N}\sum\mbox{err}\right)。
\]</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png" alt="SVM与正则化" /></a><div class="caption">Figure 2:  SVM与正则化 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png">PNG</a>]</div></div></div>

<p>加入了大分类的边界限制，可以得到更少的分类情况，也可以通过$L_2$正则化实现。
从上图对比正则化方法可知，支持向量机可以看作为特殊的正则化方法。大的$C$，对应于小的$\lambda$，更弱的正则化。</p>

<h2 id="section-1">误差度量</h2>

<p>令线性项输出$s=\mathbf w^T\mathbf z_n + b$，感知器算法、支持向量机和logistic回归的误差度量为
\begin{equation}
\left\{
\begin{aligned}
err_{0/1}(s, y)&amp;=[[\mbox{sign}(ys)\neq 1]]\\
\widehat{err}_{SVM}(s, y)&amp;=\max(1-ys, 0)\\
err_{SCE}(s, y)&amp;=\log(1+\exp(-ys))。
\end{aligned}
\right.
\end{equation}</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png" alt="误差度量比较" /></a><div class="caption">Figure 3:  误差度量比较 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png">PNG</a>]</div></div></div>

<p>几种误差曲线对比如上图所示。其中，支持向量机的这种误差度量方式通常称为hinge error measure。支持向量机和logistic回归的误差是0/1误差的上界，对于分类问题，通过上界的最小化，间接做好0/1误差的最优化。从误差曲线还可以看出，支持向量机和$L_2$正则化的logistic的误差度量非常相似。这几种分类器的比较如下：</p>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>优化方法</th>
      <th>优势</th>
      <th>劣势</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>感知器算法</td>
      <td>最小化$err_{0/1}$</td>
      <td>线性可分时效率较高</td>
      <td>只适用线性可分，否则采用pocket算法</td>
    </tr>
    <tr>
      <td>soft-margin支持向量机</td>
      <td>QP最小化误差$err_{0/1}$</td>
      <td>容易优化，理论基础较好</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
    <tr>
      <td>正则化logistic回归</td>
      <td>GD／SGD最小化误差$err_{SCE}$</td>
      <td>容易优化，正则化控制模型</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
  </tbody>
</table>

<p>从比较可以看出，logistic回归是soft-margin支持向量机的近似。</p>

<h2 id="section-2">支持向量机的概率模型</h2>

<p>如何让支持向量机输出$[0,1]$之间的概率？</p>

<p>一种思路是将支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$作为logistic回归中线性判别部分的参数
$
g(\mathbf x) = \theta\left(\mathbf w_{SVM}^T\mathbf x + b_{SVM}\right)
$，
直接利用支持向量机和logistic回归。这在实际应用中表现尚佳，但丧失了logistic回归的优良特性（比如maxmum likehood）。</p>

<p>另一种思路将向支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$当作logistic的起始点，最终得到logistic回归模型。这和直接利用logistic结果差不多，还丧失了支持向量机核方法等优良特性。</p>

<p>如何融合支持向量机和logistic回归的优点？</p>

<p>组合logistic回归和支持向量机
\begin{equation}
g(\mathbf x) = \theta\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x) + b_{SVM}\right)+B\right)，
\label{eq:mixture-svm-logistic-model}
\end{equation}
这样既融合了支持向量机的核特性，又通过$A$和$B$两个自由度调节分类超平面适合最大似然。如果$A&gt;0$，表示支持向量机得到的$\mathbf w_{SVM}$较好；如果$B\approx 0$，表示支持向量机得到的$b_{SVM}$较好。新的logistic问题就变为了
\begin{equation}
\min_{A, B}{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}\right)+B\right)\right)\right)，
\end{equation}
该模型可以分为2阶段，首先利用支持向量机得到1维特征，然后采用简单的logistic模型，这称为<strong>支持向量机的Platt概率模型</strong>：</p>

<ol>
  <li>先在数据集上$\mathcal D$上利用支持向量机得到模型参数$\mathbf w_{SVM}$和$b_{SVM}$（或者$\boldsymbol\alpha$），再进行变换$\mathbf z’_n=\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}$；</li>
  <li>在$\{\left(\mathbf z’_n, y_n\right)\}_{n=1}^N$上得到logistic模型的参数$A,B$；</li>
  <li>将公式\eqref{eq:mixture-svm-logistic-model}的结果作为模型输出。</li>
</ol>

<p>从模型可以看出，这并不是严格的$\mathcal Z$空间logistic回归。</p>

<h2 id="logistic">核logistic回归</h2>

<p>最佳的$\mathbf w$是$\mathbf z_n$的线性组合，这是能使用核方法的关键。</p>

<p>从SGD来看，logistic回归的$\mathbf w$也是$\mathbf z_n$的线性组合
\[
\mathbf w_{LOGREG}＝\sum_{n=1}^N\left(\alpha_ny_n\right)\mathbf z_n。
\]</p>

<blockquote>
  <h4 id="representer-theorem">表示定理（representer theorem）</h4>
  <hr />
  <p>对任意的$L_2$正则化线性模型     <br />
\begin{equation*}
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^Nerr\left(y_n,\mathbf w^T\mathbf z_n\right)\right)，
\end{equation*} 
存在能用$\mathbf z_n$线性表示的最佳解$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$。</p>
</blockquote>

<p>任意$L_2$正则化的线性模型都能使用核方法。$L_2$正则化的logistic回归优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\mathbf w^T\mathbf z_n\right)\right)\right)。
\]
由上述定理可知，最佳$\mathbf w$一定是$\mathbf z_n$的线性组合。直接将用$\mathbf z_n$表示的$\mathbf w$代入上式，再用核方法表示，可以得到基于$L_2$正则化的核logistic回归优化模型
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\sum_{m=1}^N\beta_mK(\mathbf x_m,\mathbf x_n)\right)\right)\right)，
\end{equation}
这是无约束最优化问题，GD／SGD等都可求解。与支持向量机不同，KLR的大部分$\beta_n\neq 0$。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（4）：soft-margin支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-soft-margin-svm" />
      <pubdate>2015-01-07T17:24:31+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-soft-margin-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="soft-margin">soft-margin支持向量机</h2>

<p>在求解支持向量机参数的二次规划中，约束条件要求所有点被正确分类的叫<strong>hard-margin支持向量机</strong>。这类支持向量机过拟合原因：（1）特征转换功能太强；（2）坚持所有点都被正确分类。</p>

<p>可以通过放宽条件，容忍部分噪声（容许这部分噪声数据分类错误），使得支持向量机具有更好的泛化性能。</p>

<p>对于pocket PLA，目标函数为
\[
\min\limits_{b,\mathbf w}\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\]
结合<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqlinear-svm-model">线性支持向量机</a>，可以得到容忍错分的优化模型
\[
\begin{aligned}
\min\limits_{b,\mathbf w}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\infty\cdot\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\end{aligned}
\]
$C$是调节最大边界和噪声容忍度的参数。但是，上述模型不是QP，并且不能区别分类错误时误差的大小，进一步降模型变为<strong>soft-margin支持向量机</strong>
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\label{eq:soft-margin-primal-svm}
\end{equation}
该QP模型有$\tilde d+1+N$个变量和$2N$个约束条件，也被称为基于$\ell_1$损失的soft-margin。如果采用$\xi_n^2$，则被称为基于$\ell_2$损失的soft-margin
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n^2\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n，
\end{aligned}
\end{equation*}
此时不再需要约束条件$\xi_n\geq 0$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png" alt="边界容忍度" /></a><div class="caption">Figure 1:  边界容忍度 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png">PNG</a>]</div></div></div>

<p>$C$是调节最大边界和边界容忍度的参数，$\xi_n$是容忍误差的大小，如上图所示。$C$越小，对最大边界要求越高；$C$越大，能容忍的边界误差越小。</p>

<h2 id="soft-margin-1">对偶soft-margin支持向量机</h2>

<p>soft-margin支持向量机的拉格朗日函数为
\[
\begin{aligned}
\mathcal L(b,\mathbf w,\boldsymbol\alpha, \boldsymbol\beta)=
&amp;{1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
&amp;+\sum_{n=1}^N\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)+\sum_{n=1}^N\beta_n\left(-\xi_n\right)，
\end{aligned}
\]
根据${\partial\mathcal L\over\partial\xi_n}=0$可得$C-\alpha_n-\beta_n=0$，利用化解<a href="/2015/01/svm-dual-svm/#lagrange-dual-problem">拉格朗日对偶问题</a>相同的方法可得
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad 0\leq\alpha_n\leq C,\mbox{ for }n=1,2,\ldots,N \\
\mbox{implicitly}&amp;\quad \mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n\\
&amp;\quad\beta_n=C-\alpha_n,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-soft-margin-dual-svm}
\end{equation}
与hard-margin对偶支持向量机不同的地方只是$\alpha_n$多了一个上界$C$，这是$N$个变量$2N+1$个约束条件的QP。$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf 0_N\leq \mathbf I_N\boldsymbol\alpha \leq C\cdot \mathbf 1_N。
\end{equation}</p>

<h2 id="soft-margin-2">核soft-margin支持向量机</h2>

<p>soft-margin是实际中广泛应用的支持向量机。</p>

<p>soft-margin的支持向量机与<a href="/2015/01/svm-kernel-svm/#kernel-trick">hard-margin的支持向量机</a>基本相同，$\alpha_n$上界$C$的限制，导致$b$的计算不同。</p>

<p>利用complementary slackness条件可得
\begin{equation}
\begin{aligned}
\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=&amp;0\\
\left(C-\alpha_n\right)\xi_n=&amp;0，
\end{aligned}
\label{eq:soft-margin-complementary-slackness}
\end{equation}
当$\alpha_s&gt;0$时，$b=y_s-y_s\xi_s-\mathbf w^T\mathbf z_s$；当$\alpha_s&lt;C$时，$\xi_s=0$。满足$0&lt;\alpha_s&lt;C$的点称为<strong>自由支持向量</strong>$\left(\mathbf x_s, y_s\right)$，利用这些点容易得到
\begin{equation}
b=y_s-\sum\limits_{SV}\alpha_ny_nK\left(\mathbf x_n, \mathbf x_s\right)。
\end{equation}
在极少数情况下，不存在自由支持向量，$b$通过不等式限定，只要满足KKT条件的取值都是合理的$b$。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png" alt="soft-margin高斯核支持向量机" /></a><div class="caption">Figure 2:  soft-margin高斯核支持向量机 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png">PNG</a>]</div></div></div>

<p>上图展示了soft-margin高斯核支持向量机的效果，灰色的区域表示最大分类间隔。从图中可以看出，$C$越大，对误差的容忍越弱，越容易导致过拟合。</p>

<h2 id="alphan">$\alpha_n$的物理含义</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png" alt="不同类型的数据点" /></a><div class="caption">Figure 3:  不同类型的数据点 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png">PNG</a>]</div></div></div>

<p>通过公式\eqref{eq:soft-margin-complementary-slackness}可知，$\alpha_n$将数据点分为如上图所示的3种类型：</p>

<ol>
  <li>当$\alpha_n=0$时，非支持向量，$\xi_n=0$，位于边界之外，极少数可能在边界上；</li>
  <li>当$0&lt;\alpha_n&lt;C$时，自由（free）支持向量$\square$，$\xi_n=0$，位于边界上，用于计算$b$；</li>
  <li>当$\alpha_n=C$时，有界（bounded）支持向量$\triangle$，$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n+b\right)$，落在边界内，可能正确分类也可能分错，极少数可能在边界上。</li>
</ol>

<h2 id="section">模型选择</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png" alt="［中］：交叉验证误差；［右］：支持向量个数" /></a><div class="caption">Figure 4:  ［中］：交叉验证误差；［右］：支持向量个数 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png">PNG</a>]</div></div></div>

<p>上图左是soft-margin高斯核支持向量机的分类效果，横轴是$C$的变化，纵轴是$\gamma$的变化。由于$E_{cv}(C,\gamma)$不光滑，通常的模型选择方法是通过$C$和$\gamma$的数据网格，利用交叉验证的方法选择合适的模型，上图中所示，选择了左下角的模型。</p>

<p>交叉验证中，将数据分为$N$份的验证称为<strong>leave-one-out交叉验证</strong>，它的误差上界是
\begin{equation}
E_{loocv}\leq\frac{\#SV}{N}，
\label{eq:eloocv-upper-bound}
\end{equation}
$\#SV$表示支持向量的个数。可以通过支持向量的个数进行模型选择。由于支持向量个数的函数也是非光滑的，难以优化，也采取利用$C$和$\gamma$的数据网格，多次计算后做选择。</p>

<p>由于\eqref{eq:eloocv-upper-bound}也只是给出了$E_{loocv}$的上界，通常用于当$E_{cv}$计算量很大时模型的安全检查，剔除那些支持向量过多的危险模型，然后再在剩余模型中进一步做交叉验证选择合适的模型。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（3）：核支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-svm" />
      <pubdate>2015-01-06T14:25:58+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">回顾对偶支持向量机</h2>

<p>当特征空间维数$\tilde d$很大时，计算$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$是对偶支持向量机的求解瓶颈。</p>

<p>能否找到比$O(\tilde d)$快的方法计算$\mathbf z_n^T\mathbf z_m=\Phi(\mathbf x_n)^T\Phi(\mathbf x_m)$？能否将先特征转换再计算内积的两步合为一步呢？</p>

<h2 id="kernel-trick">核技巧</h2>

<p>对于2阶多项式变换
\[
\Phi_2(\mathbf x)=\left(1, x_1,x_2,\ldots,x_d,x_1^2,x_1x_2,\ldots,x_1x_d,x_2x_1,x_2^2,\ldots,x_2x_d,\ldots,x_d^2\right)，
\]
为了简化同时包含了$x_1x_2$和$x_2x_1$这样的项。变换之后$Z$空间的内积可以可直接通过$X$空间计算
\[
\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)=1+\left(\mathbf x^T\mathbf x’\right)+\left(\mathbf x^T\mathbf x’\right)^2。
\]
这种特征转换和内积合并的方法称之为<strong>核函数</strong>，
\begin{equation*}
K_{\Phi_2}\left(\mathbf x,\mathbf x’\right)=\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)。
\end{equation*}
利用核函数，可以简化对偶支持向量机的实现，二次项的系数为
\begin{equation}
q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m=y_ny_mK\left(\mathbf x_n,\mathbf x_m\right)，
\end{equation}
利用支持向量$\left(\mathbf x_s, y_s\right)$计算偏移量
\begin{equation}
\begin{aligned}
b
=&amp;y_s-\mathbf w^T\mathbf z_s\\
=&amp;y_s-\left(\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right)^T\mathbf z_s\\
=&amp;y_s-\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x_s\right)
，
\end{aligned}
\end{equation}
对于特定的输入$\mathbf x$，判别函数为
\begin{equation}
\begin{aligned}
g_{SVM}(\mathbf x)
=&amp;\mbox{sign}\left(\mathbf w^T\Phi(\mathbf x)+b\right)\\
=&amp;\mbox{sign}\left(\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x\right)+b\right)
。
\end{aligned}
\end{equation}
从上面的公式可以看出，计算不再依赖变换后的空间，只依赖原空间，$b$和判别函数只依赖原空间的支持向量，大大简化了计算。</p>

<h2 id="section-1">多项式核</h2>

<p>仿照2阶多项式核的定义，可以推导高阶多项式核的定义为
\begin{equation}
K_Q(\mathbf x,\mathbf x’)=\left(\zeta + \gamma\mathbf x^T\mathbf x’\right)^Q\quad\zeta\geq 0,\gamma &gt; 0。
\end{equation}
事实上，系数$\zeta$和$\gamma$的取值不会改变多项式所在的空间，这些系数会被$\mathbf w$所吞噬。但是，不同的系数会得到不同的支持向量和判别函数。选择不同的核，相当于改变了边界的定义。多项式核可以在几乎不增加计算量的情况下，得到复杂的判别界。支持向量机通过large-margin控制判别界的复杂度。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png" alt="2阶多项式核SVM的效果" /></a><div class="caption">Figure 1:  2阶多项式核SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是2阶多项式核支持向量机的效果，不同系数下的支持向量和判别界不同。</p>

<p>当$\zeta=0,\gamma=1$时，多项式核就变为了线性核。线性核利用原始支持向量机就比较高效。因此，应当首先尝试线性核，当线性核不能满足要求时再尝试其它高阶核。</p>

<h2 id="section-2">高斯核</h2>

<p>对于1维数据的高斯核，利用Taylor展式可得
\begin{equation*}
\begin{aligned}
K(x,x’)
=&amp;\exp\left(-\left(x-x’\right)^2\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\exp\left(2xx’\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\sum_{i=0}^\infty\frac{\left(2xx’\right)^i}{i!}\\
=&amp;\sum_{i=0}^\infty\exp\left(-x^2\right)\exp\left(-x’^2\right)\sqrt{\frac{2^i}{i!}}\sqrt{\frac{2^i}{i!}}x^ix’^i\\
=&amp;\Phi(x)^T\Phi(x’)，
\end{aligned}
\end{equation*}
其中$\Phi(x)=\exp\left(-x^2\right)\cdot\left(1,\sqrt{\frac{2^1}{1!}}x,\sqrt{\frac{2^2}{2!}}x^2,\ldots\right)$，容易看出这是无穷维的特征变换。更一般的高斯核定义为
\begin{equation}
K\left(\mathbf x,\mathbf x’\right)=\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x’\right\rVert^2\right)\quad \gamma&gt;0。
\end{equation}
高斯核也成为径向基函数（RBF，Radial Basis Function）核。基于高斯核的判别函数为
\begin{equation}
g_{SVM}(\mathbf x)=\mbox{sign}\left(\sum_{SV}\alpha_ny_n\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_n\right\rVert^2\right)+b\right)，
\end{equation}
它是以支持向量为中心的高斯函数的线性组合，不再依赖$\mathbf w$，只依赖于支持向量和系数$\alpha_n$。</p>

<p>高斯核相当于进行了无限维的特征转换，可以得到复杂的判别界，泛化性能通过最大边界“保证”。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png" alt="高斯核的SVM的效果" /></a><div class="caption">Figure 2:  高斯核的SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是不同系数的高斯核支持向量机的效果，$\gamma$越大，高斯函数越尖，越容易过拟合。当$\gamma\rightarrow\infty$时，$K_{lim}\left(\mathbf x,\mathbf x’\right)=[[\mathbf x＝\mathbf x’]]$，相当于严格判断是否与支持向量一致。</p>

<h2 id="section-3">小结</h2>

<p>各种核函数的比较如下表：</p>

<table>
  <thead>
    <tr>
      <th>核函数</th>
      <th>优势</th>
      <th>局限性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>线性核</td>
      <td>计算快；<br />通过$\mathbf w$和支持向量容易解释</td>
      <td>无法处理线性不可分数据</td>
    </tr>
    <tr>
      <td>多项式核</td>
      <td>可以通过次数$Q$控制复杂度</td>
      <td>当$Q$很大时数值计算困难<br />（当$\left\lvert\zeta+\gamma\mathbf x^T\mathbf x’\right\rvert&lt;1$时，$K(\mathbf x, \mathbf x’)\rightarrow 0$）；<br />3个参数难以选择</td>
    </tr>
    <tr>
      <td>高斯核</td>
      <td>强大；<br />$K(\mathbf x, \mathbf x’)$有界；<br />1个参数难以选择；</td>
      <td>没有显示的$\mathbf w$供解读；<br />计算比线性核慢；<br />太强大导致容易过拟合</td>
    </tr>
  </tbody>
</table>

<p>当采用多项式核时，如果$Q$较小，可以尝试直接利用特征变换和原始支持向量机，求解速度可能比核方法更快。</p>

<p>核是一种特殊的相似性度量，但是不是所有的相似性度量都可以作为核。有效的核必须满足<strong>Mercer条件</strong>（充要条件）：核函数矩阵
\begin{equation*}
\begin{aligned}
\mathbf K
=&amp;\begin{bmatrix}
\Phi(\mathbf x_1)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_1)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_1)^T\Phi(\mathbf x_N)\\
\Phi(\mathbf x_2)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_2)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_2)^T\Phi(\mathbf x_N)\\
\ldots&amp;\ldots&amp;\ldots&amp;\ldots\\
\Phi(\mathbf x_N)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_N)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_N)^T\Phi(\mathbf x_N)
\end{bmatrix}\\
=&amp;\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]^T\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]\\
=&amp;\mathbf Z\mathbf Z^T
\end{aligned}
\end{equation*}
必须是对称半正定。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（2）：对偶支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-dual-svm" />
      <pubdate>2015-01-05T18:24:26+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-dual-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">对偶支持向量机的价值</h2>

<p>对于求解非线性支持向量机系数的QP，有$\tilde d+1$个变量和$N$个约束条件，对于经过非线性特征变换后$Z$空间的特征维数$\tilde d$一般很大。当$\tilde d$很大时，求解比较具有挑战性。</p>

<p>对偶支持向量机是支持向量机的另一种形式，它不依赖$\tilde d$，有$N$个变量和$N+1$个约束条件。</p>

<h2 id="section-1">拉格朗日乘子法</h2>

<p>正则化采用带约束的最小化方法
\[
\min\limits_{\mathbf w} E_{in}(\mathbf w)\mbox{ s.t. }\mathbf w^T\mathbf w \leq C，
\]
可以通过如下等价的拉格朗日乘子法实现
\[
\min\limits_{\mathbf w} E_{aug}(\mathbf w)=E_{in}(\mathbf w)+{\lambda\over N}\mathbf w^T\mathbf w。
\]
正则化通过将$\lambda$作为<strong>确定值</strong>，代替$C$作为约束条件，求解更容易。对偶支持向量机不同于正则化，它将$N$个$\lambda$视为<strong>变量</strong>求解。</p>

<p>当$\boldsymbol\alpha_n$为拉格朗日乘子时，求解支持向量机参数的拉格朗日函数为
\begin{equation}
\mathcal L(b, \mathbf w, \boldsymbol \alpha) = {1\over 2}\mathbf w^T\mathbf w + \sum_{n=1}^N\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)。
\end{equation}</p>

<h2 id="lagrange-dual-problem">拉格朗日对偶问题</h2>

<p>根据拉格朗日函数，可得
\begin{equation}
\begin{aligned}
\mbox{SVM}\equiv &amp;\min\limits_{b,\mathbf w}\max\limits_{\mbox{all }\alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\\
=&amp;\min\limits_{b,\mathbf w}\left(\infty\mbox{ if violating };{1\over 2}\mathbf w^T\mathbf w\mbox{ if feasible}\right)。
\end{aligned}
\end{equation}
根据$b$和$\mathbf w$取值的划分，分两种情况解释上述公式：</p>

<ol>
  <li>任意violating的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{some positive})\right)\rightarrow\infty$；</li>
  <li>任意feasible的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{all non-positive})\right)=\square$。</li>
</ol>

<p>对$\alpha’_n\geq 0$的任意$\boldsymbol\alpha’$可得
\[
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha’)，
\]
于是有
\begin{equation}
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\max\limits_{\mbox{all } \alpha_n\geq 0}\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha)。
\end{equation}</p>

<p>若果解决了对偶问题，就得到原问题的下界。上式右边<strong>将对$b$和$\mathbf w$的优化问题转化成了对$\alpha_n$的优化问题</strong>，同时内层是对$b$和$\mathbf w$无约束条件的最优化，方便求解。</p>

<p>如果上式中只是“$\geq$”，则表示弱对偶（weak duality）；如果上式中“$=$”成立，则为强对偶（strong duality）。对于QP，“$=$”成立的条件是：（1）原问题是凸的；（2）原问题有解（对本问题而言，数据是可分的）；（3）线性约束条件。</p>

<p>对于支持向量机，“$=$”成立，求解右边的问题即可。</p>

<h2 id="section-2">化简拉格朗日对偶问题</h2>

<p>对于对偶问题，内层无约束优化取得最小值的条件是
\[
{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial b} = 0;\quad{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial w_i} = 0，
\]
也就是
\[
\sum_{n=1}^N\alpha_ny_n=0;\quad\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\]
将上述变量带入对偶问题，可以化解为
\[
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\min\limits_{b,\mathbf w}\left(\frac{1}{2}\mathbf w^T\mathbf w+\sum_{n=1}^N\alpha_n-\mathbf w^T\mathbf w\right)，
\]
继续将$\mathbf w$的取值带入，可得
\begin{equation*}
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\left(-\frac{1}{2}\left\lVert\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right\rVert^2+\sum_{n=1}^N\alpha_n\right)，
\end{equation*}
化为二次规划的标准形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm}
\end{equation}</p>

<p>该优化问题有$N$个变量，$N+1$个约束条件。根据<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqqp-standard-format">QP的标准形式</a>，可以得到利用QP求解对偶二次规划的系数
\[
\begin{aligned}
&amp;q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m;\quad\mathbf p=-\mathbf 1_N;\\
&amp;\mathbf a_{\geq}=\mathbf y,c_{\geq}=0;\quad\mathbf a_{\leq}=-\mathbf y,c_{\leq}=0;\\
&amp;\mathbf a_n^T=\mbox{n-th unit direction},c_n=0。
\end{aligned}
\]
为了利用标准的QP，将“$=$”约束转换成了“$\geq$”和“$\leq$”约束。通常情况$q_{n,m}\neq 0$，系数对应着$N\times N$的非稀疏矩阵，$N$很大时需要占用很大的存储空间。因此，通常会采用针对支持向量机设计的特殊QP加速求解过程。</p>

<p>对偶支持向量机优化问题的矩阵形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\boldsymbol\alpha^T\mathbf Q_D\boldsymbol\alpha-\mathbf 1^T\boldsymbol\alpha \\
\mbox{subject to}&amp;\quad\mathbf y^T\boldsymbol\alpha=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm2}
\end{equation}
其中$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$，$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf I_N\boldsymbol\alpha \geq \mathbf 0_N。
\end{equation}</p>

<h2 id="kkt">KKT条件</h2>

<p>原问题和对偶问题都是最佳解需要$b,\mathbf w,\boldsymbol\alpha$之间满足如下条件：</p>

<ol>
  <li>原问题可行：$y_n(\mathbf w^T\mathbf z_n+b)\geq 1$；</li>
  <li>对偶问题可行：$\alpha_n\geq 0$；</li>
  <li>对偶问题内最优化：$\sum_{n=1}^N\alpha_ny_n=0;\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n$；</li>
  <li>原问题内最优化：$\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=0$，这个条件也称为complementary slackness，其中至少一项为$0$。</li>
</ol>

<p>这称为KKT条件，它是原问题和对偶问题都是最佳解的必要（necessary）条件，此处也是充分（sufficient）条件。</p>

<blockquote>
  <h4 id="example">Example</h4>
  <hr />
  <p>For a single variable $w$, consider minimizing ${1\over 2}w^2$ subject to two linear constraints $w\geq 1$ and $w\leq 3$. We know that the Lagrange function $\mathcal L(w,\alpha)={1\over 2}w^2+\alpha_1(1-w)+\alpha_2(w-3)$. Which of the following equations that contain $\alpha$ are among the KKT conditions of the optimization problem?</p>

  <ol>
    <li>$\alpha_1\geq 0$ and $\alpha_2\geq 0$</li>
    <li>$w=\alpha_1-\alpha_2$</li>
    <li>$\alpha_1(1-w)=0$ and $\alpha_2(w-3)=0$ </li>
    <li>all of the above</li>
  </ol>

  <p>Answer：4</p>
</blockquote>

<p>通过KKT条件，可以利用$\boldsymbol\alpha$求解$b$和$\mathbf w$。利用KKT条件3容易求解$\mathbf w$，利用KKT条件4，当$\alpha_n&gt;0$时，$1-y_n\left(\mathbf w^T\mathbf z_n+b\right)=0$两边同时乘以$y_n$可得$b$，
\begin{equation}
\left\{
\begin{aligned}
b=&amp;y_n-\mathbf w^T\mathbf z_n \quad\mbox{if }\alpha_n\neq 0；\\
\mathbf w=&amp;\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\end{aligned}
\right.
\end{equation}
利用不同$\alpha_n&gt;0$时的数据，理论上求解到的$b$应该是一样的，可以计算多个$b$然后平均得到更稳定的解。</p>

<h2 id="section-3">支持向量</h2>

<p>$\alpha_n&gt;0$对应的点一定在边界上，这些点称为<strong>支持向量</strong>。也有些在边界上的点，对应的$\alpha_n$不一定大于$0$。容易发现，计算$b$和$\mathbf w$只需要支持向量就够了。</p>

<p>从计算公式可以发现，$\mathbf w$可以由$y_n\mathbf z_n$的线性组合表示，也就是$\mathbf w$可由数据表示，这和PLA算法相似
\begin{equation*}
\mathbf w_{SVM}=\sum_{n=1}^N\alpha_n\left(y_n\mathbf z_n\right)，
\mathbf w_{PLA}=\sum_{n=1}^N\beta_n\left(y_n\mathbf z_n\right)，
\end{equation*}
其中$\beta_n$表示犯错误的次数。</p>

<h2 id="section-4">两种形式的支持向量机</h2>

<p>原始支持向量机适合特征维数$\tilde d$较少的情形，对偶形式的支持向量机适合数据点$N$较少的情形，两者都是通过最优化找到最大边界的判别界。</p>

<p>事实上，对偶形式的支持向量机和特征维数$\tilde d$也有关系，$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$的计算也是$\mathbb R^{\tilde d}$空间的内积。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（1）：线性支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-linear-svm" />
      <pubdate>2015-01-03T18:57:13+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-linear-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">最佳判别界</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png" alt="最佳判别界" /></a><div class="caption">Figure 1:  最佳判别界 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png">PNG</a>]</div></div></div>

<p>对相同的数据集，PLA可能得到不同的判别界，如上图所示，那条判别界最好呢？</p>

<ul>
  <li>测量是有误差的，能容忍误差越大的分界线越好，如上图上排所示，灰色的圆半径越大表示对误差的容忍度越大。噪声是导致过拟合的主要原因，对误差的容忍度越好，过拟合的可能性越低。</li>
  <li>判别界能膨胀得越胖越鲁棒，如上图下排所示。</li>
</ul>

<p>根据上两条标准，最右的判别界最佳。判别界的胖瘦程度称为边界（margin），最大边界的判别界最佳，也就是离判别界距离最近的点离判别界距离越大越好，</p>

<p>\begin{equation*}
\begin{aligned}
\max\limits_{\mathbf w} &amp;\quad\mbox{margin}(\mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\mathbf w^T\mathbf x_n &gt; 0\\
&amp;\quad\mbox{margin}(\mathbf w)=\min\limits_{n=1,\ldots,N}\mbox{distance}(\mathbf x_n, \mathbf w)。
\end{aligned}
\end{equation*}</p>

<p>$y_n\mathbf w^T\mathbf x_n &gt; 0$表示点被正确的分类，$\mbox{margin}(\mathbf w)$的定义可以保证判别界在两类的“中间”，不会偏向任何一边。</p>

<h2 id="section-1">支持向量机</h2>

<p>与线性感知器的定义不同，定义$b= w_0, \mathbf w=\left[ w_1, \ldots,  w_d\right]^T$，$\mathbf x=\left[ x_1, \ldots,  x_d\right]^T$。点到判别界的距离可表示为
\begin{equation*}
\mbox{distance}(\mathbf x, b, \mathbf w)=\frac{1}{\lVert\mathbf w\rVert}\left\lvert\mathbf w^T\mathbf x + b\right\rvert，
\end{equation*}
因此可得最大间隔
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad\mbox{margin}(b, \mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 0\\
&amp;\quad\mbox{margin}(b, \mathbf w)=\min\limits_{n=1,\ldots,N}\frac{1}{\lVert\mathbf w\rVert}y_n\left(\mathbf w^T\mathbf x_n + b\right)。
\end{aligned}
\end{equation*}
通过系数缩放，总可以做到$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$，最大间隔问题可以转化为等价问题
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad{1\over\lVert\mathbf w\rVert}\\
\mbox{subject to}&amp; \min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1。
\end{aligned}
\end{equation*}
上述条件$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$可以用等价的条件$y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1$代替。如果仅仅是$y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 1$而无法取“$=$”，那么不等式两边可以除以大于$1$的系数，使得条件仍然成立，$\mathbf w$除以大于$1$的系数后，${1\over\lVert\mathbf w\rVert}$会更大。因此，标准形式的最优化是
\begin{equation}
\begin{aligned}
\min\limits_{b, \mathbf w} &amp;\quad{1\over 2}\mathbf w^T\mathbf w\\
\mbox{subject to}&amp; \quad y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1\mbox{ for all }n。
\end{aligned}
\label{eq:linear-svm-model}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png" alt="支持向量" /></a><div class="caption">Figure 2:  支持向量 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png">PNG</a>]</div></div></div>

<p>通过求解可以发现，只有部分点对解有作用，如上图边界上方框内的点，这些点称为<strong>候选</strong>支持向量（support vector）。支持向量机就是利用支持向量学习“最胖”判别界。</p>

<h2 id="section-2">二次规划求解</h2>

<p>求解最佳判别界的是一个二次规划问题（QP，quadratic programming）。二次规划的标准形式为
\begin{equation}
\begin{aligned}
\mbox{optimal} &amp;\quad\mathbf u\leftarrow QP(\mathbf Q, \mathbf p, \mathbf A, \mathbf c) \\
\min\limits_{\mathbf u} &amp;\quad{1\over 2}\mathbf u^T\mathbf Q\mathbf u + \mathbf p^T\mathbf u\\
\mbox{subject to}&amp;\quad\mathbf a_n^T\mathbf u\geq c_n\mbox{ for }n=1,2,\ldots,N。
\end{aligned}
\label{eq:qp-standard-format}
\end{equation}
支持向量机利用二次规划求解时参数为
\begin{equation*}
\mathbf u =<br />
\begin{bmatrix}
b\\
\mathbf w
\end{bmatrix}
，
\end{equation*}
目标函数系数为
\begin{equation*}
\mathbf Q = 
\begin{bmatrix}
0 &amp;\mathbf 0_d^T\\
\mathbf 0_d &amp;\mathbf I_d
\end{bmatrix};
\mathbf p=\mathbf 0_{d+1}
，
\end{equation*}
约束条件系数为
\begin{equation*}
\mathbf a_n^T=y_n\left[1\quad\mathbf x_n^T\right];c_n=1。
\end{equation*}</p>

<p>这里所讲的支持向量机叫做线性hard-margin支持向量机，其中hard-margin是指所有的点能被正确分类。</p>

<p>利用$\mathbf z_n=\Phi\left(\mathbf x_n\right)$，用$\mathbf z_n$替代$\mathbf x_n$，可以得到非线性的支持向量机。</p>

<h2 id="section-3">最大边界的价值</h2>

<p>正则化（regularization）通过约束条件$\mathbf w^T\mathbf w\leq C$最小化$E_{in}$，支持向量机通过$E_{in}=0$等约束条件最小化$\mathbf w^T\mathbf w$。因此，支持向量机也可以看作是一种<strong>特殊的正则化方法</strong>。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png" alt="无法打碎3个点的情况" /></a><div class="caption">Figure 3:  无法打碎3个点的情况 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png">PNG</a>]</div></div></div>

<p>对于最大边界算法$\mathcal A_{\rho}$，脚标表示分界线的宽度要大于$\rho$。当$\rho = 0$时就是PLA，它可以打碎2维平面的3个输入点；若$\rho＝1.126$，如上图所示，无法打碎3个点。</p>

<p>如果加入了$\rho$条件的限制，可能的二分类情况少了，可认为<strong>VC维更小了，有更好的泛化性能</strong>。对于算法的VC维，有数据依赖的VC维比没有依赖的要低，$d_{VC}\left(\mathcal A_\rho\right)&lt;d_{VC}(\mathcal H)$，有数据依赖意味着加入了跟多的约束条件。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png" alt="单位圆上的数据" /></a><div class="caption">Figure 4:  单位圆上的数据 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png">PNG</a>]</div></div></div>

<p>当$\mathcal X$是上图所示半径为$R$的单位圆上点时，
\begin{equation}
d_{VC}\left(\mathcal A_\rho\right)\leq\min\left({R^2\over \rho^2},d\right)+1\leq d+1。
\end{equation}</p>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png" alt="各种判别界的对比" /></a><div class="caption">Figure 5:  各种判别界的对比 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png">PNG</a>]</div></div></div>

<p>几种情况的判别界对比如上图所示，large-margin的判别界简单且数量较少，有特征变换$\Phi$的判别界较复杂但是数量较多。</p>

<p>较少的判别界对$d_{VC}$和泛化有利，复杂的判别界可能得到更好的$E_{in}$。非线性支持向量机同时具备大边界的判别界和采用多种特征变换$\Phi$，因此判别界较少而且比较复杂，兼顾了这两方面的优点。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：噪声与误差</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-noise-and-error" />
      <pubdate>2014-12-27T16:56:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-noise-and-error</guid>
      <content:encoded>&lt;![CDATA[<p>如果数据存在噪声，对机器学习可学习理论推导有何影响？</p>

<h2 id="section">噪声起源</h2>

<p>对于$f:\mathcal X\rightarrow \mathcal Y$，噪声即可能来源于$\mathcal X$，也可能来源于$\mathcal Y$。以信用卡申办信息为例说明（$\mathcal X$为客户资料，$\mathcal Y$为是否办理信用卡）：</p>

<ul>
  <li>$y$的噪声：良好纪录的用户，被标记为Bad；</li>
  <li>$y$的噪声：同样资料的用户，标记却不同（不同专家评价有差异）；</li>
  <li>$\mathbf x$的噪声：不准确的客户资料。</li>
</ul>

<h2 id="target-distribution">目标分布<sup id="fnref:target-distribution"><a href="#fn:target-distribution" class="footnote">1</a></sup></h2>

<p>由于噪声的存在，弹珠模型的弹珠不再是确定的颜色，而是会随时变色，但是通过记录抽出瞬间弹珠的颜色，仍可以对罐中弹珠颜色概率做估计。$y=f(\mathbf x) + \mbox{noise}$，$y$用<strong>目标分布</strong>表示
\begin{equation}
y\sim P(y|\mathbf x)。
\end{equation}
对于满足$(\mathbf x,y)\sim P(\mathbf x,y)$的噪声数据，VC界仍成立，通过$E_{in}$仍然可以估计$E_{out}$。
如果$P(\circ|\mathbf x)=0.7$，$P(\times|\mathbf x)=0.3$，<strong>理想最小目标函数</strong>（ideal mini-target）为$f(\mathbf x)=\circ$，在最佳选择下噪声水平是$0.3$。对于确定的（无噪声）的目标函数$f$，也可用目标分布（target distribution）表示
\[
P(y|\mathbf x)=
\left\{
\begin{aligned}
1&amp;\quad\mbox{for }y=f(\mathbf x)\\
0&amp;\quad\mbox{for }y\neq f(\mathbf x)。
\end{aligned}
\right.
\]</p>

<p>机器学习的目标就是预测常见数据（w.r.t. $P(\mathbf x)$）的理想最小目标函数（w.r.t. $P(y|\mathbf x)$）。</p>

<p>由于噪声的存在，并且$\mathcal D$也是通过采样得到的，从数据$\mathcal D$无法得出目标函数$f$是线性还是非线性等特征。</p>

<h2 id="section-1">误差度量</h2>

<p>回顾out-of-sample误差度量\[E_{out} = \varepsilon_{\mathbf x\sim P}[[g(\mathbf x)\neq f(\mathbf x)]]，\]它是基于每个点的误差度量（pointwise error measure），$[[\mbox{prediction}\neq\mbox{targe}]]$度量的分类错误称为<strong>0/1错误</strong>。基于每个点的in-sample误差度量为\[E_{in} = {1\over N}\sum_{n=1}^Nerr\left(g\left(\mathbf x_n\right),f\left(\mathbf x_n\right)\right)。\]</p>

<p>令$\tilde y=g(\mathbf x)，y=f(\mathbf x)$，0/1误差和<strong>平方误差</strong>分别定义为\[err(\tilde y,y)=[[\tilde y\neq y]]\qquad err(\tilde y,y)=(\tilde y- y)^2。\]</p>

<p>误差是如何影响学习算法的呢？</p>

<p>下面示例展示了不同度量方式下误差的差异：</p>

<p><img src="/assets/images/2014-12-27-machine-learning-noise-and-error-ideal-mini-target-example.png" alt="误差度量的影响" /></p>

<p>$P(y|\mathbf x)$和$err$确定了理想最小目标函数$f(\mathbf x)$。在0/1误差和平方误差度量方式下，理想最小目标函数分别为
\[
f(\mathbf x)=\arg\max_{y\in\mathcal Y}P(y|\mathbf x)\qquad f(\mathbf x)=\sum_{y\in\mathcal Y}yP(y|\mathbf x)。
\]
若采用平方误差度度量方式，上例中$f(\mathbf x)=1.9$。</p>

<p>根据具体应用场景，选择相应的错误（误差）衡量方法，对机器学习至关重要。</p>

<p>VC理论的推导不一定依赖于目标函数$f(\mathbf x)$，只需目标分布。VC理论对很多假设集$\mathcal H$和误差衡量$err$都有效，比如将定义稍作修改可将VC理论推广到回归分析可得到类似的结果。</p>

<h2 id="section-2">误差度量范例</h2>

<p>以指纹识别为例，介绍误差度量。$+1$表示合法用户，$-1$表示入侵者。指纹识别犯的两种错误：</p>

<ol>
  <li>false reject：合法用户识别为非法；</li>
  <li>false accept：非法用户识别为合法。</li>
</ol>

<p>在不同的应用场景下，两种错误导致的损失不一样。在设计算法的时候，须考虑将误差的度量方式。设计算法的时候，不仅要考虑误差定义的合理性，还要考虑算法$\mathcal A$是否容易优化（比如：闭式解或者凸目标函数）。误差的度量是算法的关键。</p>

<p id="class-weighted-error">对于错分的不同损失，可根据犯错误所属的不同类别定义如下的加权误差度量方式</p>
<p>\[
E_{in}^w(h)={1\over N}\sum_{n=1}^N
\left\{
\begin{aligned}
&amp;1 &amp;\quad\mbox{if }y_n=+1\\
&amp;1000 &amp;\quad\mbox{if }y_n=-1
\end{aligned}
\right\}
\cdot \left[\left[y_n\neq h(\mathbf x_n)\right]\right]。
\]
对于线性可分数据的PLA，这种定义并无影响。对于pocket算法，假设将$-1$样本复制$1000$倍，在新的数据上仍用$E_{in}^{0/1}$度量误差，就和在原数据上用$E_{in}^{w}$度量一致。</p>

<p>实际使用中，不会真的复制数据，采取虚拟复制（virtual copying）的策略。</p>

<blockquote>
  <h4 id="pocket">加权pocket算法</h4>
  <hr />

  <ol>
    <li>以$1000$倍的概率检查$-1$样本犯的错误；       </li>
    <li>当$\mathbf w_{t+1}$犯的错误$E_{in}^w$比$\hat{\mathbf w}$小时，用$\hat{\mathbf w}$更新$\mathbf w_{t+1}$。</li>
  </ol>
</blockquote>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-4">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:target-distribution">
      <p>不太明白这节的意思…… <a href="#fnref:target-distribution" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
