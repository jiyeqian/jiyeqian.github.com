<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-04-03T20:45:15+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>向量空间模型</title>
      <link href="http://qianjiye.de/2015/04/vector-space-model" />
      <pubdate>2015-04-03T19:40:53+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/04/vector-space-model</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">基本概念</h2>

<p><strong>向量空间模型</strong>（VSM，vector space model）是用向量表示文本的代数模型，它将文本转换为向量，也称为<strong>词语向量模型</strong>（term vector model）<a href="#Salton:1975:VSM:361219.361220">[1]</a><sup id="fnref:tfidf"><a href="#fn:tfidf" class="footnote">1</a></sup>。最容易想到的方法就是利用词频（TF，term frequency）。</p>

<h3 id="section-1">词频</h3>

<p><strong>词频</strong>就是统计词语在文本中出现的次数。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">mydoclist</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;Julie loves me more than Linda loves me&#39;</span><span class="p">,</span>
<span class="s">&#39;Jane likes me more than Julie loves me&#39;</span><span class="p">,</span>
<span class="s">&#39;He likes basketball more than baseball&#39;</span><span class="p">]</span>

<span class="c">#mydoclist = [&#39;sun sky bright&#39;, &#39;sun sun bright&#39;]</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mydoclist</span><span class="p">:</span>
    <span class="n">tf</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
        <span class="n">tf</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span>
    <span class="k">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    
<span class="c"># Output:</span>
<span class="c"># [(&#39;me&#39;, 2), (&#39;Julie&#39;, 1), (&#39;loves&#39;, 2), (&#39;Linda&#39;, 1), (&#39;than&#39;, 1), (&#39;more&#39;, 1)]</span>
<span class="c"># [(&#39;me&#39;, 2), (&#39;Julie&#39;, 1), (&#39;likes&#39;, 1), (&#39;loves&#39;, 1), (&#39;Jane&#39;, 1), (&#39;than&#39;, 1), (&#39;more&#39;, 1)]</span>
<span class="c"># [(&#39;basketball&#39;, 1), (&#39;baseball&#39;, 1), (&#39;likes&#39;, 1), (&#39;He&#39;, 1), (&#39;than&#39;, 1), (&#39;more&#39;, 1)]</span></code></pre></div>

<p>以上代码统计了每个文本中词语出现的频率。虽然将文本量化，但是由于生成每个文本的字典不同，文本之间无法比较。因此，需要将每个文本表示成长度相同的向量，这个长度是由全体词语构成的<strong>语料库</strong>（corpus）决定的。</p>

<div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">string</span> <span class="c">#allows for format()</span>
    
<span class="k">def</span> <span class="nf">build_lexicon</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="n">lexicon</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="n">lexicon</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">lexicon</span>

<span class="k">def</span> <span class="nf">tf</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">document</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">freq</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">document</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">freq</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">document</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">document</span><span class="o">.</span><span class="n">split</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>

<span class="n">vocabulary</span> <span class="o">=</span> <span class="n">build_lexicon</span><span class="p">(</span><span class="n">mydoclist</span><span class="p">)</span>

<span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">print</span> <span class="s">&#39;Our vocabulary vector is [&#39;</span> <span class="o">+</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span> <span class="o">+</span> <span class="s">&#39;]&#39;</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mydoclist</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">&#39;The doc is &quot;&#39;</span> <span class="o">+</span> <span class="n">doc</span> <span class="o">+</span> <span class="s">&#39;&quot;&#39;</span>
    <span class="n">tf_vector</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">]</span>
    <span class="n">tf_vector_string</span> <span class="o">=</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">format</span><span class="p">(</span><span class="n">freq</span><span class="p">,</span> <span class="s">&#39;d&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">tf_vector</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&#39;The tf vector for Document </span><span class="si">%d</span><span class="s"> is [</span><span class="si">%s</span><span class="s">]&#39;</span> <span class="o">%</span> <span class="p">((</span><span class="n">mydoclist</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf_vector_string</span><span class="p">)</span>
    <span class="n">doc_term_matrix</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf_vector</span><span class="p">)</span>
    
    <span class="c"># here&#39;s a test: why did I wrap mydoclist.index(doc)+1 in parens?  it returns an int...</span>
    <span class="c"># try it!  type(mydoclist.index(doc) + 1)</span>

<span class="k">print</span> <span class="s">&#39;All combined, here is our master document term matrix: &#39;</span>
<span class="k">print</span> <span class="n">doc_term_matrix</span>

<span class="c"># Output:</span>
<span class="c"># Our vocabulary vector is [me, basketball, Julie, baseball, likes, loves, Jane, Linda, He, than, more]</span>
<span class="c"># The doc is &quot;Julie loves me more than Linda loves me&quot;</span>
<span class="c"># The tf vector for Document 1 is [2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1]</span>
<span class="c"># The doc is &quot;Jane likes me more than Julie loves me&quot;</span>
<span class="c"># The tf vector for Document 2 is [2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1]</span>
<span class="c"># The doc is &quot;He likes basketball more than baseball&quot;</span>
<span class="c"># The tf vector for Document 3 is [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]</span>
<span class="c"># All combined, here is our master document term matrix: </span>
<span class="c"># [[2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1], [2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]]</span></code></pre></div>

<p>通过这一过程，将文本转换到了向量空间。如果有一篇文本单词频率出现过高，会破坏分析，需将向量规范化，比如$L_2$规范化，使得向量元素的平方和为$1$。</p>

<div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">l2_normalizer</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">el</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">vec</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">el</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">denom</span><span class="p">))</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">vec</span><span class="p">]</span>

<span class="n">doc_term_matrix_l2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">vec</span> <span class="ow">in</span> <span class="n">doc_term_matrix</span><span class="p">:</span>
    <span class="n">doc_term_matrix_l2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l2_normalizer</span><span class="p">(</span><span class="n">vec</span><span class="p">))</span>

<span class="k">print</span> <span class="s">&#39;A regular old document term matrix: &#39;</span> 
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;</span><span class="se">\n</span><span class="s">A document term matrix with row-wise L2 norms of 1:&#39;</span>
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">doc_term_matrix_l2</span><span class="p">)</span>

<span class="c"># if you want to check this math, perform the following:</span>
<span class="c"># from numpy import linalg as la</span>
<span class="c"># la.norm(doc_term_matrix[0])</span>
<span class="c"># la.norm(doc_term_matrix_l2[0])</span>

<span class="c"># Output:</span>

<span class="c"># A regular old document term matrix: </span>
<span class="c"># [[2 0 1 0 0 2 0 1 0 1 1]</span>
<span class="c">#  [2 0 1 0 1 1 1 0 0 1 1]</span>
<span class="c">#  [0 1 0 1 1 0 0 0 1 1 1]]</span>

<span class="c"># A document term matrix with row-wise L2 norms of 1:</span>
<span class="c"># [[ 0.57735027  0.          0.28867513  0.          0.          0.57735027</span>
<span class="c">#    0.          0.28867513  0.          0.28867513  0.28867513]</span>
<span class="c">#  [ 0.63245553  0.          0.31622777  0.          0.31622777  0.31622777</span>
<span class="c">#    0.31622777  0.          0.          0.31622777  0.31622777]</span>
<span class="c">#  [ 0.          0.40824829  0.          0.40824829  0.40824829  0.          0.</span>
<span class="c">#    0.          0.40824829  0.40824829  0.40824829]]</span></code></pre></div>

<p>通过$L_2$规范化，向量元素的取值范围变为了$[0, 1]$。如果要提升某篇文本和主题的相关性，可以一遍遍重复单词，这种方法可以压低这样的频率提升。</p>

<h3 id="section-2">逆文档频率</h3>

<p>词频只考虑了词语在某个特定文档中出现的频率，并未考虑词语在所有文档中的价值。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html">例如</a>：一篇文本中“中国”、“蜜蜂”、“养殖”这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？显然不是这样。因为“中国”是很常见的词，相对而言，“蜜蜂”和“养殖”不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，“蜜蜂”和“养殖”的重要程度要大于“中国”，也就是说，在关键词排序上面，“蜜蜂”和“养殖”应该排在“中国”的前面。</p>

<p>用统计学语言表达，就是在词频的基础上，要对每个词分配一个重要性权重。最常见的词给予最小的权重，较常见的词给予较小的权重，较少见的词给予较大的权重。这个权重叫做<strong>逆文档频率</strong>（IDF，inverse document frequency），它的大小与一个词的常见程度成反比<sup id="fnref:log-base"><a href="#fn:log-base" class="footnote">2</a></sup>，</p>

<p>\begin{equation}
IDF(\mbox{word}) = \log\left(\mbox{num of documents}\over\mbox{num of documents including word}+1\right)。
\end{equation}</p>

<div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">numDocsContaining</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doclist</span><span class="p">):</span>
    <span class="n">doccount</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">doclist</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">freq</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">doccount</span> <span class="o">+=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">doccount</span> 

<span class="k">def</span> <span class="nf">idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doclist</span><span class="p">):</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">doclist</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">numDocsContaining</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">doclist</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span><span class="o">+</span><span class="n">df</span><span class="p">))</span>

<span class="n">my_idf_vector</span> <span class="o">=</span> <span class="p">[</span><span class="n">idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">mydoclist</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">]</span>

<span class="k">print</span> <span class="s">&#39;Our vocabulary vector is [&#39;</span> <span class="o">+</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span> <span class="o">+</span> <span class="s">&#39;]&#39;</span>
<span class="k">print</span> <span class="s">&#39;The inverse document frequency vector is [&#39;</span> <span class="o">+</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">format</span><span class="p">(</span><span class="n">freq</span><span class="p">,</span> <span class="s">&#39;f&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">my_idf_vector</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#39;]&#39;</span>

<span class="c"># Output:</span>

<span class="c"># Our vocabulary vector is [me, basketball, Julie, baseball, likes, loves, Jane, Linda, He, than, more]</span>
<span class="c"># The inverse document frequency vector is [0.000000, 0.405465, 0.000000, 0.405465, 0.000000, 0.000000, 0.405465, 0.405465, 0.405465, -0.287682, -0.287682]</span></code></pre></div>

<p>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。</p>

<h3 id="tf-idf">TF-IDF</h3>

<p>\begin{equation}
TF-IDF(\mbox{word})=TF(\mbox{word})\times IDF(\mbox{word})。
\end{equation}</p>

<div class="highlight"><pre><code class="language-python"><span class="n">doc_term_matrix_tfidf</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c">#performing tf-idf matrix multiplication</span>
<span class="k">for</span> <span class="n">tf_vector</span> <span class="ow">in</span> <span class="n">doc_term_matrix</span><span class="p">:</span>
    <span class="n">doc_term_matrix_tfidf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">tf_vector</span><span class="p">,</span> <span class="n">my_idf_vector</span><span class="p">))</span>

<span class="c">#normalizing</span>
<span class="n">doc_term_matrix_tfidf_l2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tf_vector</span> <span class="ow">in</span> <span class="n">doc_term_matrix_tfidf</span><span class="p">:</span>
    <span class="n">doc_term_matrix_tfidf_l2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l2_normalizer</span><span class="p">(</span><span class="n">tf_vector</span><span class="p">))</span>
                                    
<span class="k">print</span> <span class="n">vocabulary</span>
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">doc_term_matrix_tfidf_l2</span><span class="p">)</span> <span class="c"># np.matrix() just to make it easier to look at</span>

<span class="c"># Output:</span>
<span class="c"># </span>
<span class="c"># set([&#39;me&#39;, &#39;basketball&#39;, &#39;Julie&#39;, &#39;baseball&#39;, &#39;likes&#39;, &#39;loves&#39;, &#39;Jane&#39;, &#39;Linda&#39;, &#39;He&#39;, &#39;than&#39;, &#39;more&#39;])</span>
<span class="c"># [[ 0.          0.          0.          0.          0.          0.          0.</span>
<span class="c">#    0.70590555  0.         -0.50084796 -0.50084796]</span>
<span class="c">#  [ 0.          0.          0.          0.          0.          0.</span>
<span class="c">#    0.70590555  0.          0.         -0.50084796 -0.50084796]</span>
<span class="c">#  [ 0.          0.49957476  0.          0.49957476  0.          0.          0.</span>
<span class="c">#    0.          0.49957476 -0.35445393 -0.35445393]]</span></code></pre></div>

<p>事实上，scikits-learn包含了计算TF-IDF的算法，但由于处理除以0的问题，<code>TfidfVectorizer/TfidfTransformer</code>得到的<a href="http://stackoverflow.com/questions/18687879/error-in-computing-text-similarity-using-scikit-learn/18692538#18692538">结果与以上计算过程不同</a>。</p>

<div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">term_freq_matrix</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">mydoclist</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&quot;Vocabulary:&quot;</span><span class="p">,</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span>

<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>

<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s">&quot;l2&quot;</span><span class="p">)</span>
<span class="n">tfidf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">term_freq_matrix</span><span class="p">)</span>

<span class="n">tf_idf_matrix</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">term_freq_matrix</span><span class="p">)</span>
<span class="k">print</span> <span class="n">tf_idf_matrix</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>

<span class="c"># Output:</span>
<span class="c"># Vocabulary: {u&#39;me&#39;: 8, u&#39;basketball&#39;: 1, u&#39;julie&#39;: 4, u&#39;baseball&#39;: 0, u&#39;likes&#39;: 5, u&#39;loves&#39;: 7, u&#39;jane&#39;: 3, u&#39;linda&#39;: 6, u&#39;more&#39;: 9, u&#39;than&#39;: 10, u&#39;he&#39;: 2}</span>
<span class="c"># [[ 0.          0.          0.          0.          0.28945906  0.</span>
<span class="c">#    0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]</span>
<span class="c">#  [ 0.          0.          0.          0.41715759  0.3172591   0.3172591</span>
<span class="c">#    0.          0.3172591   0.6345182   0.24637999  0.24637999]</span>
<span class="c">#  [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358</span>
<span class="c">#    0.          0.          0.          0.28561676  0.28561676]]</span></code></pre></div>

<h2 id="section-3">应用示例</h2>

<p>向量空间模型的TF-IDF可用于<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html">自动提取关键词</a>、<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html">找出相似文章</a>和<a href="http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html">自动摘要</a>等。</p>

<h2 id="section-4">参考资料</h2>

<ol class="bibliography"><li><span id="Salton:1975:VSM:361219.361220">[1]G. Salton, A. Wong, and C.-S. Yang, “A Vector Space Model for Automatic Indexing,” <i>Communications of the ACM</i>, vol. 18, no. 11, pp. 613–620, Nov. 1975.</span>

[<a href="http://doi.acm.org/10.1145/361219.361220">Online</a>]

</li></ol>

<h3 id="section-5">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:tfidf">
      <p>代码主要参考了<a href="http://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/tfidf.html">The Vector Space Model of text</a>，但文中计算有错误。 <a href="#fnref:tfidf" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:log-base">
      <p>$\log$的底是多少？ <a href="#fnref:log-base" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>工业机器人（2）：技术发展综述</title>
      <link href="http://qianjiye.de/2015/03/industrial-robot-technique-development" />
      <pubdate>2015-03-25T19:29:43+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/industrial-robot-technique-development</guid>
      <content:encoded>&lt;![CDATA[<p>未完成……</p>

<h2 id="section">简介</h2>

<p><strong>工业机器人</strong>是制造业竞争的基石，它能以较小代价实现高生产力、高质量和自适应性的制造。快速成长的行业和新兴的制造工艺，将越来越多的依赖先进的机器人技术。这些行业的机器人数量一直在稳步增长。</p>

<p>为满足大规模的制造，机器人的设计应当满足最广泛的潜在需求，但这在实际中很难做到。不同类别机器人的设计会根据负载、自由度而定，并且工作空间会根据应用类别（比如装配、码垛、喷漆、焊接、机械加工和一般的处理任务）而确定。每个<strong>机器人工作单元</strong>（robot workcell）包括一个或多个带控制器的机器人，以及外设（例如：夹持器或工具<a href="#chenwenjie2014handbook">[1]</a>、安全装置、传感器、物料传送装置等）。通常，完整的机器人工作单元的成本是机器人本身的4倍。机器人工作单元根据实际需求而确定，利用标准化的工程方法、工具和借鉴最佳实践范例能降低成本，并能预估性能。目前，工业机器人的需求主要来自于资本密集和批量化生产的行业，比如：汽车工业、电子和电器工业。未来的机器人不仅是现今机器人特点和性能的扩展，它应当遵循新的设计原则，以满足更广阔的应用领域和行业。同时，新技术，特别是IT技术，也将极大影响未来工业机器人的设计、性能和成本<a href="#martin2008handbook">[2]</a>。</p>

<p>如今，国际和国内标准有助于量化机器人的性能，规范安全防范措施、几何尺寸和媒介接口。大多数机器人都在安全屏障后面操作，以确保人员工作在安全距离范围<a href="#dhillon2002robot">[3]</a>。随着安全标准的提高，人和机器人可以直接协作，允许他们在同一个工作空间工作<a href="#dieter2002man">[4]</a>。</p>

<h2 id="section-1">技术发展简史</h2>

<ul>
  <li>1954年，George Devol申请了一项工业机器人专利“programmed article transfer”<a href="#martin2008handbook">[2]</a>。</li>
  <li>1961年，George Devol与Joseph Engelberger合作，成立了第一家机器人公司Unimation，并在通用汽车工厂将机器人用于从压铸机中提取部件。此后几年，大多数液压驱动的Unimate机器人被用于工件处理和汽车点焊<a href="#nof1999handbook">[5]</a>。由于机器人自身的工作和产出都很稳定，很多公司迅速投入工业机器人的研发。</li>
  <li>1969年，斯坦福人工智能实验室学生Victor Scheinman设计出了6自由度的斯坦福臂（Stanford Arm）<a href="#scheinman1969design">[6]</a>。</li>
  <li>1973年，ASEA（如今的ABB）推出了第一台微电脑控制的全电动工业机器人IRB-6，它的运动轨迹连续，这是弧焊和加工的前提。这种方案稳定性好，机器人的寿命可长达20年<a href="#groover2007automation">[7]</a>。20世纪70年代，机器人在汽车制造业中迅猛发展，主要用于点焊和操纵应用（handling application）。</li>
  <li>1978年，日本山梨大学的Hiroshi Makino发明了平面双关节型机器人 (SCARA，Selective Compliance Assembly Robot Arm)<a href="#makino1982assembly">[8]</a>。这种独创的四轴廉价方案能实现快速和复杂的手臂运动，非常适合组装小部件，它为全球电子和消费产品的繁荣做出了贡献<a href="#boothroyd1992design">[9]</a>。</li>
  <li>1998年，Güdel公司推出了弧形轨道龙门，不同于传统的三正交平移轴方案，它以高的速度和精度极大增加了机器人的工作空间。这对物流（或machine tending）尤为重要<a href="#bloss2003innovation">[10]</a>。</li>
  <li>2005年，MOTOMAN推出第一个同步双手操控机器人的商用产品。灵巧的双手操控对复杂的装配至关重要，可以同时完成处理和工件加工，或者处理大型对象<a href="#wilson2006international">[11]</a>。</li>
  <li>2006年，KUKA实现了拥有先进动力控制能力的紧凑型7自由度机器手臂<a href="#hirzinger2002dlr">[12]</a><sup id="fnref:Another-approach-towards-lightweight"><a href="#fn:Another-approach-towards-lightweight" class="footnote">1</a></sup>，它是轻巧型机器人，满足了对机器人速度和重量的要求<sup id="fnref:weight-to-load-ratio"><a href="#fn:weight-to-load-ratio" class="footnote">2</a></sup>。</li>
  <li>到2007年：（1）机器人的平均价格大约降到了1990年的三分之一，大大提高了速度、负载、平均无故障时间（MTBF，mean time between failures ）等性能指标。（2）可以通过一个控制器对多个机器人编程和同步，使得它们可以在同一个工件上精确协作。（3）用于识别、定位和质量控制的视觉系统，逐步成为机器人控制器的组成部分。（4）机器人通过现场总线或以太网互联，进行控制、配置和维护。（5）机器人租赁和提供机器人服务的公司逐步发展。</li>
</ul>

<p>在工业机器人发展的同时，自动牵引车（AGV，automated guided vehicle）也出现了。这些移动的机器人用于将工件从一点运到另一点。在自动化柔性制造系统（FMS，flexible manufacturing system），AGV已经成为实现其弹性流程的重要部分。起初，由地板下预置的电缆或磁体引导AGV的运动。与此同时，自主导航的AGV被用于大规模的制造和物流。AGV导航利用激光扫描器，获得真实环境的精确二维地图，进行自我定位和避障<a href="#siegwart2011introduction">[13]</a>。在早期，AGV和机器手臂的组合用于加载和卸载机床（machine tool），只有在特定环境，例如半导体行业设备的加载和卸载，这些移动手臂才有经济价值。</p>

<h2 id="section-2">……</h2>

<p>工业机器人的4个显著特点：</p>

<ol>
  <li>仿人功能：利用传感器感知环境，在功能上模仿人的腰、臂、手腕、手爪等部位以实现工业自动化；</li>
  <li>可编程：通过编程适应工作环境的改变；</li>
  <li>通用性：通过更换末端执行器完成不同的工业生产任务；</li>
  <li>良好的环境交互：在无人为干预的条件下，对工作环境自适应控制能力和自我规划能力。</li>
</ol>

<ul>
  <li>传统类容：运动学、动力学、基本控制系统等</li>
  <li>新技术：机器视觉、机器听觉、移动机器人的自主定位等</li>
</ul>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"><li><span id="chenwenjie2014handbook">[1]W. Chen, S. Zhao, and S. L. Chow, <i>Handbook of Manufacturing Engineering and Technology: Grippers and End-Effectors</i>. Springer, 2014.</span>

[<a href="http://link.springer.com/referenceworkentry/10.1007%2F978-1-4471-4670-4_96">Online</a>]

</li>
<li><span id="martin2008handbook">[2]M. Hägele, K. Nilsson, and J. N. Pires, <i>Handbook of Robotics: Industrial Robotics</i>. Springer, 2008.</span>

</li>
<li><span id="dhillon2002robot">[3]B. S. Dhillon, A. R. M. Fashandi, and K. L. Liu, “Robot systems reliability and safety: A review,” <i>Journal of quality in maintenance engineering</i>, vol. 8, no. 3, pp. 170–212, 2002.</span>

</li>
<li><span id="dieter2002man">[4]R. Dieter Schraft, S. Schmid, and S. Thiemermann, “Man-robot cooperation in a flexible assembly cell,” <i>Assembly Automation</i>, vol. 22, no. 2, pp. 136–138, 2002.</span>

</li>
<li><span id="nof1999handbook">[5]S. Y. Nof, <i>Handbook of industrial robotics</i>, vol. 1. John Wiley &amp; Sons, 1999.</span>

</li>
<li><span id="scheinman1969design">[6]V. D. Scheinman, “Design of a computer controlled manipulator.,” DTIC Document, 1969.</span>

</li>
<li><span id="groover2007automation">[7]M. P. Groover, <i>Automation, production systems, and computer-integrated manufacturing</i>. Prentice Hall Press, 2007.</span>

</li>
<li><span id="makino1982assembly">[8]H. Makino, “Assembly robot.” Google Patents, Jul-1982.</span>

[<a href="https://www.google.com/patents/US4341502">Online</a>]

</li>
<li><span id="boothroyd1992design">[9]G. Boothroyd and L. Alting, “Design for assembly and disassembly,” <i>CIRP Annals-Manufacturing Technology</i>, vol. 41, no. 2, pp. 625–636, 1992.</span>

</li>
<li><span id="bloss2003innovation">[10]R. Bloss, “Innovation at IMTS,” <i>Industrial Robot: An International Journal</i>, vol. 30, no. 2, pp. 159–161, 2003.</span>

</li>
<li><span id="wilson2006international">[11]M. Wilson and Y. Kusuda, “The international robot exhibition 2005 in Tokyo,” <i>Industrial Robot: An International Journal</i>, vol. 33, no. 5, pp. 342–348, 2006.</span>

</li>
<li><span id="hirzinger2002dlr">[12]G. Hirzinger, N. Sporer, A. Albu-Schaffer, M. Hahnle, R. Krenn, A. Pascucci, and M. Schedl, “DLR’s torque-controlled light weight robot III-are we reaching the technological limits now?,” in <i>Robotics and Automation, 2002. Proceedings. ICRA’02. IEEE International Conference on</i>, 2002, vol. 2, pp. 1710–1716.</span>

</li>
<li><span id="siegwart2011introduction">[13]R. Siegwart, I. R. Nourbakhsh, and D. Scaramuzza, <i>Introduction to autonomous mobile robots</i>. MIT press, 2011.</span>

</li></ol>

<h3 id="section-4">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:Another-approach-towards-lightweight">
      <p>Another approach towards lightweight and stiff structures has been pursued since the 1980s by developing parallel kinematic machines which connect the machine’s basis with its end-effector by three to six parallel struts<a href="#bloss2003innovation">[10]</a>. These so-called parallel robots are particularly suited to achieve the highest speeds (e.g., for picking), precision (e.g., for machining), or handling high work loads. However, workspace volumes tend to be smaller than those of serial or open kinematic chain robots which are comparable in size. <a href="#fnref:Another-approach-towards-lightweight" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:weight-to-load-ratio">
      <p>From early on, the reduction of the mass and inertia of robot structures was a primary research target, where the human arm with a weight-to-load ratio of 1:1 was con- sidered the ultimate benchmark.  <a href="#fnref:weight-to-load-ratio" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>工业机器人（1）：行业发展综述</title>
      <link href="http://qianjiye.de/2015/03/industrial-robot-industry-development" />
      <pubdate>2015-03-19T21:38:49+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/industrial-robot-industry-development</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">摘要</h2>

<p>本文主要从中国机器人市场占有率的角度，以及国家政府主导下机器人发展的历程，对工业机器人行业的发展进行简要概述。</p>

<h2 id="section-1">简介</h2>

<p><strong>工业机器人</strong>是机器人在应用环境中的重要分支，其操作机具有自动控制、可重复编、多用途、可对3个以上轴进行编程等显著特点，其底座可固定也可移动<a href="#iso8373">[1]</a>。工业机器人是集机械、电子、控制、计算机、传感器、人工智能等多学科的技术于一体的现代制造业自动化装备<a href="#jianggang2011industrialrobot">[2]</a>，广泛应用于汽车制造、机械制造<a href="#hanjiexiehui2014robot">[3]</a>、电子器件、集成电路、电力系统<a href="#liuzhenghong2014robot">[4]</a>、基建<a href="#heqinghua2005robot">[5]</a>等领域。</p>

<p><strong>机器人代替人工，主要解决三个层次问题</strong>：人干不了（特种机器人<sup id="fnref:robot-in-danger"><a href="#fn:robot-in-danger" class="footnote">1</a></sup>）、人干不好（汽车等高端领域）和人不想干（国内一般制造业）的，“现在到了人不想干的阶段，需要用到大量的机器人”。<sup id="fnref:persistent-ailment"><a href="#fn:persistent-ailment" class="footnote">2</a></sup></p>

<p>根据应用领域的不同，工业机器人可分为焊接机器人、切割机器人、装配机器人、喷涂机器人、搬运机器人、码垛机器人、自动牵引车（AGV）等。根据发展过程，工业机器人可分为三代<sup id="fnref:strong-robot-china"><a href="#fn:strong-robot-china" class="footnote">3</a></sup>：</p>

<ol>
  <li>第一代是示教再现型机器人。给它编程，教它干什么活，它会忠实地按程序做。这种机器人不能感知环境变化，不能规划自己的行为。<strong>现在90%都是第一代机器人，国外工业机器人应用最多的还是第一代机器人。</strong></li>
  <li>第二代是具有感知功能的机器人。比如说有视觉功能，它能判断零件的位置和运动，甚至可以将圆形、方形、三角形等不同形状的零件放入不同的箱子。这些机器已经有少量应用于生产。</li>
  <li>第三代的智能机器人还没有出现，它们能完全自主判断、决定自己该怎么做。</li>
</ol>

<p>国内十分重视机器人产业的发展，尤其是近几年。“机器人革命”有望成为“第三次工业革命”的一个切入点和重要增长点，将影响全球制造业格局，而且中国将成为全球最大的机器人市场。国际上有舆论认为，机器人是“制造业皇冠顶端的明珠”，其研发、制造、应用是衡量一个国家科技创新和高端制造业水平的重要标志<sup id="fnref:xis-robot"><a href="#fn:xis-robot" class="footnote">4</a></sup>。但是，中国机器人产业仍很落后。在2014年6月9日的两院院士大会上，国家主席习近平表示了自己忧虑<sup id="fnref:xis-robot:1"><a href="#fn:xis-robot" class="footnote">4</a></sup>：<strong>我国将成为机器人的最大市场，但我们的技术和制造能力能不能应对这场竞争？</strong>我们不仅要把我国机器人水平提高上去，而且要尽可能多地占领市场。这样的新技术新领域还很多，我们要审时度势、全盘考虑、抓紧谋划、扎实推进。</p>

<h2 id="section-2">行业现状</h2>

<p><strong>哈尔滨工业大学和中国科学院沈阳自动化研究所是中国最具实力的机器人科研机构</strong>，<a href="http://robot.hit.edu.cn">分别拥有机器人技术与系统国家重点实验室</a>（哈尔滨工业大学）和<a href="http://www.rlab.ac.cn">机器人学国家重点实验室</a>（中国科学院沈阳自动化研究所）。中国科学院沈阳自动化研究所主要从事水下机器人的研究，哈尔滨工业大学主要从事焊接机器人的研究。中科院沈阳自动化所研制了中国第一台水下机器人“海人一号”，潜水深度达200米<sup id="fnref:30-years-chinese-robot"><a href="#fn:30-years-chinese-robot" class="footnote">5</a></sup>；哈尔滨工业大学研制成功了“天龙一号”5自由度弧焊机器人<a href="#TL01-1991">[6]</a>。</p>

<p>2013年4月21日，<a href="http://cria.mei.net.cn/hyzx.asp">中国机器人产业联盟</a>（CRIA，China Robot Industry Alliance）成立，国内主要的机器人企业都是该联盟的成员单位，目前已有成员单位152家，包括2家理事长单位：</p>

<ul>
  <li>沈阳新松机器人自动化股份有限公司，</li>
  <li>中国机械工业联合会；</li>
</ul>

<p>包括14家副理事长单位：</p>

<ul>
  <li>安徽埃夫特智能装备有限公司，</li>
  <li>安川电机（中国）有限公司［日本］，</li>
  <li>广州数控设备有限公司，</li>
  <li>固高科技（深圳）有限公司［香港］，</li>
  <li>库卡机器人（上海）有限公司［德国］，</li>
  <li>南京埃斯顿机器人工程有限公司，</li>
  <li>欧德神思软件系统（北京）有限公司［德国］，</li>
  <li>唐山开元电器集团，</li>
  <li>中国科学院深圳先进技术研究院，</li>
  <li>长沙长泰机器人有限公司，</li>
  <li>上海ABB工程有限公司［瑞士］，</li>
  <li>三菱电机自动化（中国）有限公司［日本］，</li>
  <li>上海发那科机器人有限公司［日本］，</li>
  <li>哈尔滨博实自动化股份有限公司。</li>
</ul>

<p>这些企业主要专注工业机器人，该领域涉及的技术面广、要求极高，<strong>需要长期的资金投入和深厚的技术积累</strong>。它们要么与高校有长期合作关系、要么有大企业作为支撑，要么是国外著名企业在中国设立的分公司。<strong>新松机器人是国内工业器人的领军企业</strong>，它是中国科学院直属的国有大型企业；博实自动化发源于哈尔滨工业大学；长泰机器人是中轻集团下属公司；埃夫特智能装备的前身为芜湖奇瑞装备有限责任公司。</p>

<p>2005年~2012年，全球工业机器人的年均销售增长率为9%，同期中国工业机器人的年均销售增长率达到25%，2012年底中国超越韩国成为仅次于日本的全球第二大机器人市场，占全球市场15%<sup id="fnref:huge-robot-market"><a href="#fn:huge-robot-market" class="footnote">6</a></sup>。2013年，全球销售工业机器人17.9万台，同比增长12%，其中，中国市场销量达到3.65万台，同比大幅增长59%，占比20.4%，超过日本（2.6万台），<strong>中国已成为全球最大的工业机器人消费市场</strong>，但是国产的只占9500台，<strong>中国市场并未被本土企业所占有</strong>。国产机器人处于行业低端，以搬运和上下料机器人为主，且主体多是单价较低的三、四轴机器人，六轴的多关节型机器人占全国工业机器人销量的比重不足6%<sup id="fnref:chinese-robot-longrun"><a href="#fn:chinese-robot-longrun" class="footnote">7</a></sup>；<strong>国外品牌的机器人几乎垄断了汽车制造、焊接等高端行业领域</strong>，占比达96%<sup id="fnref:4-questions-to-robot"><a href="#fn:4-questions-to-robot" class="footnote">8</a></sup><sup id="fnref:robot-war"><a href="#fn:robot-war" class="footnote">9</a></sup>。截至2014年9月，中国机器人相关企业数量已达到428家，其中88%是系统集成商，从区域看，广东地区117家，占27%，如把浙江、江苏和上海企业加起来，则长三角的企业数量超过珠三角<sup id="fnref:robot-domestic-confidence"><a href="#fn:robot-domestic-confidence" class="footnote">10</a></sup>。2014年底，有70余家上市公司并购或者投资了机器人、智能自动化项目，并且这一势头延续到了2015年；全国已建和在建的工业机器人产业园近40家，相当于平均每个省有超过一家工业机器人产业园，而更多的园区还在筹备中；中国机器人相关企业的数量甚至超过了4000家<sup id="fnref:behind-beautiful-data"><a href="#fn:behind-beautiful-data" class="footnote">11</a></sup>。</p>

<p><strong>核心零部件技术落后是制约中国机器人产业发展最重要的因素</strong>。核心零部件主要分成三部分：精密减速器、伺服电机和控制器<sup id="fnref:robot-controller"><a href="#fn:robot-controller" class="footnote">12</a></sup>。在工业机器人成本中，<strong>核心部件成本约占75%</strong>，成本占比最高的为减速机，约占35%，伺服电机约占25%，控制器约占15%，机器人本体在总成本中占比不到25%<sup id="fnref:ART-8321202-8420-28925966"><a href="#fn:ART-8321202-8420-28925966" class="footnote">13</a></sup>。目前，国内还没有能够提供规模化且性能可靠的精密减速器生产企业，几乎垄断在纳博特斯克（Nabtesco）和哈默纳科（Harmonic）两家日本企业手上<sup id="fnref:lost-key-techniques"><a href="#fn:lost-key-techniques" class="footnote">14</a></sup>。由国产材料生产的谐波减速器寿命只有一到两年，一年后磨损就非常严重，直接影响了机器人的速度和精度<sup id="fnref:oversea-robot-company"><a href="#fn:oversea-robot-company" class="footnote">15</a></sup>。相对于减速器，伺服电机和控制器市场虽未形成主要厂商垄断现象，但几大国际厂商在中国也建立了分工厂，供应充足，产品价格相对合理。国产机器人中80%~90%使用国外减速器，60%~70%使用的是国外电机、40%~50%使用国外控制器<sup id="fnref:4-questions-to-robot:1"><a href="#fn:4-questions-to-robot" class="footnote">8</a></sup>。虽然国内劳动力便宜，但关键元器件必须从国外进口，因此国产企业想降低机器人制造成本非常困难<sup id="fnref:strong-robot-china:1"><a href="#fn:strong-robot-china" class="footnote">3</a></sup>。国内多数机器人企业聚集在机器人本体等领域争夺菲薄的利润，挣扎在盈亏线上。目前国内机器人企业只有广州数控、南京埃斯顿、沈阳新松机器人等少数公司在进行控制系统、驱动和伺服电机等核心零部件自主研发<sup id="fnref:persistent-ailment:1"><a href="#fn:persistent-ailment" class="footnote">2</a></sup>。即使像新松这样的公司，它的减速器、电机、轴承等机器人的核心零部件也要采用国外的。</p>

<p><strong>提升整体系统设计和软件开发能力是当前国产机器人亟需解决的问题</strong>。面对核心部件靠采购，机器人市场需求巨大的现实，通过提升系统设计能力，充分利用现有资源才是解决问题之道。即使国外著名的机器人企业，也是采购核心部件。库卡机器人的软件和整个系统的设计是自己做的，控制器、驱动器、电机等部件外包给其它公司（驱动器由德国力士乐定制生产，电机是西门子的，减速机是日本Nabtesco和捷克Spinea的），最后在松江的总装厂组装到一起<sup id="fnref:newsDetail_forward_1298169"><a href="#fn:newsDetail_forward_1298169" class="footnote">16</a></sup>。由于国内机器人在轨迹平滑算法上不够领先，打磨、焊接等机器人还是以国外品牌居多<sup id="fnref:oversea-robot-company:1"><a href="#fn:oversea-robot-company" class="footnote">15</a></sup>。若能在算法上下功夫，通过提升软件性能，也能提高国产机器人的竞争力。</p>

<h2 id="section-3">发展历程</h2>

<h3 id="section-4">国内行业自身的发展</h3>

<p>中国机器人的发展主要由政府主导，主要集中在水下机器人和焊接机器人，从特种机器人开始，逐步转向工业机器人：</p>

<ul>
  <li>1977年，机器人获准被列入中科院1978至1985年发展规划<sup id="fnref:200307_t20030704_1866103"><a href="#fn:200307_t20030704_1866103" class="footnote">17</a></sup>。</li>
  <li>1985年，国家科学技术委员会开始组织实施工业机器人<strong>“七五”计划</strong>，列入计划的主要项目有：主机，包括点焊、弧焊、上下料、喷漆等工业机器人，机器人关键元部件技术<sup id="fnref:content_283986"><a href="#fn:content_283986" class="footnote">18</a></sup>。</li>
  <li>1985年12月，中国科学院沈阳自动化研究所和上海交通大学研制的<strong>海人一号</strong>在南海下潜到199米，创造了我国自行制造的无人遥控潜水器深潜纪录<a href="#JL1987">[7]</a>。</li>
  <li>1986年，<strong>智能机器人</strong>被中央列为863计划自动化领域两个主题之一，研究目标是跟踪世界先进水平，工作内容是围绕特种机器人进行攻关<sup id="fnref:content_283986:1"><a href="#fn:content_283986" class="footnote">18</a></sup>。</li>
  <li>1987年10月29日，上海交通大学和上海机床厂联合研制的<strong>上海一号弧焊机器人</strong>通过专家鉴定<a href="#SHHT1997">[8]</a>。</li>
  <li>1991年，中华人民共和国科学技术发展十年规划和<strong>“八五”计划</strong>纲要：建成机器人装配示范实验线，研制用于核工业的遥控移动式作业机器人、壁面爬行检查机器人和恶劣环境下作业机器人、水下无缆机器人、精密装配机器人等5种机器人产品<sup id="fnref:t20050831_24436"><a href="#fn:t20050831_24436" class="footnote">19</a></sup>。</li>
  <li>1991年3月27日，上海交通大学和上海机床厂共同研制的<strong>上海三号（SHⅢ）喷涂机器人</strong>通过鉴定；机器人采用电、液伺服系统控制，为空间关节式机器人，有6个自由度，最大手腕负荷为5kg<a href="#SHIII1991">[9]</a>。</li>
  <li>1994年，中国科学院沈阳自动化研究所的<strong>探索者号无缆水下机器人</strong>研制成功，工作深度达到1000米；它是我国第一台无缆水下机器人<a href="#TSZ1995">[10]</a>。</li>
  <li>1995年5月8日，上海交通大学的<strong>精密1号装配机器人</strong>原型样机通过国家验收；该机器人是国家863计划智械机器人主题型号样机之一，带有多传感器、多任务操作、可离线编程的高速、高精度、4轴SCARA平面关节型、直接驱动伺服控制的智能精密装配机器人<a href="#SHJM1995">[11]</a>。</li>
  <li>1995年8月， 中俄联合的<strong>CR-01 6000米无缆自治水下机器人</strong>研制成功（获国家科技进步一等奖）；它的本体长4.374米，宽0.8米，高0.93米，它在空气中的重量为1305公斤，最大潜深6000米，最大水下航速2节，续航能力10小时，定位精度10～15米；我国成为世界上拥有潜深6000米自治水下机器人的少数国家之一<sup id="fnref:6000meter-robot"><a href="#fn:6000meter-robot" class="footnote">20</a></sup>。</li>
  <li>1995年9月，全国科技发展<strong>“九五”计划</strong>和到2010年长期规划纲要：重点研究开发现代设计、精密成形与加工、激光、<strong>机器人</strong>、数控、传感、系统管理等技术，为传统制造业的高技术化提供技术支撑<sup id="fnref:t20050831_24435"><a href="#fn:t20050831_24435" class="footnote">21</a></sup>。</li>
  <li>1997年5月，由北京航空航天大学、清华大学和海军总医院共同研制开发的<strong>脑外科机器人</strong>为病人实施了首例脑手术<sup id="fnref:863plan-15years"><a href="#fn:863plan-15years" class="footnote">22</a></sup>。</li>
  <li>1999年7月15日，哈尔滨工业大学和沈阳自动化所自联合研制的<strong>HT—100A点焊机器人</strong>通过验收<a href="#HT100A1999">[12]</a>。</li>
  <li>2000年11月底，由湖南湘潭江麓工程机械有限公司与国防科技大学联合开发研制的<strong>W1102DZ型高性能无人驾驶振动压路机</strong>通过了国家“863”专家组的验收<a href="#W1102DZ2002">[13]</a>。</li>
  <li>2001年5月，国民经济和社会发展<strong>第十个五年计划</strong>科技教育发展专项规划（科技发展规划）：围绕现代集成制造系统技术和机器人技术，重点研究在全球化敏捷制造环境下的产品开发与设计技术，系统集成与优化技术，现代管理技术，先进制造工艺与装备，攻克面向先进制造的基于机器人的制造单元及系统、特种机器人以及微机电系统等关键技术<sup id="fnref:t20050831_24434"><a href="#fn:t20050831_24434" class="footnote">23</a></sup>。</li>
  <li>2006年2月，<strong>国家中长期科学和技术发展规划纲要</strong>：以服务机器人和危险作业机器人应用需求为重点，研究设计方法、制造工艺、智能控制和应用系统集成等共性基础技术<sup id="fnref:t20081129_65774"><a href="#fn:t20081129_65774" class="footnote">24</a></sup>。</li>
  <li>2006年10月，<strong>国家“十一五”科学技术发展规划</strong>：重点研究极端制造技术、智能机器人技术、重大产品和重大设施寿命预测技术、现代制造集成技术<sup id="fnref:t20061031_55485"><a href="#fn:t20061031_55485" class="footnote">25</a></sup>。</li>
  <li>2011年07月，<strong>国家“十二五”科学技术发展规划</strong>：发展工业机器人、智能控制、微纳制造、制造业信息化等相关系统和装备，重点研发工业机器人的模块化核心技术和功能部件、重大工程自动化控制系统和智能测试仪器及基础件等技术装备<sup id="fnref:sewkjfzgh"><a href="#fn:sewkjfzgh" class="footnote">26</a></sup>。</li>
  <li>2012年7月，<strong>蛟龙号</strong>在马里亚纳海沟试验海区创造了下潜7062米的中国载人深潜纪录，同时也创造了世界同类作业型潜水器的最大下潜深度纪录<sup id="fnref:jiaolong2012"><a href="#fn:jiaolong2012" class="footnote">27</a></sup>。</li>
  <li>2013年4月21日，<strong>中国机器人产业联盟</strong>（CRIA，China Robot Industry Alliance）成立，目前共有成员单位152家，其中2家理事长单位和14家副理事长单位。</li>
  <li>2013年12月30日，工信部发布<strong>《关于推进工业机器人产业发展的指导意见》</strong>，强调要在核心技术及零部件上实现突破。</li>
  <li>2014年11月初，北京赛福斯特技术有限公司与德国库卡机器人联合开发的国内首台<strong>重载机器人搅拌摩擦焊系统</strong>向广州有色金属研究院成功验收交付<sup id="fnref:robot8838"><a href="#fn:robot8838" class="footnote">28</a></sup>。</li>
  <li>2015年2月，国家质检总局同意由中国科学院沈阳自动化研究所和沈阳产品质量监督检验院按B级国家质检中心标准，联合筹建<strong>国家机器人质量监督检验中心</strong><sup id="fnref:t20150223_1584021"><a href="#fn:t20150223_1584021" class="footnote">29</a></sup>。</li>
</ul>

<p>早在1999年9月18日，国家就授予9家单位<sup id="fnref:the-nine-companies"><a href="#fn:the-nine-companies" class="footnote">30</a></sup>“国家863计划智能机器人主题产业化基地”的称号<a href="#863plan1999robot">[14]</a>。16年之后，这些产业化基地中，如今仅剩沈阳新松机器人自动化股份有限公司和哈尔滨博实自动化股份有限公司继续留在机器人行业，天津市南开太阳高技术发展有限公司转行做门卡和管理系统，其余6家单位在机器人行业难觅踪影<sup id="fnref:9-863plan-robot-companies"><a href="#fn:9-863plan-robot-companies" class="footnote">31</a></sup>。</p>

<p>从中国机器人发展的历程可以看出，<strong>国家从20世纪80年代开始就大力扶持机器人的发展</strong>。“七五”以来，机器人产业始终榜上有名，“十二五”规划中更将其列为国家战略性新兴产业。过于丰厚的援助反而成为国产机器人水平提升的阻碍。很多企业上机器人项目，都是在玩机器人概念，其实在变相圈国家的钱，这些企业大部分不是做机器人的，这帮人不会把心思放在产业发展上。但近30年过去，<strong>机器人核心零部件技术却依然攻而不克、制而不精</strong>。政府投入大量资金的“863”攻关似乎没有根本突破。<strong>机器人产业国内专利多，国际专利少，转化价值不大</strong>，这些专利还是拿来报喜用的，是拿来争取项目、评职称用的，不是市场化用的。<sup id="fnref:persistent-ailment:2"><a href="#fn:persistent-ailment" class="footnote">2</a></sup></p>

<h3 id="section-5">国外企业在中国的发展</h3>

<p>1954年，美国人George Devol申请了一项工业机器人专利<a href="#martin2008handbook">[15]</a>；1958年，美国机械与铸造公司研制成功一台数控自动通用机器Versatran，并以工业机器人为商品广告投入市场，这是世界上最早的工业机器人。日本、苏联、西欧等国大都从1967、1968年开始，以美国的Versatran和Unimate型机器人为蓝本开始研制。1967年，日本丰田织机和川崎重工分别引进了美国的Versatran和Unimate，通过引进技术、仿制、改造创新，很快赶上美国并超越了其它国家，从1980年开始进入广泛的普及时代<a href="#zhang1988industrial-robot">[16]</a>。</p>

<p>目前，工业机器人的<strong>四大企业</strong>，瑞士的ABB、德国的库卡、日本的发那科和安川，占有的全球市场份额已超过五成，在中国占有的市场份额已经超过80%，他们<strong>以上海为中心</strong>在中国开展业务：</p>

<ul>
  <li>1979年，瑞士的<strong>ABB</strong>在北京设立办事处，成为四巨头中最早进驻中国的公司；1992年，在<strong>厦门</strong>投资建立了第一家合资企业；1995年，在<strong>北京</strong>注册了投资性控股公司——ABB（中国）有限公司；2006年，其全球机器人业务总部正式进驻<strong>上海</strong>；2013年，在华销售收入超过56亿美元，保持ABB集团全球第二大市场的地位<sup id="fnref:robot-war:1"><a href="#fn:robot-war" class="footnote">9</a></sup>。</li>
  <li>1986年，德国的<strong>库卡</strong>将一台机器人赠送给一汽卡车，这台机器人也是中国汽车工业应用的第一台工业机器人；2000年，成立全资子公司——库卡机器人（<strong>上海</strong>）有限公司。</li>
  <li>1992年，日本的<strong>发那科</strong>成立北京发那科机电有限公司；1994年后开始大批量进入中国，东风汽车及长安汽车分别引进了库卡公司的一条焊装线；2000年，与上海电气兴建合资公司——<strong>上海</strong>发那科机器人有限公司。</li>
  <li>1996年，日本的<strong>安川</strong>与首钢成立了安川首钢机器人有限公司（前身为首钢莫托曼机器人有限公司）；1999年，在<strong>上海</strong>成立全资公司——安川电机（中国）有限公司。</li>
</ul>

<p><strong>技术和资金资源最为集中的汽车制造业是工业机器人最主要的应用领域</strong><sup id="fnref:robot-war:2"><a href="#fn:robot-war" class="footnote">9</a></sup>。汽车工厂使用最多的是ABB和库卡的机器人。库卡机器人主要用在德系汽车工厂，ABB机器人在国产汽车、日系汽车，甚至德系车厂都有用，国产车厂似乎尤其偏爱ABB机器人。ABB和库卡这种欧洲品牌的价格较高，日系的相对便宜<sup id="fnref:abb-invade"><a href="#fn:abb-invade" class="footnote">32</a></sup>。工业机器人如果安装在生产线上，一旦坏掉，就意味着整条生产线都会受影响，这是无法承受之重，使用国外品牌机器人，也是无奈之举<sup id="fnref:ART-8321202-8420-28925037_3"><a href="#fn:ART-8321202-8420-28925037_3" class="footnote">33</a></sup>。</p>

<h2 id="section-6">结论</h2>

<p>中国已沦为全球工业机器人最大的消费市场，由于国产机器人技术落后，中国并未从这个市场直接获得足够利益，绝大部分市场被国外品牌占领，尤其是在高端市场。</p>

<p>中国机器人的发展由政府主导的模式不再有效。从历史发展来看，在中国政府的主导和扶持下，中国的机器人已经成功的实现了从无到有。从目前现状来看，中国机器人如果要从有到做强，实现产业化，应对国际竞争，这种依靠国家政府主导科研院所研发的模式是失败的。</p>

<p>如今机器人的蓬勃发展，不仅是技术的积累与进步，也是市场需求推动的结果。机器人涵盖的技术领域广泛，其发展既需要关键技术的突破，也依赖系统工程的积累和行业技术的积淀。曾经的技术积累，至少从产业化和批量化生产的角度来看，也明显不足。如今中国的机器人产业仍处于探索阶段，还未进入发展期。</p>

<h2 id="section-7">附录</h2>

<h3 id="representative-figure">代表人物<sup id="fnref:representative-figure"><a href="#fn:representative-figure" class="footnote">34</a></sup></h3>

<ul>
  <li><strong>蒋新松</strong>：1931年8月3日～1997年3月30日，中国工程院院士，原沈阳中科院自动化研究所所长、研究员、博士生导师，提出、组织并直接负责水下机器人的研究、开发及产品系列化工作，负责组织研制工业机器人及特种机器人，创建国家机器人技术研究开发工程中心和机器人学开放实验室。</li>
  <li><strong>王天然</strong>：生于1943年3月，中国工程院院士，现任中国科学院沈阳自动化研究所研究员、博士生导师，我国机器人和自动化工程技术界的学科带头人之一，在智能机器人体系结构、工业机器人控制系统、大型自动化系统技术等方面做出了重要贡献，<a href="http://www.siasun.com">沈阳新松机器人自动化股份有限公司</a>董事长。</li>
  <li><strong>蒋厚宗</strong>：生于1935年7月，现任上海交通大学教授，先后担任“上海1号弧焊机器人”、“上海3号喷涂机器人”总设计师，“863精密装配机器人”前期总设计师。创建了上海交通大学海泰科技发展有限公司并任总经理。</li>
  <li><strong>徐如玉</strong>：1942年7月29日～2012年2月17日，中国工程院院士，生前任哈尔滨工程大学海洋综合技术工程研究中心主任，教授、博士生指导教师，在发展我国水下智能机器人技术方面做出了突出贡献，“探索者”号水下机器人主要研究成员。</li>
  <li><strong>封锡盛</strong>：生于1941年12月17日，中国工程院院士，中国科学院沈阳自动化研究所研究员、博士生导师、水下机器人研究室副主任，曾经担任我国第一台有缆遥控水下机器人“海人一号”电控系统负责人、我国第一台无缆自治水下机器人“探索者号”的总设计师。</li>
  <li><strong>张启先</strong>：1925年8月25日～2002年5月25日，中国工程院院士，北京航天航空大学教授、机器人研究所所长，70年代后期，率先突破传统机构学范畴开展机器人技术的研究，负责主持了七自由度机器人、新型三指灵巧手、基于多传感器局部自主的机器人臂与灵巧手集成系统、新型擦窗机器人等项目。</li>
  <li><strong>潘际銮</strong>：生于1927年12月24日，中国科学院院士，著名焊接专家、南昌大学博士生导师，2003年研制成功爬行式弧焊机器人，属国际领先水平。</li>
  <li><strong>蔡鹤皋</strong>：生于1934年6月5日，中国工程院院士，哈尔滨工业大学机器人研究所名誉所长，国产工业机器人事业的开创者之一，研制成功我国第一台弧焊机器人和点焊机器人，解决了机器人轨迹控制精度及路径预测控制等关键技术。<a href="http://www.boshi.cn">哈尔滨博实自动化股份有限公司</a>董事，在<a href="http://www.sgcc.com.cn/xwzx/gsxw/2010/11/234884.shtml">山东电力研究院</a>和<a href="http://www.efort.com.cn">安徽埃夫特智能装备有限公司</a>建有院士工作站。</li>
  <li><strong>熊有伦</strong>：生于1939年，中国科学院院士，中国机器人、机械工程专家，华中科技大学机械科学与工程学院工程信息和智能技术研究所名誉所长，为机器人分析和运动规划提供了统一的准则和方法，开发了机器人离线编程系统HOLPS，建立了点接触约束的机器人操作定性分析理论。</li>
  <li><strong>徐杨生</strong>：中国工程院院士，香港中文大学机械与自动化工程学讲座教授、香港中文大学副校长、香港中文大学（深圳）首任校长，研究的领域为空间机器人、机器人动力学与控制、智能系统、人机界面、及电动车辆。</li>
  <li><strong>张建伟</strong>：德国科学院（汉堡）院士，德国汉堡大学信息学科学系教授，从事及领导高端传感器、认知机器人技术、服务机器人、人机界面的研究与开发。<a href="http://www.flyingwings.cn">弗徕威智能机器人科技有限公司</a>首席科学家，指导弗徕威公司研发团队开发出首款基于3G/4G/Wifi环境的智能服务机器人，并已率先实用化。</li>
  <li><strong>黄田</strong>：生于1955年7月，天津大学教授，现任机械工程学院机械电子工程系主任、机械设计制造及自动化专业负责人，成功开发出中国第一台大型六自由度键锐类并联数控机床和第一台三自由度高速多功能并联数控机床，达到国际领先水平。</li>
  <li><strong>马宏绪</strong>：国防科大机电工程与自动化学院教授、博士生导师、机器人技术专家，我国第一台类人型机器人主要研制者。</li>
  <li><strong>王田苗</strong>：生于1960年，北航机械工程及自动化学院院长、北航机器人研究所所长，主要从事嵌入式智能感知与控制以及在医疗外科机器人、特种服务机器人等方面理论技术和应用研究。</li>
  <li><strong>王杰高</strong>：加拿大拉瓦尔大学机器人学博士，<a href="http://www.estun-robotics.com">埃斯顿机器人</a>总工程师，在加拿大从事了15年的机器人研发工作，曾参与美国国防部卫星修复工程，设计机器人到太空为卫星添加燃料。</li>
</ul>

<h2 id="section-8">参考资料</h2>

<ol class="bibliography"><li><span id="iso8373">[1]ISO, “ISO 8373: Manipulating Industrial Robots–Vocabulary.” 1994.</span>

</li>
<li><span id="jianggang2011industrialrobot">[2]蒋刚, 龚迪琛, 蔡勇, 刘念聪, and 张静, <i>工业机器人</i>. 西南交通大学出版社, 2011.</span>

</li>
<li><span id="hanjiexiehui2014robot">[3]中国焊接协会成套设备与专业机具分会 and 中国机械工程学会焊接学会机器人与自动化专业委员会, <i>中国焊接协会会员读物之六：焊接机器人实用手册</i>. 机械工业出版社, 2014.</span>

</li>
<li><span id="liuzhenghong2014robot">[4]刘洪正, <i>输电线路巡检机器人</i>. 中国电力出版社, 2014.</span>

</li>
<li><span id="heqinghua2005robot">[5]何清华, <i>隧道凿岩机器人</i>. 中南大学出版社, 2005.</span>

</li>
<li><span id="TL01-1991">[6]晏金华, “国产弧焊机器人在汽车车厢焊接生产线上的应用,” <i>电焊机</i>, no. 4, 1991.</span>

</li>
<li><span id="JL1987">[7]王季平, <i>吉林年鉴</i>. 吉林省地方志编纂委员会, 1987.</span>

</li>
<li><span id="SHHT1997">[8]陆正廷, 王德鸿, 陈永一, 郭建伟, and 张蓉蓉, <i>上海航天志</i>. 上海社会科学院出版社, 1997.</span>

</li>
<li><span id="SHIII1991">[9]朱云龙, “上海三号喷涂机器人通过鉴定,” <i>磨床与磨削</i>, no. 3, 1991.</span>

</li>
<li><span id="TSZ1995">[10]李硕, 郭廷志, and 封锡盛, “‘探索者’号无缆水下机器人控制系统,” <i>机器人技术与应用</i>, no. 4, 1995.</span>

</li>
<li><span id="SHJM1995">[11]赵锡芳, “我国第一台高性能精密装配机器人——‘精密1号’装配机器人,” <i>机电一体化</i>, no. 1, 1995.</span>

</li>
<li><span id="HT100A1999">[12]“国产机器人走向实用化——HT-100A点焊机器人通过专家验收,” <i>机器人技术与应用</i>, no. 5, 1999.</span>

</li>
<li><span id="W1102DZ2002">[13]陈罗星, “W1102DZ型高性能无人驾驶振动压路机,” <i>工程机械</i>, no. 7, 2002.</span>

</li>
<li><span id="863plan1999robot">[14]“‘国家863计划智能机器人主题产业化基地’发牌仪式在北京举行,” <i>机器人技术与应用</i>, no. 6, 1999.</span>

</li>
<li><span id="martin2008handbook">[15]M. Hägele, K. Nilsson, and J. N. Pires, <i>Handbook of Robotics: Industrial Robotics</i>. Springer, 2008.</span>

</li>
<li><span id="zhang1988industrial-robot">[16]张建民, <i>工业机器人</i>. 北京理工大学出版社, 1988.</span>

</li></ol>

<h3 id="section-9">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:robot-in-danger">
      <p><a href="http://robot.ofweek.com/2015-03/ART-8321202-8120-28939020.html">安监总局局长：危险岗位机器人操作是方向</a> <a href="#fnref:robot-in-danger" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:persistent-ailment">
      <p><a href="http://robot.ofweek.com/2015-01/ART-8321202-8420-28927492_2.html">庆父不死 鲁难未已：机器人发展四大”顽固派”应给予打压</a> <a href="#fnref:persistent-ailment" class="reversefootnote">&#8617;</a> <a href="#fnref:persistent-ailment:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:persistent-ailment:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:strong-robot-china">
      <p><a href="http://www.guancha.cn/Industry/2014_08_04_253239.shtml?BJJX">院士谈工业机器人制造：中国1亿人完成美国1000万人工作量</a>。 <a href="#fnref:strong-robot-china" class="reversefootnote">&#8617;</a> <a href="#fnref:strong-robot-china:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:xis-robot">
      <p><a href="http://www.guancha.cn/Science/2014_06_10_236328.shtml">习近平科学院工程院院士大会谈机器人革命：我在想如何应对</a>。 <a href="#fnref:xis-robot" class="reversefootnote">&#8617;</a> <a href="#fnref:xis-robot:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:30-years-chinese-robot">
      <p><a href="http://www.lwdf.cn/article_881_1.html">中国机器人30年：一直在追赶</a>。 <a href="#fnref:30-years-chinese-robot" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:huge-robot-market">
      <p><a href="http://robot.ofweek.com/2015-02/ART-8321202-8420-28930467.html">中国蕴藏千亿机器人大市场 机器人企业仍不能过于乐观</a> <a href="#fnref:huge-robot-market" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:chinese-robot-longrun">
      <p><a href="http://robot.ofweek.com/2015-02/ART-8321202-8420-28933950.html">国产机器人四大困扰 未来之路漫长艰辛</a> <a href="#fnref:chinese-robot-longrun" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4-questions-to-robot">
      <p><a href="http://www.lwdf.cn/article_879_1.html">四问中国机器人</a>。 <a href="#fnref:4-questions-to-robot" class="reversefootnote">&#8617;</a> <a href="#fnref:4-questions-to-robot:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:robot-war">
      <p><a href="http://www.lwdf.cn/article_880_1.html">中国市场上的机器人之战</a>。 <a href="#fnref:robot-war" class="reversefootnote">&#8617;</a> <a href="#fnref:robot-war:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:robot-war:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:robot-domestic-confidence">
      <p><a href="http://robot.ofweek.com/2015-01/ART-8321202-8420-28920963_2.html">好事不怕晚 给国产机器人以更多信心</a> <a href="#fnref:robot-domestic-confidence" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:behind-beautiful-data">
      <p><a href="http://robot.ofweek.com/2015-02/ART-8321202-8420-28935174.html">中国机器人市场美好数据的背后</a> <a href="#fnref:behind-beautiful-data" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:robot-controller">
      <p><a href="http://robot.ofweek.com/2015-01/ART-8321202-8420-28920889.html">机器人控制器的现状分析及未来展望</a> <a href="#fnref:robot-controller" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:ART-8321202-8420-28925966">
      <p><a href="http://robot.ofweek.com/2015-01/ART-8321202-8420-28925966.html">缺乏核心技术 国产机器人正艰难走在“非标”路上</a> <a href="#fnref:ART-8321202-8420-28925966" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:lost-key-techniques">
      <p><a href="http://robot.ofweek.com/2015-02/ART-8321202-8420-28932427.html">无核心技术 何谈崛起：核心零部件成中国机器人最大掣肘</a> <a href="#fnref:lost-key-techniques" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:oversea-robot-company">
      <p><a href="http://robot.ofweek.com/2015-02/ART-8321202-8120-28933923_2.html">跨国公司技术垄断 中国工业机器人难“自主”</a> <a href="#fnref:oversea-robot-company" class="reversefootnote">&#8617;</a> <a href="#fnref:oversea-robot-company:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:newsDetail_forward_1298169">
      <p><a href="http://www.thepaper.cn/newsDetail_forward_1298169">机器人产业调查②：上海机器人产业只能走高端路线</a> <a href="#fnref:newsDetail_forward_1298169" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:200307_t20030704_1866103">
      <p><a href="http://www.syb.ac.cn/zt/sycx/200307/t20030704_1866103.html">水下机器人从零到6000米的突破</a> <a href="#fnref:200307_t20030704_1866103" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:content_283986">
      <p><a href="http://wb.lzbs.com.cn/html/2011-08/12/content_283986.htm">中国机器人是如何发展起来的？</a> <a href="#fnref:content_283986" class="reversefootnote">&#8617;</a> <a href="#fnref:content_283986:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:t20050831_24436">
      <p><a href="http://www.most.gov.cn/ztzl/gjzcqgy/zcqgylshg/200508/t20050831_24436.htm">中华人民共和国科学技术发展十年规划和“八五”计划纲要（1991-2000）</a> <a href="#fnref:t20050831_24436" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6000meter-robot">
      <p><a href="http://www.kepu.net.cn/gb/technology/robot/special/spe202.html">历史性突破——水下6000米无缆自治机器人</a> <a href="#fnref:6000meter-robot" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t20050831_24435">
      <p><a href="http://www.most.gov.cn/ztzl/gjzcqgy/zcqgylshg/200508/t20050831_24435.htm">全国科技发展“九五”计划和到2010年长期规划纲要（汇报稿）</a>。 <a href="#fnref:t20050831_24435" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:863plan-15years">
      <p><a href="http://www.ca800.com/news/d_1nrusj6oaltln.html">863计划15周年成就展“机器人”唱主角</a> <a href="#fnref:863plan-15years" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t20050831_24434">
      <p><a href="http://www.most.gov.cn/ztzl/gjzcqgy/zcqgylshg/200508/t20050831_24434.htm">国民经济和社会发展第十个五年计划科技教育发展专项规划（科技发展规划）</a> <a href="#fnref:t20050831_24434" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t20081129_65774">
      <p><a href="http://www.most.gov.cn/mostinfo/xinxifenlei/gjkjgh/200811/t20081129_65774.htm">国家中长期科学和技术发展规划纲要（2006━2020年）</a> <a href="#fnref:t20081129_65774" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t20061031_55485">
      <p><a href="http://www.most.gov.cn/kjgh/kjfzgh/200610/t20061031_55485.htm">国家“十一五”科学技术发展规划</a>。 <a href="#fnref:t20061031_55485" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:sewkjfzgh">
      <p><a href="http://www.most.gov.cn/kjgh/sewkjfzgh/">“十二五”科技发展规划</a> <a href="#fnref:sewkjfzgh" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:jiaolong2012">
      <p><a href="http://baike.baidu.com/link?url=yeh4j2zezT1DvQuTFrsSBQv352ELHOeKX_eVUIU-Kydd02qAHN8Fu8rwZ7f7OvFubeO5ZvRUd2GMHXfFqy8QQK">蛟龙号载人潜水器</a> <a href="#fnref:jiaolong2012" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:robot8838">
      <p><a href="http://news.weldinfo.net/show.php?itemid=8838">中国首台重载机器人搅拌摩擦焊系统顺利交付</a> <a href="#fnref:robot8838" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t20150223_1584021">
      <p><a href="http://www.ln.gov.cn/zfxx/jrln/wzxw/201502/t20150223_1584021.html">沈阳市筹建国内首家机器人质量监督检验中心</a> <a href="#fnref:t20150223_1584021" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:the-nine-companies">
      <p>国家863计划智能机器人主题产业化基地：<a href="http://www.siasun.com">沈阳新松机器人自动化股份有限公司</a>、<a href="http://www.boshi.cn">哈尔滨博实自动化股份有限公司</a>、<a href="http://www.riambsoft.com">北京机械工业自动化研究所工业机器人与应用技术工程研究中心</a>、<a href="http://www.nkty.com">天津市南开太阳高技术发展有限公司</a>、一汽集团涂装技术开发中心、大连贤科机器人技术有限公司、上海交大海泰科技发展有限公司、上海机电一体化工程有限公司、四川绵阳四维焊接自动化设备有限公司。 <a href="#fnref:the-nine-companies" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:9-863plan-robot-companies">
      <p>单位的现状来源于网络资料。 <a href="#fnref:9-863plan-robot-companies" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:abb-invade">
      <p><a href="http://gongkong.ofweek.com/2015-01/ART-310058-8420-28926968.html">ABB机器人入侵中国市场 狼早就来了</a>。 <a href="#fnref:abb-invade" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:ART-8321202-8420-28925037_3">
      <p><a href="http://robot.ofweek.com/2015-01/ART-8321202-8420-28925037_3.html">技术攻关竹篮打水 机器人项目沦为“赚钱工具”</a> <a href="#fnref:ART-8321202-8420-28925037_3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:representative-figure">
      <p>资料来源于网络。 <a href="#fnref:representative-figure" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>纹理分类系统</title>
      <link href="http://qianjiye.de/2015/03/texture-classification-system" />
      <pubdate>2015-03-16T19:28:50+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/texture-classification-system</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">概述</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-16-texture-classification-system-texture-example.jpg"><img src="/assets/images/2015-03-16-texture-classification-system-texture-example.jpg" alt="纹理示例" /></a><div class="caption">图 1:  纹理示例 [<a href="/assets/images/2015-03-16-texture-classification-system-texture-example.jpg">JPG</a>]</div></div></div>

<p>目前生产线自动化程度不高，在初期面料选择，以及后期质量检测等多个环节基本都是人工完成，使得生产效率低下，柔性生产能力不足，产品质量不稳定。在初步调研基础上，针对上述问题，提出了基于视觉检测的生产线自动化的改进方案。</p>

<p>本项目利用计算机视觉算法，对不同材质的纹理进行分类，以及实现其它机器视觉相关的辅助功能。其中，纹理样式如上图所示。系统面临的主要问题包括：</p>

<ul>
  <li>相同纹路的材质，色彩或亮度之间存在差异；</li>
  <li>相同颜色或亮度的材质，纹路之间存在差异；</li>
  <li>待分类的纹理种类很多，超过100种，各类别之间差异很小。</li>
</ul>

<h4 id="section-1">系统需要实现的主要功能（第一期工程）：</h4>

<ul>
  <li>对不同纹理的材质分类；</li>
</ul>

<h4 id="section-2">系统需要实现的辅助功能包括（第二期工程）：</h4>

<ul>
  <li>工件尺寸检测（误差大小于1mm）：测量对象的长和宽；</li>
  <li>物料缺陷检测：材质是否有划痕、是否平坦；</li>
  <li>打孔质量评估：若在材质上打孔，是否在预设误差范围；</li>
  <li>颜色花型检测；</li>
  <li>条码识别。</li>
</ul>

<h4 id="section-3">根据功能需求，系统划分为两部分（两个子系统）：</h4>

<ol>
  <li>实现色彩相关的功能：采用彩色面阵工业相机，成像分辨率精度略低，主要实现纹理分类。</li>
  <li>实现色彩无关的功能：采用黑白面阵工业相机，成像分辨率较高，实现测量、缺陷检测等剩余功能。颜色花型检测可能会同时使用这个两个子系统。</li>
</ol>

<h2 id="section-4">现场评估</h2>

<p>确定硬件方案和预估系统性能，搜集所需的信息：</p>

<ul>
  <li><strong>环境评估</strong>：一般工业生产环境比较复杂，由于机器视觉系统是一个精密度较高系统，所以需要对生产现场环境从光、尘、冲击振动、温湿度、水雾、电力稳定等几方面考察。</li>
  <li><strong>处理对象评估</strong>：工件尺寸大小范围，工件运动方式、速度，视场内工件数量，是否重叠，可提取特征分析等工件的静动态信息。</li>
</ul>

<h4 id="section-5">完成以上评估后，通过与工作人员沟通，在尽量不干扰原生产线流程的情况下，进行硬件系统设计，并为视觉检测系统选择合适安装机位：</h4>

<ul>
  <li>成像系统外围保护装置 (如防水，防尘，防震等)；</li>
  <li>成像系统光源设计；</li>
  <li>确定视场大小、分辨率（相机分辨率，空间分辨率），相机与工件之间距离范围，相机与后端处理机器的距离和线缆布置等。</li>
</ul>

<h4 id="section-6">根据目前了解的情况（通过与工作人员交流，以及查看部分面料实物和图片），得出如下结论：</h4>

<ul>
  <li>面料颜色花型识别在流水线最前端，生产方可以提供一个可控的环境来安装视觉识别设备；</li>
  <li>流水线对纹理分类的实时性要求不是很高，初步估计在3s以内；</li>
  <li>面料纹理种类繁多，有些仅有颜色上差异，有些反光较强，因此对光源设计、成像系统的颜色还原性要求很高；</li>
  <li>从目前掌握图片来看，纹理周期性不详，因此无法确定视场大小（暂假定为10cm*10cm）；</li>
  <li>为了尽可能提取纹理特征，空间分辨率设定为0.05mm/pixel，那相机分辨率为2000*2000=400万像素；</li>
  <li>镜头的选择：
    <ul>
      <li>焦距：目前暂无法知道相机与面板之间的距离，暂时无法确定镜头焦距；</li>
      <li>光圈：由于面板运动速度不快，甚至检测时可以暂停，所以光圈选择不必过大；</li>
      <li>选择远心镜头：生产线上的面板可能存在抖动，远心镜头可以在一定的物距范围内，使图像放大倍率不随物距的变化而变化。</li>
    </ul>
  </li>
</ul>

<h2 id="section-7">纹理分类</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-03-16-texture-classification-system-classification-workstation.jpg"><img src="/assets/images/2015-03-16-texture-classification-system-classification-workstation.jpg" alt="纹理分类的工作环境" /></a><div class="caption">图 2:  纹理分类的工作环境 [<a href="/assets/images/2015-03-16-texture-classification-system-classification-workstation.jpg">JPG</a>]</div></div></div>

<p>纹理分类就是利用采集到的图像，对不同材质的纹理自动判定其所属的类别，其目的在于：</p>

<ul>
  <li>能够对多类别的纹理进行分类；</li>
  <li>具有自学习功能，方便增减纹理种类。</li>
</ul>

<h3 id="section-8">成像系统</h3>

<p>成像系统的功能是采集满足分类要求的纹理图像，包括采集图像所需要的硬件系统，以及控制成像质量的软件标定系统。硬件主要包括机器视觉相关的相机、镜头、光源等部件，不包括机械结构。</p>

<p>纹理分类可只采集待分类对象的部分纹理，因此相机的分辨率不需要太高，可采用200W像素的彩色工业相机。由于部分材质存在反光，光源采用<a href="http://baike.baidu.com/link?url=pY1jf8L5BHH71vFyPpf_nvTb2469FHQMwhAaJeup6G__TzEOX08LsacsdC3H4iApq-al4SkTT1Npp3nNQzbXQK">同轴光源</a>。</p>

<p>通过标定系统提高成像的质量和稳定性。</p>

<h4 id="section-9">标定系统：</h4>

<p>在系统使用前，首先需要<strong>标定成像系统</strong>，确保能够稳定地采集到满足要求的图像，同时在使用过程中，标定机制可以提高系统稳定性。由于相似纹理较多，需要对相似的纹理进行有差别的成像。利用标准纹理图像库，系统能自动判断是否调节到最佳成像效果，或者成像系统是否需要重新标定。</p>

<h4 id="section-10">标定的主要步骤：</h4>

<ol>
  <li>将待识别的材质样本按纹理由浅到深排序；</li>
  <li>筛选出一组具有代表性的纹理，对其成像，调节成像参数，直到达到满意效果；</li>
  <li>在剩余材质中任意挑选一组，在不调节成像参数的情况下，对其成像：
    <ul>
      <li>如果能达到满意效果，保存成像参数、代表性材质样本以及其图像，结束标定；</li>
      <li>如果成像效果不满意，返回第2步。</li>
    </ul>
  </li>
</ol>

<p>在系统使用时，采集代表性材质样本的图像，通过计算机自动判断成像系统是否需要重新标定。</p>

<h3 id="section-11">算法思路</h3>

<p>纹理分类模型采用<strong>机器学习框架</strong>，系统上生产线运行前，先通过样本库训练分类模型，在生产线分类时，直接调用分类模型进行纹理分类。机器学习框架采用多层感知器和随机森林，通过机器学习算法提高应用的灵活性和系统的稳定性。</p>

<h4 id="section-12">训练多层感知器分类模型的流程：</h4>

<ol>
  <li>对原始纹理图像<strong>按区域随机重采样</strong>（区域的大小固定，应包含能区别纹理的足够信息），得到相同尺寸的纹理子图像，构造纹理的样本库；</li>
  <li>利用不同尺度的卷积核提取纹理特征；</li>
  <li>利用特征训练<strong>多层感知器</strong>。</li>
</ol>

<p>如果多层感知器不能满足性能要求，尝试使用随机森林。</p>

<h4 id="section-13">训练随机森林分类模型的流程：</h4>

<ol>
  <li>对纹理图像<strong>按像素随机重采样</strong>，得到相同长度的像素序列，构造纹理的样本库；</li>
  <li>对每个样本的像素按色彩排序或者Hash编码；</li>
  <li>训练适合多分类的<strong><a href="/2015/01/random-forest">随机森林</a></strong>；</li>
  <li>分析随机森林的分类效果：
    <ul>
      <li>如果满足性能指标，保存分类模型，结束训练过程；</li>
      <li>如果不满足性能指标，定位出错分的类别，对这些错分的纹理，构建下一层的随机森林：
        <ul>
          <li>对纹理图像<strong>按像素结构化重采样</strong>，构造纹理的样本库；</li>
          <li>返回第2步。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>最终的分类模型，可能是多个随机森林级连组成的多层次分类模型。</p>

<p>如果上述的随机森林模型无法达到性能指标，启用<strong><a href="/2015/02/deep-learning">深度神经网络</a></strong>作为第三备选方案。</p>

<p>系统具备自学习功能，执行加减新类别操作后，系统自动重新训练分类模型。</p>

<h2 id="section-14">尺寸测量</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-03-16-texture-classification-system-measurement-workstation.jpg"><img src="/assets/images/2015-03-16-texture-classification-system-measurement-workstation.jpg" alt="测量几何尺寸的测试环境" /></a><div class="caption">图 3:  测量几何尺寸的测试环境 [<a href="/assets/images/2015-03-16-texture-classification-system-measurement-workstation.jpg">JPG</a>]</div></div></div>

<p>尺寸测量的目的是测量工件的几何尺寸，也就长和宽。</p>

<h3 id="section-15">成像系统</h3>

<p>由于尺寸成像的精度要求高，拟采用500W像素的灰度工业相机。考虑到测量的对象也可能发光，也采用同轴光源。</p>

<h3 id="section-16">算法思路</h3>

<p>采用单目相机，利用环境标定的方法测量尺寸。基本原理是，相机固定，成像的对象位置固定，被测工件的实际尺寸与其通过相机成像的大小成比例。</p>

<p>500W像素相机图像的长边假设可达2400像素（实际相成像尺寸约为4:3），如果对2.4米长的工件成像，理论上的分辨率为1mm/pixel，如果用像素级的测量算法，理论上1个像素误差会导致1mm的测量误差，实际测量误差会大于该值。</p>

<p>对于精度较高的测量，误差还来源于几方面来源：</p>

<ul>
  <li>被测量对象可能存在毛边，边缘不平整；</li>
  <li>现场环境较复杂，设备震动，工件位置不固定；</li>
  <li>长时间工作系统的偏移量。</li>
</ul>

<h4 id="section-17">部分测量法</h4>

<p>针对以上问题，需要构建合适的测量系统，拟采用工件<strong>部分测量</strong>的方法，提高相机的分辨率，同时提供自校正机制，提高稳定性。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-03-16-texture-classification-system-measurement-scheme.png"><img src="/assets/images/2015-03-16-texture-classification-system-measurement-scheme.png" alt="工件尺寸测量方案" /></a><div class="caption">图 4:  工件尺寸测量方案 [<a href="/assets/images/2015-03-16-texture-classification-system-measurement-scheme.png">PNG</a>]</div></div></div>

<p>假设测量工件是矩形，采用对角测量法，测量方案如上图所示。通过机械装置对左上角标定，通过图像方式对右下角测量，这样减少工件需要成像的面积，极大提高了图像测量的分辨率。测量步骤如下：</p>

<ol>
  <li>通过机械传动装置在左上角测量区域对齐被测工件；</li>
  <li>右下角有标定的标准刻度，在右下角通过成像的方式测量相对位置；</li>
  <li>右下和左上两个位置相减得到工件尺寸。</li>
</ol>

<p>右下角有标定刻度，通过判断标定刻度在相机的成像的改变量，相机可以实现<strong>自标定</strong>。这种方法解决了相机分辨率对测量精度的制约，同时提供了自标定机制。</p>

<p>在算法上，可以考虑通过<strong>亚像素算法</strong>，提高测量精度。</p>

<h4 id="section-18">查表法</h4>

<p>若果被测工件的尺寸已知，且各种尺寸的工件差异较显著，测量问题实际上就是一个识别问题，或者说是个校验问题。</p>

<p>根据已知的工件尺寸将待测工件分为不同的测量区间，每个区间对应一个已知的工件尺寸，实际上这也相当于构建决策树。根据图像测量的值（不需要很高的精度）落在哪个区间，就可以查出该工件的尺寸。</p>

<p>该方法的准确度和稳定性都很高，但前提是对被测对象有限制。</p>

<h2 id="section-19">缺陷检测</h2>

<h2 id="section-20">条码识别</h2>

<h2 id="section-21">技术指标</h2>

<h2 id="section-22">系统接口</h2>

<ul>
  <li>按需输出指定格式的数据；</li>
  <li>按需提供程序调用接口。</li>
</ul>

<h2 id="section-23">实验记录</h2>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>统计学习：线性回归</title>
      <link href="http://qianjiye.de/2015/03/statistical-learning-linear-regression" />
      <pubdate>2015-03-13T20:06:43+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/statistical-learning-linear-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">应用场景</h2>

<p>线性回归是研究其它一些统计学习方法的起点，其它方法可看作线性回归的扩展。</p>

<p>在<a href="http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv">Advertising数据集</a>，某种商品销售额（sales）可视为电视（TV）、广播（radio）和报纸（newspaper）广告预算的函数。通过这些数据，能否解决如下问题：</p>

<ol>
  <li>广告预算和销量有关吗？</li>
  <li>如果相关，这种相关性有多强？也就是给定广告预算，能否准确预测销售额？</li>
  <li>各种类型的广告对销售额都有影响么吗？</li>
  <li>如果准确评估每种广告对销售额的影响？</li>
  <li>如何准确预测未来销售额？</li>
  <li>广告和销售额是线性关系吗？</li>
  <li>广告媒体之间有<strong>协同效应</strong>（synergy effect）吗？协同效应在统计学中称为<strong>相关性</strong>（interaction effect）。例如：在电视和广播分别投放\$50000的广告和将这\$100000投放到其中一种媒体，效果一样吗？</li>
</ol>

<h2 id="section-1">单变量回归分析</h2>

<p>数学上，回归问题可表示为
\begin{equation}
Y\approx \beta_0 + \beta_1 X，
\end{equation}
$X$可以指代电视，$Y$表示销售额。$\beta_0 $和$\beta_1$分别表示<strong>截距</strong>（intercept）和<strong>斜率</strong>（slope）。模型参数用训练数据上的最小二乘法估计可得
\begin{equation}
\begin{aligned}
\hat\beta_1&amp;={\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\over\sum_{i=1}^n(x_i-\bar x)^2}\\
\hat\beta_0&amp;=\bar y-\hat\beta_1\bar x，
\end{aligned}
\label{eq:lsm-coefficients}
\end{equation}
销售额可以预测
\[
\hat y=\hat\beta_0+\hat\beta_1x，
\]
符号$\hat{\;}$表示估计值。第$i$个样本的<strong>残差</strong>（residual）记为$e_i=y_i-\hat y_i$，$n$个样本的<strong>残差平方和</strong>（RSS，residual sum of squares）为
\begin{equation}
RSS=e_i^2+e_2^2+\ldots+e_n^2，
\label{eq:rss}
\end{equation}
RSS也称为SSE。</p>

<p>假设$X$和$Y$的真实关系为
\begin{equation}
Y=\beta_0+\beta_1X+\epsilon，
\label{eq:population-regression-line}
\end{equation}
截断$\beta_0$表示当$X=0$是$Y$的取值，斜率$\beta_1$表示$X$增加一个单位$Y$的平均增加量，误差$\epsilon$代表了这个简单模型缺失的其它所有量。$X$和$Y$的真实关系可能不是线性的，还可能有其它量导致$Y$的改变，也可能有测量误差。通常假定误差项与$X$独立。</p>

<p>模型\eqref{eq:population-regression-line}定义了<strong>总体回归直线</strong>（population regression line），它是$X$和$Y$真实关系的最佳线性近似。最小二乘法估计的系数\eqref{eq:lsm-coefficients}确定了<strong>最小二乘直线</strong>（least squares line）。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png"><img src="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png" alt="线性回归模型" /></a><div class="caption">图 1:  线性回归模型 [<a href="/assets/images/2015-03-13-statistical-learning-linear-regression-experiment1.png">PNG</a>]</div></div></div>

<p>上图左中，红色直线表示真实关系$f(X)=2+3X$，也就是总体回归线，深蓝色直线表示最小二乘直线，它是基于数据的最小二乘估计；上图右的浅蓝色直线是10次最小二乘法估计的结果，每次估计采用的不同数据集，但是10次估计的平均值很接近总体回归线。</p>

<p>每次实验的100组数据通过模型$Y=2+3X+\epsilon$随机生成，$\epsilon$由零均值的正态分布产生。在实际应用中，只能通过数据得到最小二乘直线，无法观察到总体回归直线。</p>

<p>在一个数据集上通过\eqref{eq:lsm-coefficients}无法得到参数的准确估计，但是通过在大量数据集上估计结果的平均，最小二乘直线就是总体回归直线的无偏（unbiased）估计。</p>

<p>参数估计估计的精度可以通过<strong>标准误差</strong>（SE，standard error）衡量，</p>

<p>\begin{equation}
\begin{aligned}
SE\left(\hat\beta_0\right)^2&amp;=\sigma^2\left[{1\over n}+{\bar x^2\over\sum_{i=1}^n(x_i-\bar x)^2}\right]\\
SE\left(\hat\beta_1\right)^2&amp;={\sigma^2\over\sum_{i=1}^n(x_i-\bar x)^2}，
\end{aligned}
\end{equation}</p>

<p>$\sigma$表示均方差（standard deviation），$\sigma^2=Var(\epsilon)$。</p>

<p>标准误差揭示了参数估计值和真实值之间的平均差异。公式严格成立的条件是每个样本的误差$\epsilon_i$和<strong>公共方差</strong>（common variance）无关。即使这个条件不成立，这个公式也可作为评估的很好近似。当$x_i$越分散时，$SE\left(\hat\beta_1\right)^2$越小；当$\bar x=0$时，$SE\left(\hat\beta_0\right)^2$也更小，此时$\hat\beta_0=\bar y$；随着样本数$n$的增加，$SE\left(\hat\beta_0\right)^2$也越来越小。通常$\sigma^2$未知，但可以通过数据集上的<strong>标准化残差</strong>（RSE，residual standard error）估计，</p>

<p>\begin{equation}
RSE=\sqrt{RSS\over n-2}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n\left(y_i-\hat y_i\right)^2}。
\label{eq:rse}
\end{equation}</p>

<p>标准误差可以用于计算<strong>置信区间</strong>（confidence interval）。$95\%$的置信区间表示该区间有$95\%$的概率包括参数的真实值。类似的均方根误差（RMSE，root-mean-square error）则定义为</p>

<p>\[
RMSE=\sqrt{RSS\over n}。
\]</p>

<p>对于线性回归的参数估计，$95\%$的置信区间记为<sup id="fnref:confidence-interval"><a href="#fn:confidence-interval" class="footnote">1</a></sup>：</p>

<p>\begin{equation}
\hat\beta_0\pm 2\cdot SE\left(\hat\beta_0\right)，\qquad
\hat\beta_1\pm 2\cdot SE\left(\hat\beta_1\right)。
\label{eq:lr-confidence-interval}
\end{equation}</p>

<p>标准误差也可用于系数的<strong>假设检验</strong>（hypothesis test）。最常用的两种假设检验是<strong>无效假设检验</strong>（null hypothesis）和<strong>备选假设检验</strong>（alternative hypothesis），
\[
\begin{aligned}
H_0&amp;:\mbox{There is no relationship between }X\mbox{ and }Y\\
H_a&amp;:\mbox{There is some relationship between }X\mbox{ and }Y，
\end{aligned}
\]
数学上表示为
\[
H_0:\beta_1=0，\qquad H_a:\beta_1\neq0。
\]
若$\beta_1=0$，那么$X$和$Y$无关。进行无效假设检验时，需要估计$\hat\beta_1$是否离0足够远，这需要通过$\hat\beta_1$的精度$SE\left(\hat\beta_1\right)$来衡量。如果$SE\left(\hat\beta_1\right)$很小，较小的$\hat\beta_1$可以足够确信$\beta_1\neq 0$；如果$SE\left(\hat\beta_1\right)$很大，$\hat\beta_1$的绝对值要很大才能拒绝无效假设。在实际应用中，通过<strong>$t$统计量</strong>（$t$-statistic）</p>

<p>\begin{equation}
t={\hat\beta_1-0\over SE\left(\hat\beta_1\right)}
\label{eq:t-statistic}
\end{equation}</p>

<p>测量$\hat\beta_1$远离$0$（假设$H_0$成立）的标准差（standard deviation）。如果$X$和$Y$真的无关，\eqref{eq:t-statistic}是$n-2$自由度的<strong>$t$分布</strong>（$t$-distribution）。当$n$大约大于$30$时，$t$分布与正态分布很相似。<strong>$p$值</strong>（$p$-value）表示$\beta_1=0$时观测值大于等于$|t|$的概率。较小的$p$值表示预测（predictor）与响应（response）之间有关联。若$p$值很小时，拒绝无效假设（reject the null hypothesis）表示$X$与$Y$有关系。拒绝无效假设的典型$p$值是$5\%$或$1\%$，当$n=30$时，相应的$t$统计量分别为$2$和$2.75$。<sup id="fnref:t-and-p-means"><a href="#fn:t-and-p-means" class="footnote">2</a></sup></p>

<p>利用R的回归模型，可以得到如下的结果：</p>

<div class="highlight"><pre><code class="language-R">advertising <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&#39;Advertising.csv&#39;</span><span class="p">)</span>
lm.advertising <span class="o">&lt;-</span> lm<span class="p">(</span>Sales <span class="o">~</span> TV<span class="p">,</span> data <span class="o">=</span> advertising<span class="p">)</span>
<span class="kp">summary</span><span class="p">(</span>lm.advertising<span class="p">)</span>

<span class="c1"># 结果如下：</span>
Call<span class="o">:</span>
lm<span class="p">(</span>formula <span class="o">=</span> Sales <span class="o">~</span> TV<span class="p">,</span> data <span class="o">=</span> advertising<span class="p">)</span>

Residuals<span class="o">:</span>
    Min      <span class="m">1</span>Q  Median      <span class="m">3</span>Q     Max 
<span class="m">-8.3860</span> <span class="m">-1.9545</span> <span class="m">-0.1913</span>  <span class="m">2.0671</span>  <span class="m">7.2124</span> 

Coefficients<span class="o">:</span>
            Estimate Std. Error t value Pr<span class="p">(</span><span class="o">&gt;|</span><span class="kp">t</span><span class="o">|</span><span class="p">)</span>    
<span class="p">(</span>Intercept<span class="p">)</span> <span class="m">7.032594</span>   <span class="m">0.457843</span>   <span class="m">15.36</span>   <span class="o">&lt;</span><span class="m">2e-16</span> <span class="o">***</span>
TV          <span class="m">0.047537</span>   <span class="m">0.002691</span>   <span class="m">17.67</span>   <span class="o">&lt;</span><span class="m">2e-16</span> <span class="o">***</span>
<span class="o">---</span>
Signif. codes<span class="o">:</span>  <span class="m">0</span> ‘<span class="o">***</span>’ <span class="m">0.001</span> ‘<span class="o">**</span>’ <span class="m">0.01</span> ‘<span class="o">*</span>’ <span class="m">0.05</span> ‘<span class="m">.</span>’ <span class="m">0.1</span> ‘ ’ <span class="m">1</span>

Residual standard error<span class="o">:</span> <span class="m">3.259</span> on <span class="m">198</span> degrees of freedom
Multiple R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.6119</span><span class="p">,</span>    Adjusted R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.6099</span> 
<span class="bp">F</span><span class="o">-</span>statistic<span class="o">:</span> <span class="m">312.1</span> on <span class="m">1</span> and <span class="m">198</span> DF<span class="p">,</span>  p<span class="o">-</span>value<span class="o">:</span> <span class="o">&lt;</span> <span class="m">2.2e-16</span></code></pre></div>

<p>Coefficients的最后一列$Pr(&gt;|t|)$表示$H_0$成立的概率，若很小就拒绝$H_0$接受$H_1$。拒绝$H_0$以为着$\beta_1\neq0$，表明$X$与$Y$相关。星号表示相关性强弱。最后一列表明销售额与电视广告有关系。$RSE$衡量了模型拟合结果与数据真实表现的差异。</p>

<p>$RSE$衡量了模型拟合结果与数据真实表现的差异。$R^2$统计量是度量拟合的另一种方式<sup id="fnref:tss-computation"><a href="#fn:tss-computation" class="footnote">3</a></sup>，
\begin{equation}
R^2 = {TSS-RSS\over TSS}=1-{RSS\over TSS}，\qquad TSS=\sum_{i=1}^n\left(y_i-\bar y\right)^2，
\label{eq:r2-statistic}
\end{equation}
TSS也记为SST。$R^2$越大越好。加入新的变量会使得$R^2$增大<sup id="fnref:R2-increase"><a href="#fn:R2-increase" class="footnote">4</a></sup>，在测试集上$R^2$可能为负数，$R^2\in(-\infty, 1]$。若$R^2$接近$0$，这表明回归模型没有很好的解释响应变化，可能是线性模型有误或内在误差$\sigma^2$很高。本例中，$R^2\approx 0.61$表示少于三分之二的销售额变化能通过基于电视广告的回归模型解释。虽然$R^2\in[0,1]$，要比较哪个$R^2$更好还要根据具体应用而定。</p>

<p>$R^2$是衡量$X$与$Y$线性相关的统计量。相关系数（correlation）
\begin{equation}
r=Cor(X,Y)={
    \sum_{i=1}^n\left(x_i-\bar x\right)\left(y_i-\bar y\right)
    \over
    \sqrt{\sum_{i=1}^n\left(x_i-\bar x\right)^2}\sqrt{\sum_{i=1}^n\left(y_i-\bar y\right)^2}
}
\label{eq:correlation}
\end{equation}
也是衡量$X$与$Y$线性相关的统计量。事实上，$R^2=r^2$。</p>

<h2 id="section-2">多变量回归分析</h2>

<p>$p$个不同预测变量（predictor）的多变量回归模型为</p>

<p>\begin{equation}
Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p+\epsilon。
\label{eq:population-regression-line2}
\end{equation}</p>

<p>这些预测变量对响应的贡献如何呢？</p>

<p>无效假设和备选假设分别为
\[
\begin{aligned}
H_0&amp;:\beta_1=\beta_2=\cdots=\beta_p=0\\
H_a&amp;:\mbox{at least one }\beta_j\mbox{ is non-zero}。
\end{aligned}
\]</p>

<p>假设检验利用$F$统计量
\begin{equation}
F={(TSS-RSS)/p\over RSS/(n-p-1)}。
\label{eq:f-statistic}
\end{equation}</p>

<p>若线性模型的假设成立，那么有</p>

<p>\[
E\{RSS/(n-p-1)\}=\sigma^2，
\]</p>

<p>如果$H_0$同时也为真，那么有</p>

<p>\[
E\{(TSS-RSS)/p\}=\sigma^2。
\]</p>

<p>当预测变量和响应之间没有关系时，$F\approx 1$。若$H_a$为真，那么$E\{(TSS-RSS)/p\}&gt;\sigma^2$，因此$F &gt; 1$。</p>

<div class="highlight"><pre><code class="language-R">advertising <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&#39;Advertising.csv&#39;</span><span class="p">)</span>
lm.advertising <span class="o">&lt;-</span> lm<span class="p">(</span>Sales <span class="o">~</span> TV <span class="o">+</span> Radio <span class="o">+</span> Newspaper<span class="p">,</span> data <span class="o">=</span> advertising<span class="p">)</span>
<span class="kp">summary</span><span class="p">(</span>lm.advertising<span class="p">)</span>

<span class="c1"># 结果如下：</span>
Call<span class="o">:</span>
lm<span class="p">(</span>formula <span class="o">=</span> Sales <span class="o">~</span> TV <span class="o">+</span> Radio <span class="o">+</span> Newspaper<span class="p">,</span> data <span class="o">=</span> advertising<span class="p">)</span>

Residuals<span class="o">:</span>
    Min      <span class="m">1</span>Q  Median      <span class="m">3</span>Q     Max 
<span class="m">-8.8277</span> <span class="m">-0.8908</span>  <span class="m">0.2418</span>  <span class="m">1.1893</span>  <span class="m">2.8292</span> 

Coefficients<span class="o">:</span>
             Estimate Std. Error t value Pr<span class="p">(</span><span class="o">&gt;|</span><span class="kp">t</span><span class="o">|</span><span class="p">)</span>    
<span class="p">(</span>Intercept<span class="p">)</span>  <span class="m">2.938889</span>   <span class="m">0.311908</span>   <span class="m">9.422</span>   <span class="o">&lt;</span><span class="m">2e-16</span> <span class="o">***</span>
TV           <span class="m">0.045765</span>   <span class="m">0.001395</span>  <span class="m">32.809</span>   <span class="o">&lt;</span><span class="m">2e-16</span> <span class="o">***</span>
Radio        <span class="m">0.188530</span>   <span class="m">0.008611</span>  <span class="m">21.893</span>   <span class="o">&lt;</span><span class="m">2e-16</span> <span class="o">***</span>
Newspaper   <span class="m">-0.001037</span>   <span class="m">0.005871</span>  <span class="m">-0.177</span>     <span class="m">0.86</span>    
<span class="o">---</span>
Signif. codes<span class="o">:</span>  <span class="m">0</span> ‘<span class="o">***</span>’ <span class="m">0.001</span> ‘<span class="o">**</span>’ <span class="m">0.01</span> ‘<span class="o">*</span>’ <span class="m">0.05</span> ‘<span class="m">.</span>’ <span class="m">0.1</span> ‘ ’ <span class="m">1</span>

Residual standard error<span class="o">:</span> <span class="m">1.686</span> on <span class="m">196</span> degrees of freedom
Multiple R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.8972</span><span class="p">,</span>    Adjusted R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.8956</span> 
<span class="bp">F</span><span class="o">-</span>statistic<span class="o">:</span> <span class="m">570.3</span> on <span class="m">3</span> and <span class="m">196</span> DF<span class="p">,</span>  p<span class="o">-</span>value<span class="o">:</span> <span class="o">&lt;</span> <span class="m">2.2e-16</span></code></pre></div>

<p>上例中$F\approx 570$，远大于$1$，这表明销售额至少与一种媒体广告有关。</p>

<p>通过$F$拒绝$H_0$还需要考虑$n$和$p$的值。当$n$很大时，略大于$1$的$F$值也可拒绝$H_0$；当$n$很小时，$F$的值远大于$1$才可拒绝$H_0$。如果$H_0$为真，$\epsilon_i$是正态分布，那么$F$统计量服从$F$分布。当$n$和$p$已知，可以利用$F$分布计算$p$值（$p$-value），然后根据$p$值判断是否拒绝$H_0$。上例中，最后一个参数的$p$-value$\approx 0$，这表明销售额至少与一种媒体广告有关。</p>

<p>有时需要测试其中的$q$个系数是否为$0$，</p>

<p>\[
H_0:\beta_{p-q+1}=\beta_{p-q+2}=\cdots=\beta_p=0，
\]</p>

<p>这可以通过对需忽略的$q$个数据排在最后实现。在这种情况下，线性回归模型需要去除最后的$q$个数据，此时的残差平方和记为$RSS_0$，那么$F$统计量为</p>

<p>\begin{equation}
F={(RSS_0-RSS)/q\over RSS/(n-p-1)}。
\label{eq:f-statistic2}
\end{equation}</p>

<p>单变量回归表明报纸广告与销售额也有关，但多变量回归表明报纸广告与销售额关系不大。对于单变量和多变量回归，电视的系数很相近，但是报纸的系数差异很大。虽然报纸对销售额的直接贡献很小，但从下面的相关系数可以看出，报纸和广播的相关系数高达$0.35$，也就是报纸投放的广告多广播也会多，从而报纸能间接提高销售额<sup id="fnref:absurd-example"><a href="#fn:absurd-example" class="footnote">5</a></sup>。报纸可作为广播广告的替代品。报纸是通过广播对销售额的促进而获益的。</p>

<div class="highlight"><pre><code class="language-R"><span class="o">&gt;</span> cor<span class="p">(</span>advertising<span class="p">[,</span><span class="m">2</span><span class="o">:</span><span class="m">5</span><span class="p">])</span>

                  TV      Radio  Newspaper     Sales
TV        <span class="m">1.00000000</span> <span class="m">0.05480866</span> <span class="m">0.05664787</span> <span class="m">0.7822244</span>
Radio     <span class="m">0.05480866</span> <span class="m">1.00000000</span> <span class="m">0.35410375</span> <span class="m">0.5762226</span>
Newspaper <span class="m">0.05664787</span> <span class="m">0.35410375</span> <span class="m">1.00000000</span> <span class="m">0.2282990</span>
Sales     <span class="m">0.78222442</span> <span class="m">0.57622257</span> <span class="m">0.22829903</span> <span class="m">1.0000000</span></code></pre></div>

<p>利用多变量回归分析每个变量的$t$统计量和$p$值，可以分析它们对销售额的影响，这等价于$q=1$时分别剔除每个变量的$F$统计量。这揭示了加入变量到模型的<strong>偏效应</strong>（partial effect）<sup id="fnref:partial-effect"><a href="#fn:partial-effect" class="footnote">6</a></sup>。$p$值表明电视和广播的广告与销售额相关，但是没有证据表明在有电视和广播的广告时，报纸广告与销量相关。</p>

<p>为什么有了每个$p$值还需要全局的$F$统计量？如果存在一个很小的$p$值，那么至少有一个变量与响应相关——这个逻辑是有瑕疵的，特别是变量个数$p$很大时。例如：当$p=100$时，$H_0$为真，事实上没有那个变量与响应相关。在这种情况下，仍然大约$5\%$变量的$p$值会小于$0.05$，也就是即使在变量与响应无关的情况下也可能发现大约5个很小的$p$值，几乎可以保证至少存在一个$p$值小于$0.05$。因此，用每个变量对应的$t$统计量和$p$值判断变量与响应是否相关很可能得到错误的结果。但是，$F$统计量不存在这样的问题。如果$H_0$为真，不论变量有多少，只有$5\%$的机会使得$F$统计量会导致存在一个小于$5\%$的$p$值。</p>

<p>用$F$统计量判断变量与响应是否相关的条件是$p$（变量数）较小，一定要比$n$小。当$p&gt;n$时，不能用最小二乘法拟合回归模型，因此也不能使用$F$统计量和前文讨论的其它概念。当$p$很大时可以采用<strong>前向选择</strong>（forward selection）。</p>

<p>好的回归模型要求$F$较大，$R^2$较大，在这种情况下，小的$p$值表明变量与响应相关。</p>

<h2 id="section-3">编程实例</h2>

<h3 id="r">R处理因子类型变量</h3>

<p>数据集中<a href="https://courses.edx.org/c4x/MITx/15.071x_2/asset/pisa2009test.csv">pisa2009test.csv</a>和<a href="https://courses.edx.org/c4x/MITx/15.071x_2/asset/pisa2009train.csv">pisa2009train.csv</a>中，<code>raceeth</code>的数据类型是因子，线性模型不能直接使用该数据。<code>raceeth</code>有7种取值<code>White</code>、<code>American Indian/Alaska Native</code>、<code>Asian</code>、<code>Black</code>、<code>Hispanic</code>、<code>More than one race</code>、<code>Native Hawaiian/Other Pacific Islander</code>。R会选择出现频率最高的作为参考，本例中为<code>White</code>，自动将<code>raceeth</code>用6个变量替代，形如<code>raceethAmerican Indian/Alaska Native</code>。如果<code>raceeth</code>原来取值为<code>White</code>，那么新的6个变量取值均为<code>0</code>；如果原来取值为<code>American Indian/Alaska Native</code>，那么新变量<code>raceethAmerican Indian/Alaska Native</code>取值为<code>1</code>，其余5个取值为<code>0</code>。这也是一种<strong>特征变换</strong>策略。</p>

<div class="highlight"><pre><code class="language-R">pisaTrain <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&#39;Data/pisa2009train.csv&#39;</span><span class="p">)</span>
pisaTest <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&#39;Data/pisa2009test.csv&#39;</span><span class="p">)</span>

<span class="c1"># 忽略NA数据</span>
pisaTrain <span class="o">&lt;-</span> na.omit<span class="p">(</span>pisaTrain<span class="p">)</span>
pisaTest <span class="o">&lt;-</span> na.omit<span class="p">(</span>pisaTest<span class="p">)</span>

<span class="c1"># White出现频率最高，排在最前面</span>
pisaTrain<span class="o">$</span>raceeth <span class="o">=</span> relevel<span class="p">(</span>pisaTrain<span class="o">$</span>raceeth<span class="p">,</span> <span class="s">&quot;White&quot;</span><span class="p">)</span>
pisaTest<span class="o">$</span>raceeth <span class="o">=</span> relevel<span class="p">(</span>pisaTest<span class="o">$</span>raceeth<span class="p">,</span> <span class="s">&quot;White&quot;</span><span class="p">)</span>

<span class="c1"># 带因子（factor）变量的线性回归</span>
lmScore <span class="o">&lt;-</span> lm<span class="p">(</span>readingScore <span class="o">~</span> <span class="m">.</span><span class="p">,</span> data <span class="o">=</span> pisaTrain<span class="p">)</span></code></pre></div>

<h3 id="step"><code>step</code>逐步回归</h3>

<p>R中的<code>step</code>是<code>MASS</code>包中<code>stepAIC</code>函数的简版，它通过AIC准则进行模型选择，可以得到减缓的模型。</p>

<div class="highlight"><pre><code class="language-R"><span class="c1"># 接上例</span>
lmScoreSimple <span class="o">&lt;-</span> step<span class="p">(</span>lmScore<span class="p">)</span></code></pre></div>

<h2 id="section-4">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-5">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:confidence-interval">
      <p>Approximately for several reasons. Equation relies on the assumption that the errors are Gaussian. Also, the factor of $2$ in front of the $SE\left(\hat\beta_1\right)$ term will vary slightly depending on the number of observations $n$ in the linear regression. To be precise, rather than the number $2$, Equation should contain the $97.5\%$ quantile of a $t$-distribution with $n−2$ degrees of freedom.  <a href="#fnref:confidence-interval" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:t-and-p-means">
      <p>如何理解$t$分布、$p$值？ <a href="#fnref:t-and-p-means" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tss-computation">
      <p>在测试集上计算TSS时，$\bar y$是在训练集上计算的。 <a href="#fnref:tss-computation" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:R2-increase">
      <p>The model’s $R^2$ value can never decrease from adding new variables to the model. This is due to the fact that it is always possible to set the coefficient for the new variable to zero in the new model. However, this would be the same as the old model. So the only reason to make the coefficient non-zero is if it improves the $R^2$ value of the model, since linear regression picks the coefficients to minimize the error terms, which is the same as maximizing the $R^2$. <a href="#fnref:R2-increase" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:absurd-example">
      <p>This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspaper. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature. <a href="#fnref:absurd-example" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:partial-effect">
      <p>偏效应：回归模型中的其他因素保持不变时，某个解释变量对因变量的影响。 <a href="#fnref:partial-effect" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>模式发掘（2）：高效的模式挖掘方法</title>
      <link href="http://qianjiye.de/2015/03/efficient-pattern-mining-methods" />
      <pubdate>2015-03-07T23:11:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/efficient-pattern-mining-methods</guid>
      <content:encoded>&lt;![CDATA[<h2 id="apriori">Apriori性质</h2>

<p><strong>向下闭包</strong>（downward closure）特性是指频繁项集的所有非空子集也必须是频繁的，这也称为<strong>Apriori性质</strong>。若$\{beer, diaper, nuts\}$是频繁的，那么$\{beer, diaper\}$也是，每个包含$\{beer, diaper, nuts\}$的事务必包含$\{beer, diaper\}$。</p>

<p>如果项集$S$的存在非频繁的子集，那么$S$必不是频繁的。如果项集是非频繁的，它的超集（superset）必是非频繁的，这称为<strong>Apriori剪枝原理</strong>（Apriori pruning principle）。基于这个原理，主要有三种可扩展的（scalable）频繁模式挖掘方法：Apriori、Eclat和FPgrowth。</p>

<h2 id="apriori-1">Apriori算法</h2>

<p>Apriori算法是第一个基于候选集生成与测试的频繁集挖掘方法。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png"><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png" alt="Apriori算法" /></a><div class="caption">图 1:  Apriori算法 [<a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-algorithm.png">PNG</a>]</div></div></div>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png"><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png" alt="Apriori算法示例" /></a><div class="caption">图 2:  Apriori算法示例 [<a href="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-example.png">PNG</a>]</div></div></div>

<p>Airport算法生成候选项集的集合分两步：</p>

<ol>
  <li>自连接（self-join）$F_k$生成候选项集的集合；</li>
  <li>剪枝，删除候选项集的集合中的非频繁模式。</li>
</ol>

<p>若频繁模式的集合$F_3=\{abc,abd,acd,ace,bcd\}$，自连接可得候选模式的集合$C’_4=\{abcd, acde\}$，但是$ade\notin F_3$，因此剪枝后可得$C_4=\{abcd\}$。</p>

<p>基于SQL语句生成候选项集的集合：</p>

<p><img src="/assets/images/2015-03-07-efficient-pattern-mining-methods-apriori-SQL.png" alt="SQL implementation" /></p>

<h2 id="section">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-1">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>模式发掘（1）：基本概念</title>
      <link href="http://qianjiye.de/2015/03/pattern-discovery-basic-concepts" />
      <pubdate>2015-03-07T19:11:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/pattern-discovery-basic-concepts</guid>
      <content:encoded>&lt;![CDATA[<p><strong>模式</strong>（pattern）是数据集中频繁出现或强相关的条目、子序列或子结构组成的集合，它代表了数据集的本质特征和重要属性。<strong>模式发掘</strong>（pattern discovery）就是从大规模数据集中发现模式。</p>

<h2 id="section">模式发掘的重要性</h2>

<p>模式发掘的典型应用场景包括：</p>

<ul>
  <li>哪些商品常被一起购买？</li>
  <li>买了iPad之后还会买哪些商品？</li>
  <li>哪些代码片段可能包括复制粘贴错误（copy-and-paste bug）？</li>
  <li>语料库中哪些词串可能构成短语？</li>
</ul>

<p>模式发掘是从数据集中发现内在规则，在数据挖掘中有广泛应用：</p>

<ul>
  <li>关联分析、相关分析以及因果分析；</li>
  <li>挖掘序列模式、结构（例如：子图）模式；</li>
  <li>分析时空数据、多媒体、时间序列和留数据中的模式；</li>
  <li>基于判别模式分析的分类；</li>
  <li>基于模式子空间的聚类。</li>
</ul>

<p>模式发现广泛应用于购物篮分析（market basket analysis）、交叉营销（cross-marketing）、目录设计（catalog design）、销售活动分析（sale campaign analysis）、Web日志分析（Web log analysis）和生物序列分析（biological sequence analysis）。</p>

<h2 id="a-hrefagrawal1993mar1700361700721aa-hrefhan2007frequent2a">频繁模式与关联规则<a href="#Agrawal:1993:MAR:170036.170072">[1]</a><a href="#han2007frequent">[2]</a></h2>

<p>一个或多个项组成的集合称为<strong>项集</strong>（itemset），$k$-项集记为$ X=\{ x_1,\ldots,x_k\}$。<strong>绝对支持度</strong>（absolute support）是指项集$  X$在事务中出现的次数，<strong>相对支持度</strong>（relative support）是指包含项集$  X$的事务所占比率。如果项集$X$的支持度大于某个$minsup$阈值$\sigma$，则称$X$是<strong>频繁模式</strong>。</p>

<p><img src="/assets/images/2015-03-07-pattern-discovery-basic-concepts-itemset-table.png" alt="项集表格" /></p>

<p>令$minsup=50\%$，那么：</p>

<ul>
  <li>频繁的$1$-项集包括：$Beer: 3(60\%), Nuts: 3(60\%), Diaper: 4(80\%), Eggs:3(60\%)$；</li>
  <li>频繁的$2$-项集包括：$\{Beer, Diaper\}:3(60\%)$。</li>
</ul>

<p><strong>关联规则</strong>（association rule）记为$  X\rightarrow  Y(s,c)$，支持度$s$为包含项集$  X\cup  Y$事务出现的概率，<strong>置信度</strong>（confidence）$c$为包含项集$  X$的事务中包含项集$  X\cup  Y$的事务出现的概率，$c={\sup(  X\cup  Y)\over\sup(  X)}$。</p>

<p><strong>关联规则挖掘</strong>是找出大于最小支持度和置信度的所有关联规则$  X\rightarrow  Y$。令$minconf=50\%$，关联规则为
\[
Beer\rightarrow Diapper(60\%, 100\%),~Diaper\rightarrow Beer(60\%,75\%)。
\]</p>

<h2 id="section-1">闭模式与最大模式</h2>

<p>在频繁模式和关联规则挖掘时，一个长的模式包含很多子模式。</p>

<p>若模式（项集）$  X$是频繁的，并且不存在与$  X$有相同支持度的超模式（super-pattern）$  Y\supset  X$，则称$  X$为<strong>闭模式</strong>（closed pattern）<a href="#Pasquier1999discovering">[3]</a>。闭模式是频繁模式的无损压缩，虽降低了模式数量，但没有损失支持度信息。</p>

<p>若模式$X$是频繁的，并且不存在频繁的超模式$Y\supset X$，则称$X$为<strong>最大模式</strong>（max-pattern）<a href="#roberto667815">[4]</a>。最大模式不关注子模式的真实支持度，它是频繁模式的有损压缩，只知道子模式是频繁的，但不知道子模式的真实支持度。</p>

<p>令事务集$TDB_1$的事务为$T_1:\{a_1,\ldots,a_{50}\},T_2:\{a_1,\ldots,a_{100}\}$并且$minsup=1$，那么$TDB_1$只有两个闭模式$\{a_1,\ldots,a_{50}\}:2,\;\{a_1,\ldots,a_{100}\}:1$和唯一的最大模式$\{a_1,\ldots,a_{100}\}:1$。</p>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="Agrawal:1993:MAR:170036.170072">[1]R. Agrawal, T. Imieliński, and A. Swami, “Mining Association Rules Between Sets of Items in Large Databases,” <i>Sigmod Record</i>, vol. 22, no. 2, pp. 207–216, Jun. 1993.</span>

[<a href="http://doi.acm.org/10.1145/170036.170072">Online</a>]

</li>
<li><span id="han2007frequent">[2]J. Han, H. Cheng, D. Xin, and X. Yan, “Frequent pattern mining: current status and future directions,” <i>Data Mining and Knowledge Discovery</i>, vol. 15, no. 1, pp. 55–86, 2007.</span>

</li>
<li><span id="Pasquier1999discovering">[3]N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, “Discovering Frequent Closed Itemsets for Association Rules,” in <i>International Conference on Database Theory</i>, 1999, pp. 398–416.</span>

</li>
<li><span id="roberto667815">[4]R. J. Bayardo, “Efficiently Mining Long Patterns from Databases,” <i>Sigmod Record</i>, vol. 27, no. 2, pp. 85–93, 1998.</span>

</li></ol>

<h3 id="section-3">脚注</h3>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>R Essential</title>
      <link href="http://qianjiye.de/2015/03/r-essential" />
      <pubdate>2015-03-06T02:36:23+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/03/r-essential</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">基本操作</h2>

<h4 id="section-1">工作环境</h4>

<table>
  <thead>
    <tr>
      <th>命令</th>
      <th>功能</th>
      <th>例子</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>?"*"</code></td>
      <td>查看*的帮组文档</td>
      <td><code>?"data.frame"</code></td>
    </tr>
    <tr>
      <td><code>??"*"</code></td>
      <td>在帮助文档中搜索*</td>
      <td><code>??"network"</code></td>
    </tr>
    <tr>
      <td><code>ls()</code></td>
      <td>查看可用对象</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>rm()</code></td>
      <td>移除对象</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>getwd()</code></td>
      <td>查看工作目录</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>setwd(dir)</code></td>
      <td>设置工作目录</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><code class="language-R"><span class="c1">#删除当前环境中所有对象</span>
<span class="kp">rm</span><span class="p">(</span><span class="kt">list</span> <span class="o">=</span> <span class="kp">ls</span><span class="p">())</span></code></pre></div>

<h2 id="section-2">基本类型</h2>

<p>R不能直接存取内存，通过称之为<strong>对象</strong>（object）的特殊数据结构存取内存。这些对象通过符号（symbol）或变量（variable）访问，符号本身也是对象。R的<code>typeof</code>函数返回对象的类型<sup id="fnref:R-type"><a href="#fn:R-type" class="footnote">1</a></sup>。<a href="http://cran.r-project.org/doc/manuals/r-release/R-lang.html#Objects">R的对象类型</a>包括<code>NULL</code>、<code>closure</code>、<code>integer</code>、<code>complex</code>等。</p>

<p><code>mode</code>函数返回对象的模式（mode），主要是为了和S语言的其它实现兼容。<code>storage.mode</code>函数返回参数的存储模式（storage mode），通常用于调用C或FORTRAN时，确定数据类型。在S语言中，整数和实数向量都是<code>numeric</code>模式，但存储模式不同。</p>

<h3 id="section-3">基本类型</h3>

<p><strong>向量</strong>（vector）是最基本数据结构，R包含6种基本的原子向量（atomic vector）：</p>

<table>
  <thead>
    <tr>
      <th>typeof</th>
      <th>mode</th>
      <th>storage.mode</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>logical</td>
      <td>logical</td>
      <td>logical</td>
    </tr>
    <tr>
      <td>integer</td>
      <td>numeric</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>double</td>
      <td>numeric</td>
      <td>double</td>
    </tr>
    <tr>
      <td>complex</td>
      <td>complex</td>
      <td>complex</td>
    </tr>
    <tr>
      <td>character</td>
      <td>character</td>
      <td>character</td>
    </tr>
    <tr>
      <td>raw</td>
      <td>raw</td>
      <td>raw</td>
    </tr>
  </tbody>
</table>

<p>单个数和字符串也是长度为1的向量，例如4.2和“four point two”。原子向量中的每个元素类型必须相同。向量的长度可为0。</p>

<p><strong>列表</strong>（list）称为通用向量（generic vector），每个元素可以是不同数据类型。虽然列表也是向量，但通常说的向量是不包括列表的原子向量。</p>

<p>R语言由3种不同类型的对象构成，它们是calls、expressions和names，这些对象的模式分别为<code>call</code>、<code>expression</code>和<code>name</code>，可通过<code>as.list</code>和<code>as.call</code>在列表和<strong>语言对象</strong>之间转换。<strong>符号</strong>(symbol)指向R的对象，R对象的名字通常是一个符号。符号能够通过函数<code>as.name</code>和<code>quote</code>创建。符号的模式为<code>name</code>，存储模式和类型都为<code>symbol</code>，可通过<code>as.character</code>和<code>as.name</code>与字符串转换。</p>

<p>R的expression对象包含一个或多个语句（statement）。R的expression对象只有在显示调用<code>eval</code>时才被执行。expression对象与列表相似，访问元素的方法与列表一样。</p>

<p><strong>函数对象</strong>包含三个元素：参数列表、函数体和环境。参数列表用逗号分割，<code>…</code>可包含任意个数的参数。函数体通常是包含在大括号中的R语句。环境在函数创建时激活。这三部分可通过函数<code>formals</code>、<code>body</code>和<code>environment</code>操作。<code>as.list</code>和<code>as.function</code>函数可进行列表和函数的转换。</p>

<p>NULL是特殊的对象，与长度为0的向量不同，R中只有唯一的NULL对象。</p>

<h3 id="section-4">对象属性</h3>

<p>除NULL外的所有对象都可包含一个或多个属性。属性以成对列表（pairlist）方式存储，所有元素都有名字。通过<code>attributes</code>查看属性列表，通过<code>attr</code>获取单个属性。一些特殊的属性有特定的存取函数，例如因子的<code>levels</code>函数，数组的<code>dim</code>和<code>dimnames</code>函数。属性被用于实现R的类结构。If an object has a class attribute then that attribute will be examined during evaluation.</p>

<h2 id="section-5">数据结构</h2>

<p>R的数据结构包括数组（array）和列表（list）。数组的所有元素类型必须一致，列表元素的类型可以不同。向量（vector）是一维素组，矩阵（matrix）是二维数组。数据框（data frame）是特殊的列表。</p>

<p>数据的索引（indexing）有<code>[]</code>、<code>[[]]</code>和<code>$</code>三种方法，形如<code>x[i]</code>、<code>x[i, j]</code>、<code>x[[i]]</code>、<code>x[[i, j]]</code>、<code>x$a</code>和<code>x$”a”</code>。</p>

<ul>
  <li>对向量和矩阵，<code>[[]]</code>会抛弃所有<code>names</code>和<code>dimnames</code>属性，<code>[]</code>则不会；</li>
  <li>对列表，<code>[[]]</code>返回列表，<code>[]</code>返回列表的元素；</li>
  <li><code>[[]]</code>只能对单个元素索引，<code>[]</code>可以通过向量索引多个元素；</li>
  <li><code>$</code>只能用于列表索引，索引参数不能被计算（如果索引参数需要计算，采用<code>x[[expr]]</code>），索引的对象不存在时返回<code>NULL</code>。</li>
</ul>

<p>数据的索引从1开始，索引前加负号<code>-</code>表示删除这些索引的元素。</p>

<h3 id="section-6">数据分割与合并</h3>

<table>
  <thead>
    <tr>
      <th>命令</th>
      <th>功能</th>
      <th>备注</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>rbind</code></td>
      <td>行合并</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>cbind</code></td>
      <td>列合并</td>
      <td> </td>
    </tr>
    <tr>
      <td><code>merge</code></td>
      <td>列合并</td>
      <td>用于data frame</td>
    </tr>
    <tr>
      <td><code>subset</code></td>
      <td>抽取子集</td>
      <td><code>subset(airquality, Temp &gt; 80 &amp; is.na(Solar.R))</code></td>
    </tr>
    <tr>
      <td><code>append</code></td>
      <td>插入元素</td>
      <td>用于向量</td>
    </tr>
    <tr>
      <td><code>stack</code></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><code>split</code></td>
      <td>数据分组</td>
      <td>用于向量</td>
    </tr>
  </tbody>
</table>

<div class="highlight"><pre><code class="language-R">airquality<span class="o">$</span>Temp2 <span class="o">&lt;-</span> airquality<span class="o">$</span>Temp <span class="c1">#增加Temp2列</span>
airquality<span class="o">$</span>Temp <span class="o">&lt;-</span> <span class="kc">NULL</span> <span class="c1">#删除Temp列</span>

Top5 <span class="o">=</span> <span class="kp">subset</span><span class="p">(</span>mvt<span class="p">,</span> LocationDescription <span class="o">%in%</span> TopLocations<span class="p">)</span>
<span class="c1"># 删除多余无关的levels（Top5在执行了subset之后会包含整个集合的levels）</span>
Top5<span class="o">$</span>LocationDescription <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>Top5<span class="o">$</span>LocationDescription<span class="p">)</span>

CPS <span class="o">=</span> <span class="kp">merge</span><span class="p">(</span>CPS<span class="p">,</span> MetroAreaMap<span class="p">,</span> by.x<span class="o">=</span><span class="s">&quot;MetroAreaCode&quot;</span><span class="p">,</span> by.y<span class="o">=</span><span class="s">&quot;Code&quot;</span><span class="p">,</span> all.x<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span></code></pre></div>

<h2 id="section-7">运算符和优先级</h2>

<table>
  <thead>
    <tr>
      <th>Operator</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>[</code> <code>[[</code></td>
      <td>Indexing</td>
    </tr>
    <tr>
      <td><code>::</code> <code>:::</code></td>
      <td>Access variables in a name space</td>
    </tr>
    <tr>
      <td><code>$</code> <code>@</code></td>
      <td>Component extraction, slot extraction</td>
    </tr>
    <tr>
      <td><code>^</code></td>
      <td>Exponentiation (right to left)</td>
    </tr>
    <tr>
      <td><code>-</code> <code>+</code></td>
      <td>Unary minus and plus</td>
    </tr>
    <tr>
      <td><code>:</code></td>
      <td>Sequence creation</td>
    </tr>
    <tr>
      <td><code>%any%</code></td>
      <td>Special operators</td>
    </tr>
    <tr>
      <td><code>*</code> <code>/</code></td>
      <td>Multiplication, division</td>
    </tr>
    <tr>
      <td><code>+</code> <code>-</code></td>
      <td>Addition, subtraction</td>
    </tr>
    <tr>
      <td><code>==</code> <code>!=</code> <code>&lt;</code> <code>&gt;</code> <code>&lt;=</code> <code>&gt;=</code></td>
      <td>Comparison</td>
    </tr>
    <tr>
      <td><code>!</code></td>
      <td>Logical negation</td>
    </tr>
    <tr>
      <td><code>&amp;</code> <code>&amp;&amp;</code></td>
      <td>Logical “and”, short-circuit “and”</td>
    </tr>
    <tr>
      <td><code>|</code> <code>||</code></td>
      <td>Logical “or”, short-circuit “or”</td>
    </tr>
    <tr>
      <td><code>~</code></td>
      <td>Formula</td>
    </tr>
    <tr>
      <td><code>-&gt;</code> <code>-&gt;&gt;</code></td>
      <td>Rightward assignment</td>
    </tr>
    <tr>
      <td><code>=</code></td>
      <td>Assignment (right to left)</td>
    </tr>
    <tr>
      <td><code>&lt;-</code> <code>&lt;&lt;-</code></td>
      <td>Assignment (right to left)</td>
    </tr>
    <tr>
      <td><code>?</code></td>
      <td>Help</td>
    </tr>
  </tbody>
</table>

<p><code>%any%</code>运算符：</p>

<ul>
  <li><code>%%</code>：求模，<code>7%%4=3</code>；</li>
  <li><code>%/%</code>：整数除，<code>7%/%4=1</code>；</li>
  <li><code>%*%</code>：矩阵乘法；</li>
  <li><code>%o%</code>：外积；</li>
  <li><code>%x%</code>：Kronecker乘积；</li>
  <li><code>%in%</code>：匹配运算。</li>
</ul>

<h2 id="section-8">常用函数</h2>

<h3 id="section-9">查看数据统计特性</h3>

<h4 id="str"><code>str</code>：查看对象结构</h4>

<p>显示对象各属性。</p>

<h4 id="summary"><code>summary</code>：查看对象统计特性</h4>

<p>显示最小值、分位数、最大值等数据，可以通过<code>boxplot</code>直观的显示。</p>

<h4 id="tablefactor"><code>table</code>：按factor类型统计</h4>

<div class="highlight"><pre><code class="language-R"><span class="c1">#将Ozone（行）和Month（列）作为factor类型，统计同时满足两个条件样本的个数，以表格的形式显示。</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span> <span class="kp">table</span><span class="p">(</span>Ozone<span class="p">,</span> Month<span class="p">))</span> 

<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> Month<span class="p">,</span> useNA <span class="o">=</span> <span class="s">&quot;ifany&quot;</span><span class="p">))</span> <span class="c1">#若有NA则显示</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> Month<span class="p">,</span> useNA <span class="o">=</span> <span class="s">&quot;always&quot;</span><span class="p">))</span><span class="c1">#总是显示NA</span>
<span class="kp">with</span><span class="p">(</span>airquality<span class="p">,</span>
   <span class="kp">table</span><span class="p">(</span>OzHi <span class="o">=</span> Ozone <span class="o">&gt;</span> <span class="m">80</span><span class="p">,</span> <span class="kp">addNA</span><span class="p">(</span>Month<span class="p">)))</span>           <span class="c1">#增加一列NA</span></code></pre></div>

<h4 id="hist"><code>hist</code>：直方图</h4>

<div class="highlight"><pre><code class="language-R">hist<span class="p">(</span><span class="kp">sqrt</span><span class="p">(</span>islands<span class="p">),</span> breaks <span class="o">=</span> <span class="m">12</span><span class="p">,</span> col <span class="o">=</span> <span class="s">&quot;lightblue&quot;</span><span class="p">,</span> border <span class="o">=</span> <span class="s">&quot;pink&quot;</span><span class="p">)</span></code></pre></div>

<h4 id="boxplotbox-and-whisker"><code>boxplot</code>：盒须图（box-and-whisker）</h4>

<div class="highlight"><pre><code class="language-R">boxplot<span class="p">(</span>count <span class="o">~</span> spray<span class="p">,</span> data <span class="o">=</span> InsectSprays<span class="p">)</span> <span class="c1"># 横轴为spray，纵轴为count</span></code></pre></div>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-03-05-r-essential-boxplot.png"><img src="/assets/images/2015-03-05-r-essential-boxplot.png" alt="boxplot图示" /></a><div class="caption">图 1:  boxplot图示 [<a href="/assets/images/2015-03-05-r-essential-boxplot.png">PNG</a>]</div></div></div>

<p><a href="http://msenux.redwoods.edu/math/R/boxplot.php"><code>boxplot</code>绘制如上图</a>：</p>

<ul>
  <li>box中的粗横线表示中位数（median）；</li>
  <li>box的下边和上边分别表示第一和第三四分位数（quartile），二者之间的距离称为四分位数间距（IQR，inter-quartile range），分位数用<code>quantile</code>函数计算，IQR用<code>IQR</code>函数计算；</li>
  <li>最下和最上的横线分别表示最小和最大值；</li>
  <li>小圆圈表示离群点（outlier）。</li>
</ul>

<p>令$STEP = range\times IQR$（$range$可通过<code>boxplot</code>设置），区间$[first\; quartile - STEP, third\; quartile + STEP]$之外的是离群点，大于$first\; quartile - STEP$的最小值为图中的最小值（minimum），小于$third\; quartile + STEP$的最大值为图中的最大值（maximum），当<code>boxplot</code>的参数<code>range=0</code>时，不探测离群点，显示实际的最大和最小值。</p>

<h3 id="section-10">批处理函数</h3>

<h4 id="tapply"><code>tapply</code>：应用函数操作分组数据</h4>

<p><img src="/assets/images/2015-03-05-r-essential-tapplay.png" alt="tapply" /></p>

<div class="highlight"><pre><code class="language-R"><span class="c1"># 将warpbreaks[,-1]强制转换为factor类型；</span>
<span class="c1"># 通过warpbreaks[,-1]对warpbreaks$breaks分类；</span>
<span class="c1"># 将每类的warpbreaks$breaks作为函数sum的输入；</span>
<span class="c1"># ……</span>
<span class="kp">tapply</span><span class="p">(</span>warpbreaks<span class="o">$</span>breaks<span class="p">,</span> warpbreaks<span class="p">[,</span><span class="m">-1</span><span class="p">],</span> <span class="kp">sum</span><span class="p">)</span></code></pre></div>

<h3 id="section-11">其它</h3>

<div class="highlight"><pre><code class="language-R">DateConvert <span class="o">=</span> <span class="kp">as.Date</span><span class="p">(</span><span class="kp">strptime</span><span class="p">(</span>mvt<span class="o">$</span>Date<span class="p">,</span> <span class="s">&quot;%m/%d/%y %H:%M&quot;</span><span class="p">))</span> <span class="c1">#日期转换</span>
mvt<span class="o">$</span>Month <span class="o">=</span> <span class="kp">months</span><span class="p">(</span>DateConvert<span class="p">)</span>
mvt<span class="o">$</span>Weekday <span class="o">=</span> <span class="kp">weekdays</span><span class="p">(</span>DateConvert<span class="p">)</span></code></pre></div>

<h2 id="section-12">机器学习</h2>

<h3 id="section-13">数据集处理</h3>

<ul>
  <li><code>sample.split</code>：［caTools］将数据分为训练集合测试集；</li>
  <li><code>complete(mice())</code>：[mice]数据补齐；</li>
  <li>[CROC]：ROC曲线。</li>
</ul>

<h3 id="section-14">回归分析</h3>

<ul>
  <li><code>lm</code>：线性回归；</li>
  <li><code>glm</code>：logistic回归。</li>
</ul>

<h3 id="section-15">模型优化</h3>

<ul>
  <li><code>step</code>：机遇AIC的模型选择。</li>
</ul>

<h2 id="section-16">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-17">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:R-type">
      <p>Note that in the C code underlying R, all objects are pointers to a structure with typedef <code>SEXPREC</code>; the different R data types are represented in C by <code>SEXPTYPE</code>, which determines how the information in the various parts of the structure is used. <a href="#fnref:R-type" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>图像分类（3）：最优化</title>
      <link href="http://qianjiye.de/2015/02/image-classification-optimization" />
      <pubdate>2015-02-13T22:53:50+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/image-classification-optimization</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">理解损失函数</h2>

<p>图像分类的两个关键步骤包括：（1）通过<strong>评分函数</strong>将图像映射到类别评分；（2）通过<strong>损失函数</strong>度量评分。多分类支持向量机采用的是线性评分函数$f\left(\mathbf x_i,\mathbf W\right)=\mathbf W\mathbf x_i$，需要最小化的损失函数为
\[
L={1\over N}\sum_i\sum_{j\neq y_i}
\max\left(
0, f\left(\mathbf x_i,\mathbf W\right)_j-f\left(\mathbf x_i,\mathbf W\right)_{y_i}+1
\right)
+\alpha R(\mathbf W)。
\]</p>

<p>图像分类的第三个关键步骤：（3）通过最优化求解最小化损失函数的参数$\mathbf W$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg"><img src="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg" alt="损失函数图" /></a><div class="caption">图 1:  损失函数图 [<a href="/assets/images/2015-02-13-image-classification-optimization-loss-function-landscape.jpg">JPG</a>]</div></div></div>

<p>将$\mathbf W$视为空间中的一个点，损失函数（不含正则化项）可以用上图表示。$\mathbf W$、$\mathbf W_1$和$\mathbf W_2$是随机产生的矩阵。上图左表示损失函数$L(\mathbf W+a\mathbf W_1)$，横轴为$a$，纵轴为$L$；上图中和右表示损失函数$L(\mathbf W+a\mathbf W_1+b\mathbf W_2)$，横轴和纵轴分别为$a$和$b$，越红损失$L$越大，越蓝损失$L$越小。上图左和中，只计算一张图片的损失；上图右的碗状图是100张图片损失的平均值，相当于100张上图中的平均图。</p>

<p>每张图$i$的损失为（不含正则化项）
\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+1
\right)，
\]
它是分段线性（piecewise-linear）函数。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png"><img src="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png" alt="1维损失函数" /></a><div class="caption">图 2:  1维损失函数 [<a href="/assets/images/2015-02-13-image-classification-optimization-1d-loss.png">PNG</a>]</div></div></div>

<p>三张图片$\mathbf x_0$、$\mathbf x_1$和$\mathbf x_2$分别属于类0、1和2，它们的损失计算如下
\[
\begin{aligned}
L_0 &amp;= \max\left(0, \mathbf w_1^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_0-\mathbf w_{0}^\top\mathbf x_0+1\right)\\
L_1 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)+
\max\left(0, \mathbf w_2^\top\mathbf x_1-\mathbf w_{1}^\top\mathbf x_1+1\right)\\
L_2 &amp;= \max\left(0, \mathbf w_0^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)+
\max\left(0, \mathbf w_1^\top\mathbf x_2-\mathbf w_{2}^\top\mathbf x_2+1\right)\\
L &amp;= {1\over 3}\left(L_0+L_1+L_2\right)，
\end{aligned}
\]
如上图所示，横轴表示权值，纵轴表示损失。</p>

<p>多分类SVM的代价函数是凸函数的一个范例，当采用神经网络的评分函数$f$时，代价函数就是非凸的。损失函数不可微分（non-differentiable），因此梯度未定义，通常使用次梯度（subgradient）代替梯度。本文不区分梯度和次梯度。</p>

<p>损失函数可以评估任意权值$\mathbf W$，最优化的目标是找出最小化损失函数的权值$\mathbf W$。神经网络的优化不能方便地使用凸函数的优化工具。</p>

<h2 id="section-1">随机方法</h2>

<h3 id="section-2">随机搜索</h3>

<p>比较糟糕的优化策略是随机搜索（random search）。由于验证参数$\mathbf W$比较简单，随机搜索通过尝试不同的随机权值$\mathbf W$，从中选择最优的。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span> <span class="c"># Python assigns the highest possible float value</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.0001</span> <span class="c"># generate random parameters</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the loss over the entire training set</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span> <span class="c"># keep track of the best solution</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="n">bestW</span> <span class="o">=</span> <span class="n">W</span>
  <span class="k">print</span> <span class="s">&#39;in attempt </span><span class="si">%d</span><span class="s"> the loss was </span><span class="si">%f</span><span class="s">, best </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，随机搜索可以做到约15.5%的精度，优于随机猜想的10%。找到最优的$\mathbf W$很困难甚至不可行，但是找到更好一些的$\mathbf W$就不难么困难了。基于这个思路，优化算法可以从随机的$\mathbf W$开始，不断更新权值，使得每次更新都提升一点性能。随机搜索如同徒步者带上眼罩向山脚走。针对CIFAR-10数据集，这山有30730维，山上的每一点都相当于特定的损失值。</p>

<h3 id="section-3">随机局部搜索</h3>

<p>从随机初始化的$\mathbf W$开始，若增加扰动$\delta\mathbf W$，损失在$\mathbf W+\delta\mathbf W$比在$\mathbf W$时更低，那么更新$\mathbf W\leftarrow\mathbf W+\delta\mathbf W$。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># generate random starting W</span>
<span class="n">bestloss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&quot;inf&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.0001</span>
  <span class="n">Wtry</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">Xtr_cols</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Wtry</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="n">bestloss</span><span class="p">:</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">Wtry</span>
    <span class="n">bestloss</span> <span class="o">=</span> <span class="n">loss</span>
  <span class="k">print</span> <span class="s">&#39;iter </span><span class="si">%d</span><span class="s"> loss is </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">bestloss</span><span class="p">)</span></code></pre></div>

<p>在CIFAR-10数据集上，经过1000次尝试，精度提高到了21.4%。但这仍然是低效耗时的算法。</p>

<h2 id="section-4">梯度下降法</h2>

<p>随机搜索对寻找最佳的权值改变方向没有帮助。沿着最佳的方向改变权值，在数学上可以保证损失最速下降（steepest descend）。这个方向和梯度（gradient）相关。</p>

<p>一维函数的斜率（slope）是函数值在某点的瞬时（instantaneous）变化率。梯度是斜率在多维空间函数的推广，它是每维斜率构成的向量，通常也称为导数（derivative）。一维函数的导数为
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x)\over h}，
\]
将其推广到多变量函数时称为偏导数（partial derivative）。梯度就是每一维偏导数组成的向量，有两种计算方法：</p>

<ul>
  <li>数值梯度（numerical gradient）：近似计算，较慢，但容易；</li>
  <li>解析梯度（analytic gradient）：计算快，但是容易出错（error-prone）。</li>
</ul>

<h3 id="section-5">数值梯度</h3>

<p>梯度的数值计算方法如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">  a naive implementation of numerical gradient of f at x </span>
<span class="sd">  - f should be a function that takes a single argument</span>
<span class="sd">  - x is the point (numpy array) to evaluate the gradient at</span>
<span class="sd">  &quot;&quot;&quot;</span> 

  <span class="n">fx</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evaluate function value at original point</span>
  <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>

  <span class="c"># iterate over all indexes in x</span>
  <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;readwrite&#39;</span><span class="p">])</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

    <span class="c"># evaluate function at x+h</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
    <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c"># increment by h</span>
    <span class="n">fxh</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># evalute f(x + h)</span>
    <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="c"># restore to previous value (very important!)</span>

    <span class="c"># compute the partial derivative</span>
    <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh</span> <span class="o">-</span> <span class="n">fx</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span> <span class="c"># the slope</span>
    <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c"># step to next dimension</span>

  <span class="k">return</span> <span class="n">grad</span></code></pre></div>
<p>上述代码计算损失函数在$\mathbf x$每个维度的偏导数。在实际应用中通常使用中心差分公式（centered difference formula）
\[
{df(x)\over dx}=\lim_{h\rightarrow 0}{f(x+h)-f(x-h)\over 2h}。
\]</p>

<p>梯度只表明了最快增长的方向，还需要在这个方向前进的步长（也就是学习率）。步长是神经网络需要设定的重要超参数。</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># to use the generic code above we want a function that takes a single argument</span>
<span class="c"># (the weights in our case) so we close over X_train and Y_train</span>
<span class="k">def</span> <span class="nf">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3073</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c"># random weight vector</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">CIFAR10_loss_fun</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c"># get the gradient</span>

<span class="n">loss_original</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="c"># the original loss</span>
<span class="k">print</span> <span class="s">&#39;original loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_original</span><span class="p">,</span> <span class="p">)</span>

<span class="c"># lets see the effect of multiple step sizes</span>
<span class="k">for</span> <span class="n">step_size_log</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">step_size_log</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">df</span> <span class="c"># new position in the weight space</span>
  <span class="n">loss_new</span> <span class="o">=</span> <span class="n">CIFAR10_loss_fun</span><span class="p">(</span><span class="n">W_new</span><span class="p">)</span>
  <span class="k">print</span> <span class="s">&#39;for step size </span><span class="si">%f</span><span class="s"> new loss: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">loss_new</span><span class="p">)</span>

<span class="c"># prints:</span>
<span class="c"># original loss: 2.200718</span>
<span class="c"># for step size 1.000000e-10 new loss: 2.200652</span>
<span class="c"># for step size 1.000000e-09 new loss: 2.200057</span>
<span class="c"># for step size 1.000000e-08 new loss: 2.194116</span>
<span class="c"># for step size 1.000000e-07 new loss: 2.135493</span>
<span class="c"># for step size 1.000000e-06 new loss: 1.647802</span>
<span class="c"># for step size 1.000000e-05 new loss: 2.844355</span>
<span class="c"># for step size 1.000000e-04 new loss: 25.558142</span>
<span class="c"># for step size 1.000000e-03 new loss: 254.086573</span>
<span class="c"># for step size 1.000000e-02 new loss: 2539.370888</span>
<span class="c"># for step size 1.000000e-01 new loss: 25392.214036</span></code></pre></div>
<p>上述代码中，<code>step_size</code>表示步长（学习率），步长小损失函数减小慢，但步长太大损失函数不降反增。当损失函数有30730个参数时，每次更新参数需要计算30731次损失函数，计算复杂度非常高。</p>

<h3 id="section-6">解析梯度</h3>

<p>数值梯度虽简单但耗时，解析梯度计算高效但易错。在实际应用中，采用解析梯度时，通过比较它与数值梯度确定梯度计算是否正确，这称为<strong>梯度校验</strong>（gradient check）。</p>

<p>对每个数据，多分类SVM的损失函数为</p>

<p>\[
L_i=\sum_{j\neq y_i}\max\left(
0, \mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta
\right)，
\]</p>

<p>对$\mathbf w_{y_i}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{y_i}}L_i=
-\left(\sum_{j\neq y_i}\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\right)\mathbf x_i，
\]</p>

<p>对$\mathbf w_{j}$求偏导（梯度）</p>

<p>\[
\nabla_{\mathbf w_{j}}L_i=
\left[\left[\mathbf w_j^\top\mathbf x_i-\mathbf w_{y_i}^\top\mathbf x_i+\Delta&gt;0
\right]\right]\mathbf x_i。
\]</p>

<p>利用梯度更新参数的方法称为<strong>梯度下降法</strong>（gradient descent），通常的形式：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>梯度下降法是到目前为止最常用的优化神经网络损失函数的方法。</p>

<p>对大数据集上的应用，在整个数据集上计算损失函数的梯度非常耗时，通常从中抽取从中抽取一部分数据计算梯度，这称为mini-batch梯度下降法，例如采用256个样本计算梯度：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># Vanilla Minibatch Gradient Descent</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">data_batch</span> <span class="o">=</span> <span class="n">sample_training_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c"># sample 256 examples</span>
  <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data_batch</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c"># perform parameter update</span></code></pre></div>

<p>在实际中，采用mini-batch的方法能提高参数更新频率，更快收敛。若mini-batch梯度下降法只采用一个样本，称为随机梯度下降法（SGD，stochastic gradient descent）或在线（on-line）梯度下降法。但这不太常用，在实际中由于向量化代码的优化，计算100个样本的梯度比1个样本的梯度计算100次高效。虽然人们使用SGD这个称谓，实际通常指的是mini-batch的梯度下降法（MGD／BGD，minbatch/batch gradient descent）。min-batch的样本数量虽是超参数，但很少采用验证法确定，通常根据内存大小来决定，或者直接设定为100左右的值。</p>

<h2 id="bp">BP梯度</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit2.png" alt="BP梯度计算" /></a><div class="caption">图 3:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit2.png">PNG</a>]</div></div></div>

<p>对函数$f(x,y,z)=(x+y)z$，令$q=x+y$，那么根据链式法则（chain rule）有${\partial f\over \partial x}={\partial f\over \partial q}{\partial q\over \partial x}$。梯度${\partial f\over \partial x}$从右到左反向计算如上图“电路”所示，通过相邻节点的局部梯度链式相乘得到，每个节点用门（gate）表示。当${\partial f\over \partial q}=-4$和${\partial q\over \partial x}=1$时，${\partial f\over \partial x}=-4\times 1 = -4$。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit3.png" alt="BP梯度计算" /></a><div class="caption">图 4:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit3.png">PNG</a>]</div></div></div>

<p>当
$
f(\mathbf w,\mathbf x)={1\over 1 + e^{-\left(w_0x_0+w_1x_1+w_2\right)}}
$
时，链式计算如上如所示。链式法则的依据是复合函数的求导法则。门可以是任何可导函数（differentiable function），通常选择导数容易计算的函数作为门，如sigmoid函数$\sigma(x)={1\over 1 + e^{-x}}$。将${df\over dx}$简记为<code>dx</code>，${d\sigma(x)\over dx}=(1-\sigma(x))\sigma(x)$，上图的计算过程简化如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="c"># assume some random weights and data</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>

<span class="c"># forward pass</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">))</span> <span class="c"># sigmoid function</span>

<span class="c"># backward pass through the neuron (backpropagation)</span>
<span class="n">ddot</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">f</span> <span class="c"># gradient on dot variable, using the sigmoid gradient derivation</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into x</span>
<span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">ddot</span><span class="p">]</span> <span class="c"># backprop into w</span>
<span class="c"># we&#39;re done! we have the gradients on the inputs to the circuit</span></code></pre></div>

<p>对于复杂的函数，一次求导很复杂，如果采用链式法则，可以降低计算复杂度。对函数
\[
f(x,y)={x+\sigma(y)\over\sigma(x)+(x+y)^2}，
\]
前向计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># example values</span>
<span class="n">y</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="c"># forward pass</span>
<span class="n">sigy</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c"># sigmoid in numerator   #(1)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sigy</span> <span class="c"># numerator                               #(2)</span>
<span class="n">sigx</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c"># sigmoid in denominator #(3)</span>
<span class="n">xpy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>                                              <span class="c">#(4)</span>
<span class="n">xpysqr</span> <span class="o">=</span> <span class="n">xpy</span><span class="o">**</span><span class="mi">2</span>                                          <span class="c">#(5)</span>
<span class="n">den</span> <span class="o">=</span> <span class="n">sigx</span> <span class="o">+</span> <span class="n">xpysqr</span> <span class="c"># denominator                        #(6)</span>
<span class="n">invden</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">den</span>                                       <span class="c">#(7)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">num</span> <span class="o">*</span> <span class="n">invden</span> <span class="c"># done!                                 #(8)</span></code></pre></div>

<p>反向梯度计算如下：</p>

<div class="highlight"><pre><code class="language-python"><span class="c"># backprop f = num * invden</span>
<span class="n">dnum</span> <span class="o">=</span> <span class="n">invden</span> <span class="c"># gradient on numerator                             #(8)</span>
<span class="n">dinvden</span> <span class="o">=</span> <span class="n">num</span>                                                     <span class="c">#(8)</span>
<span class="c"># backprop invden = 1.0 / den </span>
<span class="n">dden</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">den</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dinvden</span>                                <span class="c">#(7)</span>
<span class="c"># backprop den = sigx + xpysqr</span>
<span class="n">dsigx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                                <span class="c">#(6)</span>
<span class="n">dxpysqr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dden</span>                                              <span class="c">#(6)</span>
<span class="c"># backprop xpysqr = xpy**2</span>
<span class="n">dxpy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">xpy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpysqr</span>                                        <span class="c">#(5)</span>
<span class="c"># backprop xpy = x + y</span>
<span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dxpy</span>                                                   <span class="c">#(4)</span>
<span class="c"># backprop sigx = 1.0 / (1 + math.exp(-x))</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigx</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigx</span> <span class="c"># Notice += !! See notes below  #(3)</span>
<span class="c"># backprop num = x + sigy</span>
<span class="n">dx</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                  <span class="c">#(2)</span>
<span class="n">dsigy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dnum</span>                                                <span class="c">#(2)</span>
<span class="c"># backprop sigy = 1.0 / (1 + math.exp(-y))</span>
<span class="n">dy</span> <span class="o">+=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigy</span><span class="p">)</span> <span class="o">*</span> <span class="n">dsigy</span>                                 <span class="c">#(1)</span>
<span class="c"># done! phew</span></code></pre></div>

<h4 id="section-7">注意事项：</h4>

<ol>
  <li>将前向计算结果缓存起来，供后向计算使用，如<code>xpy</code>；</li>
  <li>当变量的导数分成几部分计算时，结果要加起来，采用<code>+=</code>。</li>
</ol>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png"><img src="/assets/images/2015-02-13-image-classification-optimization-circuit4.png" alt="BP梯度计算" /></a><div class="caption">图 5:  BP梯度计算 [<a href="/assets/images/2015-02-13-image-classification-optimization-circuit4.png">PNG</a>]</div></div></div>

<p>神经网络中常使用的三种门是$+,\times,\max$，计算法则如上图。</p>

<p>对于线性分类器$\mathbf w^\top\mathbf x_i$，采用乘法门，输入数据的大小会影响权值梯度。输入数据对梯度影响很大，因此数据预处理能极大影响梯度。如果输入数据增大1000倍，权值的梯度也会增大1000倍，此时可以降低学习率补偿数据的影响。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>特征学习模型总结</title>
      <link href="http://qianjiye.de/2015/02/summary-of-extraction-models" />
      <pubdate>2015-02-13T17:48:53+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/02/summary-of-extraction-models</guid>
      <content:encoded>&lt;![CDATA[<p>特征学习／提取模型（extraction model）：除了得到最终的线性模型外，将特征变换$\Phi$作为隐含的学习变量。</p>

<p>特征学习模型是拥有众多成员的大家族：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>隐藏变量</th>
      <th>线性模型</th>
      <th>extraction技术</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>神经网络／深度学习</td>
      <td>$w_{ij}^{(\ell)}$</td>
      <td>$w_{ij}^{(L)}$</td>
      <td>梯度下降法＋BP<br />自编码器（非监督学习）</td>
    </tr>
    <tr>
      <td>RBF网络</td>
      <td>中心$\boldsymbol\mu_m$</td>
      <td>$\beta_m$</td>
      <td>k均值聚类（非监督学习）</td>
    </tr>
    <tr>
      <td>矩阵分解<sup id="fnref:v-equals-w"><a href="#fn:v-equals-w" class="footnote">1</a></sup></td>
      <td>用户特征$\mathbf v_n$</td>
      <td>电影特征$\mathbf w_m$</td>
      <td>梯度下降法<br />交替最小二乘法</td>
    </tr>
    <tr>
      <td>Ada/Gradient Boosting</td>
      <td>假设$g_t$</td>
      <td>投票权重$\alpha_t$</td>
      <td>函数梯度下降法</td>
    </tr>
    <tr>
      <td>k最近邻算法</td>
      <td>邻居$\mathbf x_n$</td>
      <td>$y_n$</td>
      <td>lazy learning</td>
    </tr>
  </tbody>
</table>

<p>特征学习模型的优劣：</p>

<table>
  <thead>
    <tr>
      <th>优势</th>
      <th>坏处</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>容易：减轻了人工提取特征的负担</td>
      <td>困难：通常是非凸优化</td>
    </tr>
    <tr>
      <td>强大：如果有足够多的隐含变量</td>
      <td>过拟合：需要正则化或验证</td>
    </tr>
  </tbody>
</table>

<p>因此，使用特征学习模型要当心！</p>

<h2 id="section">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-1">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:v-equals-w">
      <p>$\mathbf v_n$和$\mathbf w_m$实际上是对称的（等价的）。 <a href="#fnref:v-equals-w" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
