<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-01-12T10:42:31+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>支持向量机（6）：支持向量回归</title>
      <link href="http://qianjiye.de/2015/01/svm-support-vector-regression" />
      <pubdate>2015-01-12T18:31:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-support-vector-regression</guid>
      <content:encoded>&lt;![CDATA[
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（5）：核logistic回归</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-logistic-regression" />
      <pubdate>2015-01-09T15:01:15+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png" alt="4种形式的支持向量机" /></a><div class="caption">Figure 1:  4种形式的支持向量机 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png">PNG</a>]</div></div></div>

<h2 id="section">支持向量机的正则化形式</h2>

<p>回顾<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">soft-margin支持向量机</a>，当违反边界的时候$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n + b\right)$，当没有违反边界的时候$\xi_n = 0$，边界违法的情况可以统一定义为$\xi_n=\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)$，于是无约束形式的soft-margin支持向量机为
\[
\min\limits_{b,\mathbf w}{1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)，
\]
可以简写为
\[
\min\limits_{b,\mathbf w}{1\over 2}\mathbf w^T\mathbf w + C\sum\widehat{\mbox{err}}。
\]
这是目标函数的正则化形式表示方法，soft-margin可以看作一种特殊的误差$\widehat{\mbox {err}}$度量。$L_2$正则化是对$\mathbf w$长度的约束
\[
\min\limits_{b,\mathbf w}{\lambda\over N}\mathbf w^T\mathbf w + {1\over N}\sum\mbox{err}。
\]</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png" alt="SVM与正则化" /></a><div class="caption">Figure 2:  SVM与正则化 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png">PNG</a>]</div></div></div>

<p>加入了大分类的边界限制，可以得到更少的分类情况，也可以通过$L_2$正则化实现。
从上图对比正则化方法可知，支持向量机可以看作为特殊的正则化方法。大的$C$，对应于小的$\lambda$，更弱的正则化。</p>

<h2 id="section-1">误差度量</h2>

<p>令线性项输出$s=\mathbf w^T\mathbf z_n + b$，感知器算法、支持向量机和logistic回归的误差度量为
\begin{equation}
\left\{
\begin{aligned}
err_{0/1}(s, y)&amp;=[[ys\neq 1]]\\
\widehat{err}_{SVM}(s, y)&amp;=\max(1-ys, 0)\\
err_{SCE}(s, y)&amp;=\log(1+\exp(-ys))。
\end{aligned}
\right.
\end{equation}</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png" alt="误差度量比较" /></a><div class="caption">Figure 3:  误差度量比较 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png">PNG</a>]</div></div></div>

<p>几种误差曲线对比如上图所示。其中，支持向量机的这种误差度量方式通常称为hinge error measure。支持向量机和logistic回归的误差是0/1误差的上界，对于分类问题，通过上界的最小化，间接做好0/1误差的最优化。从误差曲线还可以看出，支持向量机和$L_2$正则化的logistic的误差度量非常相似。这几种分类器的比较如下：</p>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>优化方法</th>
      <th>优势</th>
      <th>劣势</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>感知器算法</td>
      <td>最小化$err_{0/1}$</td>
      <td>线性可分时效率较高</td>
      <td>只适用线性可分，否则采用pocket算法</td>
    </tr>
    <tr>
      <td>soft-margin支持向量机</td>
      <td>QP最小化误差$err_{0/1}$</td>
      <td>容易优化，理论基础较好</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
    <tr>
      <td>正则化logistic回归</td>
      <td>GD／SGD最小化误差$err_{SCE}$</td>
      <td>容易优化，正则化控制模型</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
  </tbody>
</table>

<p>从比较可以看出，logistic回归是soft-margin支持向量机的近似。</p>

<h2 id="section-2">支持向量机的概率模型</h2>

<p>如何让支持向量机输出$[0,1]$之间的概率？</p>

<p>一种思路是将支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$作为logistic回归中线性判别部分的参数
$
g(\mathbf x) = \theta\left(\mathbf w_{SVM}^T\mathbf x + b_{SVM}\right)
$，
直接利用支持向量机和logistic回归。这在实际应用中表现尚佳，但丧失了logistic回归的优良特性（比如maxmum likehood）。</p>

<p>另一种思路将向支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$当作logistic的起始点，最终得到logistic回归模型。这和直接利用logistic结果差不多，还丧失了支持向量机核方法等优良特性。</p>

<p>如何融合支持向量机和logistic回归的优点？</p>

<p>组合logistic回归和支持向量机
\begin{equation}
g(\mathbf x) = \theta\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x) + b_{SVM}\right)+B\right)，
\label{eq:mixture-svm-logistic-model}
\end{equation}
这样既融合了支持向量机的核特性，又通过$A$和$B$两个自由度调节分类超平面适合最大似然。如果$A&gt;0$，表示支持向量机得到的$\mathbf w_{SVM}$较好；如果$B\approx 0$，表示支持向量机得到的$b_{SVM}$较好。新的logistic问题就变为了
\begin{equation}
\min_{A, B}{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}\right)+B\right)\right)\right)，
\end{equation}
该模型可以分为2阶段，首先利用支持向量机得到1维特征，然后采用简单的logistic模型，这称为<strong>支持向量机的Platt概率模型</strong>：</p>

<ol>
  <li>先在数据集上$\mathcal D$上利用支持向量机得到模型参数$\mathbf w_{SVM}$和$b_{SVM}$（或者$\boldsymbol\alpha$），再进行变换$\mathbf z’_n=\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}$；</li>
  <li>在$\{\left(\mathbf z’_n, y_n\right)\}_{n=1}^N$上得到logistic模型的参数$A,B$；</li>
  <li>将公式\eqref{eq:mixture-svm-logistic-model}的结果作为模型输出。</li>
</ol>

<p>从模型可以看出，这并不是严格的$\mathcal Z$空间logistic回归。</p>

<h2 id="logistic">核logistic回归</h2>

<p>最佳的$\mathbf w$是$\mathbf z_n$的线性组合，这是能使用核方法的关键。</p>

<p>从SGD来看，logistic回归的$\mathbf w$也是$\mathbf z_n$的线性组合
\[
\mathbf w_{LOGREG}＝\sum_{n=1}^N\left(\alpha_ny_n\right)\mathbf z_n。
\]</p>

<blockquote>
  <h4 id="representer-theorem">表示定理（representer theorem）</h4>
  <hr />
  <p>对任意的$L_2$正则化线性模型     <br />
\begin{equation*}
\min_{\mathbf w}{\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^Nerr\left(y_n,\mathbf w^T\mathbf z_n\right)，
\end{equation*} 
存在能用$\mathbf z_n$线性表示的最佳解$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$。</p>
</blockquote>

<p>$L_2$正则化的logistic回归优化模型为
\[
\min_{\mathbf w}{\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\mathbf w^T\mathbf z_n\right)\right)。
\]
由上述定理可知，最佳$\mathbf w$一定是$\mathbf z_n$的线性组合。直接将用$\mathbf z_n$表示的$\mathbf w$代入上式，再用核方法表示，可以得到基于$L_2$正则化的核logistic回归优化模型
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\sum_{m=1}^N\beta_mK(\mathbf x_m,\mathbf x_n)\right)\right)\right)，
\end{equation}
这是无约束最优化问题，GD／SGD等都可求解。与支持向量机不同，KLR的大部分$\beta_n\neq 0$。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（4）：soft-margin支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-soft-margin-svm" />
      <pubdate>2015-01-07T17:24:31+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-soft-margin-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="soft-margin">soft-margin支持向量机</h2>

<p>在求解支持向量机参数的二次规划中，约束条件要求所有点被正确分类的叫<strong>hard-margin支持向量机</strong>。这类支持向量机过拟合原因：（1）特征转换功能太强；（2）坚持所有点都被正确分类。</p>

<p>可以通过放宽条件，容忍部分噪声（容许这部分噪声数据分类错误），使得支持向量机具有更好的泛化性能。</p>

<p>对于pocket PLA，目标函数为
\[
\min\limits_{b,\mathbf w}\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\]
结合<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqlinear-svm-model">线性支持向量机</a>，可以得到容忍错分的优化模型
\[
\begin{aligned}
\min\limits_{b,\mathbf w}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\infty\cdot\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\end{aligned}
\]
$C$是调节最大边界和噪声容忍度的参数。但是，上述模型不是QP，并且不能区别分类错误时误差的大小，进一步降模型变为<strong>soft-margin支持向量机</strong>
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\label{eq:soft-margin-primal-svm}
\end{equation}
该QP模型有$\tilde d+1+N$个变量和$2N$个约束条件，也被称为基于$\ell_1$损失的soft-margin。如果采用$\xi_n^2$，则被称为基于$\ell_2$损失的soft-margin
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n^2\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n，
\end{aligned}
\end{equation*}
此时不再需要约束条件$\xi_n\geq 0$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png" alt="边界容忍度" /></a><div class="caption">Figure 1:  边界容忍度 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png">PNG</a>]</div></div></div>

<p>$C$是调节最大边界和边界容忍度的参数，$\xi_n$是容忍误差的大小，如上图所示。$C$越小，对最大边界要求越高；$C$越大，能容忍的边界误差越小。</p>

<h2 id="soft-margin-1">对偶soft-margin支持向量机</h2>

<p>soft-margin支持向量机的拉格朗日函数为
\[
\begin{aligned}
\mathcal L(b,\mathbf w,\boldsymbol\alpha, \boldsymbol\beta)=
&amp;{1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
&amp;+\sum_{n=1}^N\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)+\sum_{n=1}^N\beta_n\left(-\xi_n\right)，
\end{aligned}
\]
利用化解<a href="/2015/01/svm-dual-svm/#lagrange-dual-problem">拉格朗日对偶问题</a>的方法可得
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad 0\leq\alpha_n\leq C,\mbox{ for }n=1,2,\ldots,N \\
\mbox{implicitly}&amp;\quad \mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n\\
&amp;\quad\beta_n=C-\alpha_n,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-soft-margin-dual-svm}
\end{equation}
与hard-margin对偶支持向量机不同的地方只是$\alpha_n$多了一个上界$C$，这是$N$个变量$2N+1$个约束条件的QP。$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf 0_N\leq \mathbf I_N\boldsymbol\alpha \leq C\cdot \mathbf 1_N。
\end{equation}</p>

<h2 id="soft-margin-2">核soft-margin支持向量机</h2>

<p>soft-margin是实际中广泛应用的支持向量机。</p>

<p>soft-margin的支持向量机与<a href="/2015/01/svm-kernel-svm/#kernel-trick">hard-margin的支持向量机</a>基本相同，$\alpha_n$上界$C$的限制，导致$b$的计算不同。</p>

<p>利用complementary slackness条件可得
\begin{equation}
\begin{aligned}
\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=&amp;0\\
\left(C-\alpha_n\right)\xi_n=&amp;0，
\end{aligned}
\label{eq:soft-margin-complementary-slackness}
\end{equation}
当$\alpha_s&gt;0$时，$b=y_s-y_s\xi_s-\mathbf w^T\mathbf z_s$；当$\alpha_s&lt;C$时，$\xi_s=0$。满足$0&lt;\alpha_s&lt;C$的点称为<strong>自由支持向量</strong>$\left(\mathbf x_s, y_s\right)$，利用这些点容易得到
\begin{equation}
b=y_s-\sum\limits_{SV}\alpha_ny_nK\left(\mathbf x_n, \mathbf x_s\right)。
\end{equation}
在极少数情况下，不存在自由支持向量，$b$通过不等式限定，只要满足KKT条件的取值都是合理的$b$。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png" alt="soft-margin高斯核支持向量机" /></a><div class="caption">Figure 2:  soft-margin高斯核支持向量机 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png">PNG</a>]</div></div></div>

<p>上图展示了soft-margin高斯核支持向量机的效果，灰色的区域表示最大分类间隔。从图中可以看出，$C$越大，对误差的容忍越弱，越容易导致过拟合。</p>

<h2 id="alphan">$\alpha_n$的物理含义</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png" alt="不同类型的数据点" /></a><div class="caption">Figure 3:  不同类型的数据点 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png">PNG</a>]</div></div></div>

<p>通过公式\eqref{eq:soft-margin-complementary-slackness}可知，$\alpha_n$将数据点分为如上图所示的3种类型：</p>

<ol>
  <li>当$\alpha_n=0$时，非支持向量，$\xi_n=0$，位于边界之外，极少数可能在边界上；</li>
  <li>当$0&lt;\alpha_n&lt;C$时，自由（free）支持向量$\square$，$\xi_n=0$，位于边界上，用于计算$b$；</li>
  <li>当$\alpha_n=C$时，有界（bounded）支持向量$\triangle$，$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n+b\right)$，落在边界内，可能正确分类也可能分错，极少数可能在边界上。</li>
</ol>

<h2 id="section">模型选择</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png" alt="［中］：交叉验证误差；［右］：支持向量个数" /></a><div class="caption">Figure 4:  ［中］：交叉验证误差；［右］：支持向量个数 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png">PNG</a>]</div></div></div>

<p>上图左是soft-margin高斯核支持向量机的分类效果，横轴是$C$的变化，纵轴是$\gamma$的变化。由于$E_{cv}(C,\gamma)$不光滑，通常的模型选择方法是通过$C$和$\gamma$的数据网格，利用交叉验证的方法选择合适的模型，上图中所示，选择了左下角的模型。</p>

<p>交叉验证中，将数据分为$N$份的验证称为<strong>leave-one-out交叉验证</strong>，它的误差上界是
\begin{equation}
E_{loocv}\leq\frac{\#SV}{N}，
\label{eq:eloocv-upper-bound}
\end{equation}
$\#SV$表示支持向量的个数。可以通过支持向量的个数进行模型选择。由于支持向量个数的函数也是非光滑的，难以优化，也采取利用$C$和$\gamma$的数据网格，多次计算后做选择。</p>

<p>由于\eqref{eq:eloocv-upper-bound}也只是给出了$E_{loocv}$的上界，通常用于当$E_{cv}$计算量很大时模型的安全检查，剔除那些支持向量过多的危险模型，然后再在剩余模型中进一步做交叉验证选择合适的模型。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（3）：核支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-svm" />
      <pubdate>2015-01-06T14:25:58+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">回顾对偶支持向量机</h2>

<p>当特征空间维数$\tilde d$很大时，计算$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$是对偶支持向量机的求解瓶颈。</p>

<p>能否找到比$O(\tilde d)$快的方法计算$\mathbf z_n^T\mathbf z_m=\Phi(\mathbf x_n)^T\Phi(\mathbf x_m)$？能否将先特征转换再计算内积的两步合为一步呢？</p>

<h2 id="kernel-trick">核技巧</h2>

<p>对于2阶多项式变换
\[
\Phi_2(\mathbf x)=\left(1, x_1,x_2,\ldots,x_d,x_1^2,x_1x_2,\ldots,x_1x_d,x_2x_1,x_2^2,\ldots,x_2x_d,\ldots,x_d^2\right)，
\]
为了简化同时包含了$x_1x_2$和$x_2x_1$这样的项。变换之后$Z$空间的内积可以可直接通过$X$空间计算
\[
\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)=1+\left(\mathbf x^T\mathbf x’\right)+\left(\mathbf x^T\mathbf x’\right)^2。
\]
这种特征转换和内积合并的方法称之为<strong>核函数</strong>，
\begin{equation*}
K_{\Phi_2}\left(\mathbf x,\mathbf x’\right)=\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)。
\end{equation*}
利用核函数，可以简化对偶支持向量机的实现，二次项的系数为
\begin{equation}
q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m=y_ny_mK\left(\mathbf x_n,\mathbf x_m\right)，
\end{equation}
利用支持向量$\left(\mathbf x_s, y_s\right)$计算偏移量
\begin{equation}
\begin{aligned}
b
=&amp;y_s-\mathbf w^T\mathbf z_s\\
=&amp;y_s-\left(\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right)^T\mathbf z_s\\
=&amp;y_s-\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x_s\right)
，
\end{aligned}
\end{equation}
对于特定的输入$\mathbf x$，判别函数为
\begin{equation}
\begin{aligned}
g_{SVM}(\mathbf x)
=&amp;\mbox{sign}\left(\mathbf w^T\Phi(\mathbf x)+b\right)\\
=&amp;\mbox{sign}\left(\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x\right)+b\right)
。
\end{aligned}
\end{equation}
从上面的公式可以看出，计算不再依赖变换后的空间，只依赖原空间，$b$和判别函数只依赖原空间的支持向量，大大简化了计算。</p>

<h2 id="section-1">多项式核</h2>

<p>仿照2阶多项式核的定义，可以推导高阶多项式核的定义为
\begin{equation}
K_Q(\mathbf x,\mathbf x’)=\left(\zeta + \gamma\mathbf x^T\mathbf x’\right)^Q\quad\zeta\geq 0,\gamma &gt; 0。
\end{equation}
事实上，系数$\zeta$和$\gamma$的取值不会改变多项式所在的空间，这些系数会被$\mathbf w$所吞噬。但是，不同的系数会得到不同的支持向量和判别函数。选择不同的核，相当于改变了边界的定义。多项式核可以在几乎不增加计算量的情况下，得到复杂的判别界。支持向量机通过large-margin控制判别界的复杂度。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png" alt="2阶多项式核SVM的效果" /></a><div class="caption">Figure 1:  2阶多项式核SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是2阶多项式核支持向量机的效果，不同系数下的支持向量和判别界不同。</p>

<p>当$\zeta=0,\gamma=1$时，多项式核就变为了线性核。线性核利用原始支持向量机就比较高效。因此，应当首先尝试线性核，当线性核不能满足要求时再尝试其它高阶核。</p>

<h2 id="section-2">高斯核</h2>

<p>对于1维数据的高斯核，利用Taylor展式可得
\begin{equation*}
\begin{aligned}
K(x,x’)
=&amp;\exp\left(-\left(x-x’\right)^2\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\exp\left(2xx’\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\sum_{i=0}^\infty\frac{\left(2xx’\right)^i}{i!}\\
=&amp;\sum_{i=0}^\infty\exp\left(-x^2\right)\exp\left(-x’^2\right)\sqrt{\frac{2^i}{i!}}\sqrt{\frac{2^i}{i!}}x^ix’^i\\
=&amp;\Phi(x)^T\Phi(x’)，
\end{aligned}
\end{equation*}
其中$\Phi(x)=\exp\left(-x^2\right)\cdot\left(1,\sqrt{\frac{2^1}{1!}}x,\sqrt{\frac{2^2}{2!}}x^2,\ldots\right)$，容易看出这是无穷维的特征变换。更一般的高斯核定义为
\begin{equation}
K\left(\mathbf x,\mathbf x’\right)=\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x’\right\rVert^2\right)\quad \gamma&gt;0。
\end{equation}
高斯核也成为径向基函数（RBF，Radial Basis Function）核。基于高斯核的判别函数为
\begin{equation}
g_{SVM}(\mathbf x)=\mbox{sign}\left(\sum_{SV}\alpha_ny_n\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_n\right\rVert^2\right)+b\right)，
\end{equation}
它是以支持向量为中心的高斯函数的线性组合，不再依赖$\mathbf w$，只依赖于支持向量和系数$\alpha_n$。</p>

<p>高斯核相当于进行了无限维的特征转换，可以得到复杂的判别界，泛化性能通过最大边界“保证”。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png" alt="高斯核的SVM的效果" /></a><div class="caption">Figure 2:  高斯核的SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是不同系数的高斯核支持向量机的效果，$\gamma$越大，高斯函数越尖，越容易过拟合。当$\gamma\rightarrow\infty$时，$K_{lim}\left(\mathbf x,\mathbf x’\right)=[[\mathbf x＝\mathbf x’]]$，相当于严格判断是否与支持向量一致。</p>

<h2 id="section-3">小结</h2>

<p>各种核函数的比较如下表：</p>

<table>
  <thead>
    <tr>
      <th>核函数</th>
      <th>优势</th>
      <th>局限性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>线性核</td>
      <td>计算快；<br />通过$\mathbf w$和支持向量容易解释</td>
      <td>无法处理线性不可分数据</td>
    </tr>
    <tr>
      <td>多项式核</td>
      <td>可以通过次数$Q$控制复杂度</td>
      <td>当$Q$很大时数值计算困难<br />（当$\left\lvert\zeta+\gamma\mathbf x^T\mathbf x’\right\rvert&lt;1$时，$K(\mathbf x, \mathbf x’)\rightarrow 0$）；<br />3个参数难以选择</td>
    </tr>
    <tr>
      <td>高斯核</td>
      <td>强大；<br />$K(\mathbf x, \mathbf x’)$有界；<br />1个参数难以选择；</td>
      <td>没有显示的$\mathbf w$供解读；<br />计算比线性核慢；<br />太强大导致容易过拟合</td>
    </tr>
  </tbody>
</table>

<p>当采用多项式核时，如果$Q$较小，可以尝试直接利用特征变换和原始支持向量机，求解速度可能比核方法更快。</p>

<p>核是一种特殊的相似性度量，但是不是所有的相似性度量都可以作为核。有效的核必须满足<strong>Mercer条件</strong>（充要条件）：核函数矩阵
\begin{equation*}
\begin{aligned}
\mathbf K
=&amp;\begin{bmatrix}
\Phi(\mathbf x_1)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_1)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_1)^T\Phi(\mathbf x_N)\\
\Phi(\mathbf x_2)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_2)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_2)^T\Phi(\mathbf x_N)\\
\ldots&amp;\ldots&amp;\ldots&amp;\ldots\\
\Phi(\mathbf x_N)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_N)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_N)^T\Phi(\mathbf x_N)
\end{bmatrix}\\
=&amp;\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]^T\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]\\
=&amp;\mathbf Z\mathbf Z^T
\end{aligned}
\end{equation*}
必须是对称半正定。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（2）：对偶支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-dual-svm" />
      <pubdate>2015-01-05T18:24:26+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-dual-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">对偶支持向量机的价值</h2>

<p>对于求解非线性支持向量机系数的QP，有$\tilde d+1$个变量和$N$个约束条件，对于经过非线性特征变换后$Z$空间的特征维数$\tilde d$一般很大。当$\tilde d$很大时，求解比较具有挑战性。</p>

<p>对偶支持向量机是支持向量机的另一种形式，它不依赖$\tilde d$，有$N$个变量和$N+1$个约束条件。</p>

<h2 id="section-1">拉格朗日乘子法</h2>

<p>正则化采用带约束的最小化方法
\[
\min\limits_{\mathbf w} E_{in}(\mathbf w)\mbox{ s.t. }\mathbf w^T\mathbf w \leq C，
\]
可以通过如下等价的拉格朗日乘子法实现
\[
\min\limits_{\mathbf w} E_{aug}(\mathbf w)=E_{in}(\mathbf w)+{\lambda\over N}\mathbf w^T\mathbf w。
\]
正则化通过将$\lambda$作为<strong>确定值</strong>，代替$C$作为约束条件，求解更容易。对偶支持向量机不同于正则化，它将$N$个$\lambda$视为<strong>变量</strong>求解。</p>

<p>当$\boldsymbol\alpha_n$为拉格朗日乘子时，求解支持向量机参数的拉格朗日函数为
\begin{equation}
\mathcal L(b, \mathbf w, \boldsymbol \alpha) = {1\over 2}\mathbf w^T\mathbf w + \sum_{n=1}^N\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)。
\end{equation}</p>

<h2 id="lagrange-dual-problem">拉格朗日对偶问题</h2>

<p>根据拉格朗日函数，可得
\begin{equation}
\begin{aligned}
\mbox{SVM}\equiv &amp;\min\limits_{b,\mathbf w}\max\limits_{\mbox{all }\alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\\
=&amp;\min\limits_{b,\mathbf w}\left(\infty\mbox{ if violating };{1\over 2}\mathbf w^T\mathbf w\mbox{ if feasible}\right)。
\end{aligned}
\end{equation}
根据$b$和$\mathbf w$取值的划分，分两种情况解释上述公式：</p>

<ol>
  <li>任意violating的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{some positive})\right)\rightarrow\infty$；</li>
  <li>任意feasible的$(b,\mathbf w)$：$\max\limits_{\mbox{all } \alpha_n\geq 0}\left(\square+\sum_n\alpha_n(\mbox{all non-positive})\right)=\square$。</li>
</ol>

<p>对$\alpha’_n\geq 0$的任意$\boldsymbol\alpha’$可得
\[
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha’)，
\]
于是有
\begin{equation}
\min\limits_{b,\mathbf w}\max\limits_{\mbox{all } \alpha_n\geq 0}\mathcal L(b,\mathbf w, \boldsymbol\alpha)\geq\max\limits_{\mbox{all } \alpha_n\geq 0}\min\limits_{b,\mathbf w}\mathcal L(b,\mathbf w, \boldsymbol\alpha)。
\end{equation}</p>

<p>若果解决了对偶问题，就得到原问题的下界。上式右边<strong>将对$b$和$\mathbf w$的优化问题转化成了对$\alpha_n$的优化问题</strong>，同时内层是对$b$和$\mathbf w$无约束条件的最优化，方便求解。</p>

<p>如果上式中只是“$\geq$”，则表示弱对偶（weak duality）；如果上式中“$=$”成立，则为强对偶（strong duality）。对于QP，“$=$”成立的条件是：（1）原问题是凸的；（2）原问题有解（对本问题而言，数据是可分的）；（3）线性约束条件。</p>

<p>对于支持向量机，“$=$”成立，求解右边的问题即可。</p>

<h2 id="section-2">化简拉格朗日对偶问题</h2>

<p>对于对偶问题，内层无约束优化取得最小值的条件是
\[
{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial b} = 0;\quad{\partial \mathcal L(b, \mathbf w, \boldsymbol\alpha)\over\partial w_i} = 0，
\]
也就是
\[
\sum_{n=1}^N\alpha_ny_n=0;\quad\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\]
将上述变量带入对偶问题，可以化解为
\[
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\min\limits_{b,\mathbf w}\left(\frac{1}{2}\mathbf w^T\mathbf w+\sum_{n=1}^N\alpha_n-\mathbf w^T\mathbf w\right)，
\]
继续将$\mathbf w$的取值带入，可得
\begin{equation*}
\max\limits_{\mbox{all }\alpha_n\geq 0,\sum y_n\alpha_n=0,\mathbf w=\sum\alpha_ny_n\mathbf z_n}\left(-\frac{1}{2}\left\lVert\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right\rVert^2+\sum_{n=1}^N\alpha_n\right)，
\end{equation*}
化为二次规划的标准形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm}
\end{equation}</p>

<p>该优化问题有$N$个变量，$N+1$个约束条件。根据<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqqp-standard-format">QP的标准形式</a>，可以得到利用QP求解对偶二次规划的系数
\[
\begin{aligned}
&amp;q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m;\quad\mathbf p=-\mathbf 1_N;\\
&amp;\mathbf a_{\geq}=\mathbf y,c_{\geq}=0;\quad\mathbf a_{\leq}=-\mathbf y,c_{\leq}=0;\\
&amp;\mathbf a_n^T=\mbox{n-th unit direction},c_n=0。
\end{aligned}
\]
为了利用标准的QP，将“$=$”约束转换成了“$\geq$”和“$\leq$”约束。通常情况$q_{n,m}\neq 0$，系数对应着$N\times N$的非稀疏矩阵，$N$很大时需要占用很大的存储空间。因此，通常会采用针对支持向量机设计的特殊QP加速求解过程。</p>

<p>对偶支持向量机优化问题的矩阵形式为
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\boldsymbol\alpha^T\mathbf Q_D\boldsymbol\alpha-\mathbf 1^T\boldsymbol\alpha \\
\mbox{subject to}&amp;\quad\mathbf y^T\boldsymbol\alpha=0\\
&amp;\quad\alpha_n\geq 0,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-dual-svm2}
\end{equation}
其中$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$，$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf I_N\boldsymbol\alpha \geq \mathbf 0_N。
\end{equation}</p>

<h2 id="kkt">KKT条件</h2>

<p>原问题和对偶问题都是最佳解需要$b,\mathbf w,\boldsymbol\alpha$之间满足如下条件：</p>

<ol>
  <li>原问题可行：$y_n(\mathbf w^T\mathbf z_n+b)\geq 1$；</li>
  <li>对偶问题可行：$\alpha_n\geq 0$；</li>
  <li>对偶问题内最优化：$\sum_{n=1}^N\alpha_ny_n=0;\mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n$；</li>
  <li>原问题内最优化：$\alpha_n\left(1-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=0$，这个条件也称为complementary slackness，其中至少一项为$0$。</li>
</ol>

<p>这称为KKT条件，它是原问题和对偶问题都是最佳解的必要（necessary）条件，此处也是充分（sufficient）条件。</p>

<blockquote>
  <h4 id="example">Example</h4>
  <hr />
  <p>For a single variable $w$, consider minimizing ${1\over 2}w^2$ subject to two linear constraints $w\geq 1$ and $w\leq 3$. We know that the Lagrange function $\mathcal L(w,\alpha)={1\over 2}w^2+\alpha_1(1-w)+\alpha_2(w-3)$. Which of the following equations that contain $\alpha$ are among the KKT conditions of the optimization problem?</p>

  <ol>
    <li>$\alpha_1\geq 0$ and $\alpha_2\geq 0$</li>
    <li>$w=\alpha_1-\alpha_2$</li>
    <li>$\alpha_1(1-w)=0$ and $\alpha_2(w-3)=0$ </li>
    <li>all of the above</li>
  </ol>

  <p>Answer：4</p>
</blockquote>

<p>通过KKT条件，可以利用$\boldsymbol\alpha$求解$b$和$\mathbf w$。利用KKT条件3容易求解$\mathbf w$，利用KKT条件4，当$\alpha_n&gt;0$时，$1-y_n\left(\mathbf w^T\mathbf z_n+b\right)=0$两边同时乘以$y_n$可得$b$，
\begin{equation}
\left\{
\begin{aligned}
b=&amp;y_n-\mathbf w^T\mathbf z_n \quad\mbox{if }\alpha_n\neq 0；\\
\mathbf w=&amp;\sum_{n=1}^N\alpha_ny_n\mathbf z_n。
\end{aligned}
\right.
\end{equation}
利用不同$\alpha_n&gt;0$时的数据，理论上求解到的$b$应该是一样的，可以计算多个$b$然后平均得到更稳定的解。</p>

<h2 id="section-3">支持向量</h2>

<p>$\alpha_n&gt;0$对应的点一定在边界上，这些点称为<strong>支持向量</strong>。也有些在边界上的点，对应的$\alpha_n$不一定大于$0$。容易发现，计算$b$和$\mathbf w$只需要支持向量就够了。</p>

<p>从计算公式可以发现，$\mathbf w$可以由$y_n\mathbf z_n$的线性组合表示，也就是$\mathbf w$可由数据表示，这和PLA算法相似
\begin{equation*}
\mathbf w_{SVM}=\sum_{n=1}^N\alpha_n\left(y_n\mathbf z_n\right)，
\mathbf w_{PLA}=\sum_{n=1}^N\beta_n\left(y_n\mathbf z_n\right)，
\end{equation*}
其中$\beta_n$表示犯错误的次数。</p>

<h2 id="section-4">两种形式的支持向量机</h2>

<p>原始支持向量机适合特征维数$\tilde d$较少的情形，对偶形式的支持向量机适合数据点$N$较少的情形，两者都是通过最优化找到最大边界的判别界。</p>

<p>事实上，对偶形式的支持向量机和特征维数$\tilde d$也有关系，$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$的计算也是$\mathbb R^{\tilde d}$空间的内积。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（1）：线性支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-linear-svm" />
      <pubdate>2015-01-03T18:57:13+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-linear-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">最佳判别界</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png" alt="最佳判别界" /></a><div class="caption">Figure 1:  最佳判别界 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-best-hyperplane.png">PNG</a>]</div></div></div>

<p>对相同的数据集，PLA可能得到不同的判别界，如上图所示，那条判别界最好呢？</p>

<ul>
  <li>测量是有误差的，能容忍误差越大的分界线越好，如上图上排所示，灰色的圆半径越大表示对误差的容忍度越大。噪声是导致过拟合的主要原因，对误差的容忍度越好，过拟合的可能性越低。</li>
  <li>判别界能膨胀得越胖越鲁棒，如上图下排所示。</li>
</ul>

<p>根据上两条标准，最右的判别界最佳。判别界的胖瘦程度称为边界（margin），最大边界的判别界最佳，也就是离判别界距离最近的点离判别界距离越大越好，</p>

<p>\begin{equation*}
\begin{aligned}
\max\limits_{\mathbf w} &amp;\quad\mbox{margin}(\mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\mathbf w^T\mathbf x_n &gt; 0\\
&amp;\quad\mbox{margin}(\mathbf w)=\min\limits_{n=1,\ldots,N}\mbox{distance}(\mathbf x_n, \mathbf w)。
\end{aligned}
\end{equation*}</p>

<p>$y_n\mathbf w^T\mathbf x_n &gt; 0$表示点被正确的分类，$\mbox{margin}(\mathbf w)$的定义可以保证判别界在两类的“中间”，不会偏向任何一边。</p>

<h2 id="section-1">支持向量机</h2>

<p>与线性感知器的定义不同，定义$b= w_0, \mathbf w=\left[ w_1, \ldots,  w_d\right]^T$，$\mathbf x=\left[ x_1, \ldots,  x_d\right]^T$。点到判别界的距离可表示为
\begin{equation*}
\mbox{distance}(\mathbf x, b, \mathbf w)=\frac{1}{\lVert\mathbf w\rVert}\left\lvert\mathbf w^T\mathbf x + b\right\rvert，
\end{equation*}
因此可得最大间隔
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad\mbox{margin}(b, \mathbf w)\\
\mbox{subject to}&amp; \quad\mbox{every } y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 0\\
&amp;\quad\mbox{margin}(b, \mathbf w)=\min\limits_{n=1,\ldots,N}\frac{1}{\lVert\mathbf w\rVert}y_n\left(\mathbf w^T\mathbf x_n + b\right)。
\end{aligned}
\end{equation*}
通过系数缩放，总可以做到$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$，最大间隔问题可以转化为等价问题
\begin{equation*}
\begin{aligned}
\max\limits_{b, \mathbf w} &amp;\quad{1\over\lVert\mathbf w\rVert}\\
\mbox{subject to}&amp; \min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1。
\end{aligned}
\end{equation*}
上述条件$\min\limits_{n=1,\ldots,N}y_n\left(\mathbf w^T\mathbf x_n + b\right) = 1$可以用等价的条件$y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1$代替。如果仅仅是$y_n\left(\mathbf w^T\mathbf x_n + b\right) &gt; 1$而无法取“$=$”，那么不等式两边可以除以大于$1$的系数，使得条件仍然成立，$\mathbf w$除以大于$1$的系数后，${1\over\lVert\mathbf w\rVert}$会更大。因此，标准形式的最优化是
\begin{equation}
\begin{aligned}
\min\limits_{b, \mathbf w} &amp;\quad{1\over 2}\mathbf w^T\mathbf w\\
\mbox{subject to}&amp; \quad y_n\left(\mathbf w^T\mathbf x_n + b\right) \geq 1\mbox{ for all }n。
\end{aligned}
\label{eq:linear-svm-model}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png" alt="支持向量" /></a><div class="caption">Figure 2:  支持向量 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-support-vectors.png">PNG</a>]</div></div></div>

<p>通过求解可以发现，只有部分点对解有作用，如上图边界上方框内的点，这些点称为<strong>候选</strong>支持向量（support vector）。支持向量机就是利用支持向量学习“最胖”判别界。</p>

<h2 id="section-2">二次规划求解</h2>

<p>求解最佳判别界的是一个二次规划问题（QP，quadratic programming）。二次规划的标准形式为
\begin{equation}
\begin{aligned}
\mbox{optimal} &amp;\quad\mathbf u\leftarrow QP(\mathbf Q, \mathbf p, \mathbf A, \mathbf c) \\
\min\limits_{\mathbf u} &amp;\quad{1\over 2}\mathbf u^T\mathbf Q\mathbf u + \mathbf p^T\mathbf u\\
\mbox{subject to}&amp;\quad\mathbf a_n^T\mathbf u\geq c_n\mbox{ for }n=1,2,\ldots,N。
\end{aligned}
\label{eq:qp-standard-format}
\end{equation}
支持向量机利用二次规划求解时参数为
\begin{equation*}
\mathbf u =<br />
\begin{bmatrix}
b\\
\mathbf w
\end{bmatrix}
，
\end{equation*}
目标函数系数为
\begin{equation*}
\mathbf Q = 
\begin{bmatrix}
0 &amp;\mathbf 0_d^T\\
\mathbf 0_d &amp;\mathbf I_d
\end{bmatrix};
\mathbf p=\mathbf 0_{d+1}
，
\end{equation*}
约束条件系数为
\begin{equation*}
\mathbf a_n^T=y_n\left[1\quad\mathbf x_n^T\right];c_n=1。
\end{equation*}</p>

<p>这里所讲的支持向量机叫做线性hard-margin支持向量机，其中hard-margin是指所有的点能被正确分类。</p>

<p>利用$\mathbf z_n=\Phi\left(\mathbf x_n\right)$，用$\mathbf z_n$替代$\mathbf x_n$，可以得到非线性的支持向量机。</p>

<h2 id="section-3">最大边界的价值</h2>

<p>正则化（regularization）通过约束条件$\mathbf w^T\mathbf w\leq C$最小化$E_{in}$，支持向量机通过$E_{in}=0$等约束条件最小化$\mathbf w^T\mathbf w$。因此，支持向量机也可以看作是一种<strong>特殊的正则化方法</strong>。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png" alt="无法打碎3个点的情况" /></a><div class="caption">Figure 3:  无法打碎3个点的情况 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-no3shatter.png">PNG</a>]</div></div></div>

<p>对于最大边界算法$\mathcal A_{\rho}$，脚标表示分界线的宽度要大于$\rho$。当$\rho = 0$时就是PLA，它可以打碎2维平面的3个输入点；若$\rho＝1.126$，如上图所示，无法打碎3个点。</p>

<p>如果加入了$\rho$条件的限制，可能的二分类情况少了，可认为<strong>VC维更小了，有更好的泛化性能</strong>。对于算法的VC维，有数据依赖的VC维比没有依赖的要低，$d_{VC}\left(\mathcal A_\rho\right)&lt;d_{VC}(\mathcal H)$，有数据依赖意味着加入了跟多的约束条件。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png" alt="单位圆上的数据" /></a><div class="caption">Figure 4:  单位圆上的数据 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-unit-circle-data.png">PNG</a>]</div></div></div>

<p>当$\mathcal X$是上图所示半径为$R$的单位圆上点时，
\begin{equation}
d_{VC}\left(\mathcal A_\rho\right)\leq\min\left({R^2\over \rho^2},d\right)+1\leq d+1。
\end{equation}</p>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png"><img src="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png" alt="各种判别界的对比" /></a><div class="caption">Figure 5:  各种判别界的对比 [<a href="/assets/images/2015-01-03-svm-linear-support-vector-machine-benefits-large-margin.png">PNG</a>]</div></div></div>

<p>几种情况的判别界对比如上图所示，large-margin的判别界简单且数量较少，有特征变换$\Phi$的判别界较复杂但是数量较多。</p>

<p>较少的判别界对$d_{VC}$和泛化有利，复杂的判别界可能得到更好的$E_{in}$。非线性支持向量机同时具备大边界的判别界和采用多种特征变换$\Phi$，因此判别界较少而且比较复杂，兼顾了这两方面的优点。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>Map-Reduce Essential</title>
      <link href="http://qianjiye.de/2014/12/map-reduce-essential" />
      <pubdate>2014-12-25T02:08:25+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/map-reduce-essential</guid>
      <content:encoded>&lt;![CDATA[<h2 id="map-reduce">为什么需要Map-Reduce？</h2>

<h3 id="cluster">集群（cluster）</h3>

<p>在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？</p>

<p>即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！</p>

<p>需要这样一个集群……</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-cluster-architecture.png"><img src="/assets/images/2014-12-24-map-reduce-cluster-architecture.png" alt="集群架构" /></a><div class="caption">Figure 1:  集群架构 [<a href="/assets/images/2014-12-24-map-reduce-cluster-architecture.png">PNG</a>]</div></div></div>

<h3 id="map-reduce-1">Map-Reduce解决集群带来的挑战</h3>

<h4 id="node-failures">一、节点故障（node failures）</h4>

<p>如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题：</p>

<ul>
  <li>如何存储数据，即使节点故障时仍可用？</li>
  <li>若正在进行大规模计算，如果节点发生故障该如何处理？</li>
</ul>

<p>Map-Reduce在多个节点冗余存储，保证数据持久存储和获取。</p>

<h4 id="network-bottleneck">二、网络瓶颈（network bottleneck）</h4>

<p>Map-Reduce的计算靠近数据端，减少数据移动。</p>

<h4 id="section">三、分布式程序编写困难</h4>

<p>Map-Reduce简单的编程模型，隐藏了复杂的细节。</p>

<h2 id="map-reduce-2">Map-Reduce简介</h2>

<h3 id="redundant-storage-infrastructure">冗余存储架构（redundant storage infrastructure）</h3>

<p>冗余存储架构采用分布式文件系统（distributed file system），例如：Google GFS、Hadoop HDFS。典型的应用是处理大文件，一次存储多次读取追加更新。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png"><img src="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png" alt="数据分块存储" /></a><div class="caption">Figure 2:  数据分块存储 [<a href="/assets/images/2014-12-24-map-reduce-cluster-architecture-data-chunk.png">PNG</a>]</div></div></div>

<p>数据分块（chuck）存储在多台服务器。如上图所示，一个大文件分割成C0～C5共6块，每块在多台服务器存储备份。每台存储服务器也做计算用，使得计算靠近存储端。</p>

<h3 id="computational-model">计算模型（computational model）</h3>

<blockquote>
  <h4 id="map-reduce-3">Map-Reduce计算模型</h4>
  <hr />
  <p>输入：key-value对的集合     <br />
程序实现以下两个模块：</p>

  <ol>
    <li>Map(k,v) —&gt; &lt;k’, v’&gt;*
      <ul>
        <li>输入一个key-value对，输出多个key-value对；</li>
        <li>对所有的(k,v)对，只有一个Map函数。 </li>
      </ul>
    </li>
    <li>Reduce(k’, &lt;v’&gt;*) —&gt; &lt;k’, v”&gt;
      <ul>
        <li>所有的具有相同k’的v’都被reduce到一起；</li>
        <li>对同一个k’，只有一个Reduce函数。</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Map-Reduce的计算模型分为Map和Reduce两步，Map分布式处理任务，Reduce合并任务。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-computational-model.png"><img src="/assets/images/2014-12-24-map-reduce-computational-model.png" alt="Map-Reduce单词计数实例" /></a><div class="caption">Figure 3:  Map-Reduce单词计数实例 [<a href="/assets/images/2014-12-24-map-reduce-computational-model.png">PNG</a>]</div></div></div>

<p>上图展示了用Map-Reduce统计超大规模文件中单词出现次数，红色横线将不同节点的实现分割开。对于Map节点，所有相同单词都输出到同一个节点，比如the都在第二个节点。为了保证效率，Map-Reduce都采用的是顺序读取。</p>

<h3 id="scheduling-and-data-flow">调度与数据流（scheduling and data flow）</h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-diagram.png"><img src="/assets/images/2014-12-24-map-reduce-diagram.png" alt="Map-Reduce结构" /></a><div class="caption">Figure 4:  Map-Reduce结构 [<a href="/assets/images/2014-12-24-map-reduce-diagram.png">PNG</a>]</div></div></div>

<p>Map-Reduce的数据流：</p>

<ul>
  <li>输入输出存储在分布式文件系统；</li>
  <li>中间结果存储在本地文件系统；</li>
  <li>输出通常再输入到另一个Map-Reduce任务。</li>
</ul>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-parallel.png"><img src="/assets/images/2014-12-24-map-reduce-parallel.png" alt="Map-Reduce的并行实现" /></a><div class="caption">Figure 5:  Map-Reduce的并行实现 [<a href="/assets/images/2014-12-24-map-reduce-parallel.png">PNG</a>]</div></div></div>

<p>上图是Map-Reduce分布式系统的并行实现，Partition Function部分采用Hash算法，将相同key的value映射到同一节点。</p>

<p>Map-Reduce环境的主要任务：</p>

<ul>
  <li>分割输入数据；</li>
  <li>多机之间程序调度；</li>
  <li>执行按key分组操作；</li>
  <li>处理节点故障；</li>
  <li>处理多机间通信。</li>
</ul>

<p>Map-Reduce的实现分为Master节点、Map节点和Reduce节点，Master节点的任务：</p>

<ul>
  <li>管理每个任务状态：空闲（idle，等待处理）、处理中（in-progress）、completed（结束）；</li>
  <li>将空闲任务安排到可用节点；</li>
  <li>当Map任务结束，向Master发送其R中间文件（存放在本地文件系统中）的位置和大小，每个reducer一个中间文件；</li>
  <li>Master推送信息到Reducer；</li>
  <li>Master周期性ping检测节点是否出故障。</li>
</ul>

<p>Map-Reduce系统有M个Map任务和R个Reduce任务，M比集群中的节点数目大得多，R通常比M小。</p>

<h2 id="map-reduce-4">Map-Reduce的改进</h2>

<h4 id="section-1">一、合并操作</h4>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2014-12-24-map-reduce-mapper-combiner.png"><img src="/assets/images/2014-12-24-map-reduce-mapper-combiner.png" alt="合并操作" /></a><div class="caption">Figure 6:  合并操作 [<a href="/assets/images/2014-12-24-map-reduce-mapper-combiner.png">PNG</a>]</div></div></div>

<p>通常在一个Map任务中会产生多个相同key的(k,v)对，在Map节点合并这些相同的key可有效降低网络流量，如上图所示。合并函数通常与Reduce函数相同。</p>

<p>合并时需要注意Reduce函数是否支持在Map节点的合并操作，也就是合并操作会不会改变Reduce的结果。</p>

<h4 id="section-2">二、改写分割函数</h4>

<p>例如：系统采用的默认分割函数<code>hash(key) mod R</code>可以改写为<code>hash(hostname(URL)) mod R</code>，使同一个主机的url输出到相同的文件。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：VC维</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-the-vc-dimension" />
      <pubdate>2014-12-24T05:02:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-the-vc-dimension</guid>
      <content:encoded>&lt;![CDATA[<p>本节的主要内容来自Hsuan-Tien Lin的机器学习基石课程<a href="#lin_ml_vcd_2014">[1]</a>。</p>

<p>Learning happens, if finite $d_{VC}$, large $N$, and low $E_{in}$.</p>

<h2 id="section">学习的可行性</h2>

<p>当$N\geq 2,k\geq 3$时，成长函数满足约束条件
\begin{equation}
m_{\mathcal H}(N)\leq B(N,k)=\sum_{i=0}^{k-1}\binom{N}{i}\leq N^{k-1}，
\end{equation}
由此可见，断点$k$是判断成长函数大小的重要条件。当$k\geq 3$时，才能使用上界约束$N^{k-1}$。</p>

<p>对于$\forall g=\mathcal A(\mathcal D)\in\mathcal H$和足够大的数据集$\mathcal D$，当$k\geq 3$时，
\begin{equation}
\begin{aligned}
P_{\mathcal D}\left[\lvert E_{in}(g)-E_{out}(g)\rvert&gt;\epsilon\right] \leq &amp; P_{\mathcal D}\left[\exists h\in\mathcal H\mbox{ s.t. }\lvert E_{in}(h)-E_{out}(h)\rvert&gt;\epsilon\right] \\
\leq &amp; 4m_{\mathcal H}(2N)\exp\left(-{1\over 8}\epsilon^2N\right)\\
\leq &amp; 4(2N)^{k-1}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{aligned}
\end{equation}</p>

<p>由此可知，学习是可行的，前提是满足下列条件：</p>

<ol>
  <li>好的假设集$\mathcal H$：$m_{\mathcal H}(N)$的断点是$k$；</li>
  <li>好的数据集$\mathcal D$：$N$足够大；</li>
  <li>好的演算法$\mathcal A$：$\mathcal A$能够选到一个$g$使得$E_{in}(g)$很小。</li>
</ol>

<p>其中，前两个条件可能得到$E_{out}\approx E_{in}$。</p>

<h2 id="vc">VC维</h2>

<p>$\mathcal H$的VC维$d_{VC}是指$满足$m_{\mathcal H}(N)=2^N$的最大$N$，VC维也满足：</p>

<ul>
  <li>$\mathcal H$可以打碎的最多输入数据个数；</li>
  <li>$d_{VC}=\mbox{‘minmum }k\mbox{‘} - 1$。</li>
</ul>

<p>若$N\leq d_{VC}$，$\mathcal H$能够打碎某些$N$个点的数据子集；若$k &gt; d_{VC}$，$k$必是$\mathcal H$的断点；如果$N\geq 2, d_{VC}\geq 2$， 显然有
\begin{equation}
m_{\mathcal H}(N)\leq N^{d_{VC}}。
\end{equation}</p>

<p>1维空间的正射线和正区间的$d_{VC}$分别是$1$和$2$；2维空间感知器的$d_{VC}=3$；2维空间凸包的$d_{VC}=\infty$。</p>

<p>有限$d_{VC}$的$\mathcal H$就是好的假设集。若$d_{VC}$有限，存在$g$使得$E_{out}(g)\approx E_{in}(g)$，并且不需要受学习算法$\mathcal A$、输入数据分布$P$和目标函数$f$的制约。</p>

<blockquote>
  <h4 id="section-1">练习题</h4>
  <hr />
  <p>若存在$N$个点的数据集不能被$\mathcal H$打碎，仅仅根据这条信息，可以得到关于$d_{VC}(H)$的什么结论？       </p>

  <p>［A］$d_{VC}(\mathcal H)&gt;N$；［B］$d_{VC}(\mathcal H)=N$；［C］$d_{VC}(\mathcal H)&lt;N$；［D］无法得出以上任何结论。</p>

  <p>答案：［D］。</p>
</blockquote>

<h2 id="vc-1">感知器的VC维</h2>

<p>如何证明$d$维空间感知器的VC维$d_{VC}=d+1$？若能证明$d_{VC}\geq d+1$且$d_{VC}\leq d+1$，那么就可以得到$d_{VC}=d+1$。</p>

<blockquote>
  <h4 id="dvcgeq-d1">练习题：以下哪个表明$d_{VC}\geq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［A］。</p>
</blockquote>

<p>上面练习表明，只要在$d$维空间找到一组$d+1$个点的数据集能被感知器打碎（$d+1$个点的所有二分法都能实现），那么就证明了$d_{VC}\geq d+1$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png" alt="d维空间的一组数据点" /></a><div class="caption">Figure 1:  d维空间的一组数据点 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-dvc1.png">PNG</a>]</div></div></div>

<p>这组数据点如上图所示，红色框中的数据点表述了上方$2$维空间3个点的坐标。最左边1列灰色的$1$，表示感知器的偏移常量。</p>

<p>易知，$\mathbf X$是$(d+1)\times (d+1)$的方正，且所有列线性无关（秩为$d+1$）。因此，$\mathbf X$可逆，且是$d+1$维线性空间的基。若$\mathbf y$表示任意一种二分类结果（$\mathbf X$可被打碎），那么这样的二分类方式$\mathbf w=\mathbf X^{-1}\mathbf y$总存在，因此$d_{VC}\geq d+1$。</p>

<blockquote>
  <h4 id="dvcleq-d1">练习题：以下哪个表明$d_{VC}\leq d+1$？</h4>
  <hr />

  <p>［A］存在$d+1$个输入能被打碎； <br />
［B］任何$d+1$个输入都能被打碎； <br />
［C］存在$d+2$个输入不能被打碎； <br />
［D］任何$d+2$个输入都不能被打碎。</p>

  <p>答案：［D］。</p>
</blockquote>

<p>上面练习表明，若$d$维空间任意$d+2$个点的数据集均不能被感知器打碎，那么就证明了$d_{VC}\leq d+1$。</p>

<p>若$d$维空间中任取$d+2$个点，都存在一种不能实现的二分类方法，那么这$d+2$个点就不能被打碎。</p>

<p>补上常数项$1$。因为$d+2$个$d+1$维的向量必定线性相关，任意一个均可用其它$d+1$个表示，那么
\[
\mathbf x_{d+2} = a_1\mathbf x_{1} + a_2\mathbf x_{2} + \ldots a_{d+1}\mathbf x_{d+1}，
\]
其中，$a_i(i=1,2,\dots,d+1)$不全为$0$。上式两边同时乘以$\mathbf w^T$可得
\[
\mathbf w^T\mathbf x_{d+2} = a_1\mathbf w^T\mathbf x_{1} + a_2\mathbf w^T\mathbf x_{2} + \ldots a_{d+1}\mathbf w^T\mathbf x_{d+1}。
\]
若要所有二分法都可行，上式不论右边$\mathbf x_i(i=1,2,\dots,d+1)$取何值，左边$\mathbf w^T\mathbf x_{d+2}$既可取正又可取负。采用反证法，找到一组左边$\mathbf x_i(i=1,2,\dots,d+1)$的取值，使得$\mathbf w^T\mathbf x_{d+2}$只能取正（或者负）。</p>

<p>假设这$d+2$个点能被打碎，一定存在一种分类情况使得$\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)$的符号与$a_i(i=1,2,\dots,d+1)$的符号相同，那么$a_i\mathbf w^T\mathbf x_i(i=1,2,\dots,d+1)&gt;0$，这样$\mathbf w^T\mathbf x_{d+2}$就只能取正。那么，这$d+2$个点不能被打碎，与假设矛盾。因此，$d$维空间中，任意$d+2$个点都存在不可能二分类的情况，也就是$d$维空间中的任何$d+2$个输入都不能被打碎，那么$d_{VC}\leq d+1$。</p>

<h2 id="vc-2">理解VC维</h2>

<p>VC维可以理解为假设集$\mathcal H$的自由度，它衡量了$\mathcal H$的分类能力。$d_{VC}$可以直观的认为是$\mathcal H$可调节参数的个数（但不总是这样）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png" alt="VC维相当于自由度" /></a><div class="caption">Figure 2:  VC维相当于自由度 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-vc-freedom.png">PNG</a>]</div></div></div>

<p>如上图所示，正射线只有一个可以调节参数，自由度是$1$，$d_{VC}=1$；正区间有两个可以调节参数，自由度是$2$，$d_{VC}=2$。感知器的参数向量$\mathbf w = (w_0,w_1,\ldots,w_d)$是$d+1$维（有$d+1$个可自由调节参数）和它的$d_{VC}$一致。</p>

<p>过原点的感知器的$d_{VC}=d$，因为少了一个自由度。</p>

<p>对于机器学习是否可行，有两个判断指标：（1）$E_{out}(g)\approx E_{in}(g)$？（2）$E_{in}(g)$是否足够小。$d_{VC}$（或$M$）可以作为这两个条件的评价指标。若$d_{VC}$较小，更大概率保证$E_{out}(g)\approx E_{in}(g)$，但$\mathcal H$的能力较弱，可选择的假设较少，难以使$E_{in}(g)$较小；若$d_{VC}$较大，$\mathcal H$的能力较强，可选择的假设较多，更容易使$E_{in}(g)$较小，但$E_{out}(g)\approx E_{in}(g)$的概率偏低。</p>

<h2 id="vc-3">应用VC维</h2>

<p>对于$\forall g=\mathcal A(\mathcal D)\in \mathcal H$和大的数据集$\mathcal D$，当$d_{VC}\geq 2$时，坏事儿发生概率的$VC$界为
\begin{equation}
P_{\mathcal D}\left[\left\lvert E_{in}(g)-E_{out}(g)\right\rvert&gt;\epsilon\right]\leq 4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{equation}
令$\delta=4(2N)^{d_{VC}}\exp\left(-{1\over 8}\epsilon^2N\right)$，可得$\Omega(N,\mathcal H,\delta)=\epsilon=\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}$，那么可得$E_{out}(g)$的上界
\begin{equation}
E_{out}(g)\leq E_{in}(g)+\sqrt{\frac{8}{N}\ln\left({4(2N)^{d_{VC}}\over\delta}\right)}。
\end{equation}</p>

<p>$\Omega(N,\mathcal H,\delta)$用于度量模型的复杂度，揭示了限定$E_{out}(g)$和$E_{in}(g)$差异的方法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png"><img src="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png" alt="VC维与误差的关系" /></a><div class="caption">Figure 3:  VC维与误差的关系 [<a href="/assets/images/2014-12-23-machine-learning-the-vc-dimension-pla-vc-message.png">PNG</a>]</div></div></div>

<p>上图展示了VC维和误差的关系，随着VC维$d_{VC}$的增加，模型越来越复杂，但是误差并非越来越小，$E_{out}(g)$和$E_{in}(g)$的差异却越来越大，发生坏事儿的概率变大了。由此可见，强大的$\mathcal H$（$d_{VC}$大）不总是好事儿，要选择合适的$d_{VC}^*$。</p>

<p>给定指标$\epsilon=0.1,\delta=0.1,d_{VC}=3$，可以计算大约需要$N\approx 30000$的数据集能满足要求。</p>

<p>理论上需要$N\approx 10000d_{VC}$才能满足要求，实际上通常只需$N\approx 10d_{VC}$，这是因为在推导$VC$界的时候，不等式不断放大，得到的是一个很宽松的上界。</p>

<p>$d_{VC}$虽是一个宽松的值，但可认为对所有模型都<strong>同样一致</strong>宽松，因此在模型之间比较时，仍有重要作用。</p>

<h2 id="dvc">$d_{VC}$容易计算吗？</h2>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="lin_ml_vcd_2014">[1]H.-T. Lin, “Lecture 7: The VC Dimension.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：不均衡数据问题</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-unbalanced-data-sets" />
      <pubdate>2014-12-23T22:15:19+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-unbalanced-data-sets</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">问题描述</h2>

<p>不均衡数据是指用于分类的正负样本数量差异很大。导致不均衡样本主要有两种情况：</p>

<ol>
  <li>数据本身的内在属性，比如：信用卡欺诈、异常检测、大众中癌症病人的筛查等；</li>
  <li>获取的数据不够，比如：车牌识别中的汉字。</li>
</ol>

<p>不均衡数据相关的另一个问题是小样本问题。</p>

<h2 id="section-1">处理内在不均衡</h2>

<p>内在不均衡就是指数据本身特性决定了它的不均衡性。即使获取更多的数据，仍然改变不了数据的不均衡属性。</p>

<h3 id="section-2">存在的问题</h3>

<p>这类问题主要考虑数据对机器学习的算法本身的影响。样本总数都是$N$，不同比率的正负样本对学习算法有何影响？对学习速度、学习效果有何影响？</p>

<h3 id="section-3">解决方案</h3>

<p>基本的方法是必须采用科学的<a href="/2014/11/machine-learning-advice-for-applying-machine-learning/#performance-evaluation">性能评价指标</a>，比如$F_1$ Scorce，避免大样本类淹没了小样本类。对小样本类别的分类性能也能进行有效的评估。</p>

<p><a href="/2014/12/machine-learning-anomaly-detection">异常检测</a>也是处理这类不均衡样本的方法，只对大样本类别进行建模，小样本类当作异常数据进行检测。</p>

<p>以下方法可行吗？</p>

<ol>
  <li>如果$1:1０$算是均匀的话，可以将多数类分割成为$1000$份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这$1000$个分类器用assemble的方法组合位一个分类器。</li>
  <li>设计objective function的时候给不同misclassification的情况不同的relative weights。也就是说给从小数量的样本被分成大数量的样本更大的penalty。</li>
</ol>

<h2 id="section-4">处理外在不均衡</h2>

<p>外在不均衡就是指数据本身特性并不能表明它是不均衡的，是由于数据获取手段导致数据不均衡。只要获取的数据足够多，这种不均衡就能消除。</p>

<h3 id="section-5">存在的问题</h3>

<p>从机器学习的理论来说，如果样本数$N$不够，$E_{out}$和$E_{in}$差异很大，无法学习成功，导致Low Bias问题。</p>

<h3 id="section-6">解决方案</h3>

<p>克服外在的不均衡性，需要<a href="/2014/11/machine-learning-advice-for-applying-machine-learning/#get-more-data">获得更多的数据</a>：</p>

<ol>
  <li>人工合成（伪造）数据；</li>
  <li>采集更多的数据。</li>
</ol>

<p>由于条件或成本限制，无法采集到足够的数据，在这样的情况下可以考虑人工合成。人工合成数据的前提是了解数据，拥有足够的先验知识。在OCR中，比如车牌识别时，采集到的数据可能不够，特别是有时汉字样本很少。此时，可以通过字库和随机背景融合的方法（<a href="/2014/11/machine-learning-advice-for-applying-machine-learning/#get-more-data">通过叠加高斯噪声增大样本集对提升性能帮助不大</a>），生成大量的数据。</p>

<p>更多的时候，没有足够的先验知识预知数据特性，无法合理伪造，还得采集跟多的数据。机器学习本来就是探究数据特性的过程。</p>

<h2 id="section-7">其它建议</h2>

<p><a href="http://www.weibo.com/n/机器学习那些事儿">@机器学习那些事儿</a>发起过关于<a href="http://www.weibo.com/p/1001603785752793219283?sudaref=ml.memect.com">不均匀正负样本分布下的机器学习</a>的讨论，部分建议摘录如下：</p>

<ol>
  <li>上采样、下采样、代价敏感，没什么好办法。</li>
  <li>这个之前调研过，主要分重采样和欠采样！这种不平衡是因为比率的不平衡给一些学习方法带来问题。但是在某些领域，比如反欺诈和安全，不仅是比率极不平衡，而且是正样本样本绝对数很小。需要扩散正样本方法！</li>
  <li>Synthetic Minority Over-sampling Technique 我试过这个方法，解决部分问题，主要还是需要增加样本在特征空间的覆盖！ 工程上光靠算法也解决不了问题，有的还是需要加入下经验知识来做。</li>
  <li>用排序思想构造所谓的序对。</li>
  <li>如果1：1０算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。记得读到的论文可行，但没有验证过。</li>
  <li>标准解决方法：设计objective function的时候给不同misclassification的情况不同的relative weights。也就是说给从小数量的样本被分成大数量的样本更大的penalty。</li>
  <li>训练数据与预测数据分布不一致，有专门研究的课题，sample selection bias，主要方法是各种reweighting。</li>
  <li>这个倒是可以参考positive only learning等半监督学习中如早期的spy算法等来构造合适的负例来解决正负例不平衡的问题。</li>
  <li>这个看起来像 one-class recommendation 问题，不知是否可以考虑转化成 learning to rank 问题，如果不是为了拟合一个分布的话。</li>
  <li>这在机器学习里面被称类别不平衡问题，可以参考Haibo, H. and E. A. Garcia (2009). “Learning from Imbalanced Data.” Knowledge and Data Engineering, IEEE Transactions on” 的survey.已有很多方法提出。</li>
  <li>个人觉得在类别不平衡条件下，Transductive SVM (TSVM)应该对于的active learning 来标注，可能结果更好。</li>
  <li>learning to rank对于训练数据量的要求较高，同时要确定用于learning to rank的pair，还是需要找到负例，从而将正例和负例形成偏序配对。所以learning to rank是一种方法，但个人认为这会将简单问题复杂化，且本质还是需要去找负例。</li>
</ol>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：泛化理论</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-theory-of-generalization" />
      <pubdate>2014-12-20T07:36:39+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-theory-of-generalization</guid>
      <content:encoded>&lt;![CDATA[<p>本节的主要内容来自Hsuan-Tien Lin的机器学习基石课程<a href="#lin_ml_tg_2014">[1]</a>。</p>

<p>泛化是指在训练集上得到的模型可以推广到整个数据集，也就是$E_{out}\approx E_{in}$，就是要使坏事$\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon$发生的概率足够小。</p>

<p>Generelization: $E_{out}\approx E_{in}$ possible, if $m_{\mathcal H}(N)$ breaks somewhere and $N$ large enough.</p>

<h2 id="mmathcal-hn">最大可能的$m_{\mathcal H}(N)$</h2>

<p>如何通过断点$k$限定$m_{\mathcal H}(N)$的值？</p>

<p>由前文可得，正射线的断点是$2$，$m_{\mathcal H}(2)＝3$；正区间的断点是$3$，$m_{\mathcal H}(3)＝7$；2维感知器的断点是$4$，$m_{\mathcal H}(4)＝14$。</p>

<p>如果最小断点$k=2$，在$N=1,2,3,\ldots$的情况下，对任意的$\mathcal H$而言，可能得到的最大$m_{\mathcal H}(N)$是多少呢？</p>

<ul>
  <li>若$N=1$，只有1个点，永远不存在2个点能被打碎的情况，因此$m_{\mathcal H}(1)=2^1=2$；</li>
  <li>若$N=2$，由于最小断点$k=2$，不能打碎$2$个点，因此$m_{\mathcal H}(2)&lt;2^2=4$，最多为$3$；</li>
  <li>若$N=3$，$3$个点中任取$2$个点都不能被打碎时，最多可能的二分法有多少种？</li>
  <li>……</li>
</ul>

<p>当$N=3,k=2$时，如果$m_{\mathcal H}(3)＝3$，可以肯定从3个点中任意抽取2个都不会被打碎，因为打碎2个点至少要4种二分类方法，因此$m_{\mathcal H}(3)$最少为3，可以从$4$开始考察。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-max-mH(N).png"><img src="/assets/images/2014-12-19-machine-learning-theory-of-generalization-max-mH(N).png" alt="4个二分类的情况" /></a><div class="caption">Figure 1:  4个二分类的情况 [<a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-max-mH(N).png">PNG</a>]</div></div></div>

<p>对于$4$个二分类的情况，如上图所示。上图左的二分类使得$\mathbf x_2$和$\mathbf x_3$被打碎了；上图右修改了最后一种分类方法，任意两个点都没有被打碎，因此$m_{\mathcal H}(3)$最少为4。</p>

<p>还需考察$m_{\mathcal H}(3)=5$的情况。结果表明，$m_{\mathcal H}(3)=5$时总会让其中的$2$个点被打碎。因此，若$N=3,k=2$，最多可能的二分法只有$4$种。</p>

<h2 id="section">上限函数</h2>

<p>期望对$m_{\mathcal H}(N)$进一步进行限定，
\[
m_{\mathcal H}(N)\leq\mbox{maximum possible }m_{\mathcal H}(N)\mbox{ given }k\leq\mbox{poly}(N)。
\]</p>

<p>当断点为$k$时，将最大可能的$m_{\mathcal H}(N)$定义为上限函数（bounding function）$B(N,k)$。该上限函数和假设集$\mathcal H$无关，不受感知器等特定分类器的约束，它的取值是所有$\mathcal H$中的最大值。</p>

<p>$m_{\mathcal H}(N)$受假设集（分类器）$\mathcal H$和样本点数目$N$的约束；$B(N,k)$受样本点数目$N$和断点$k$约束，而与假设集$\mathcal H$无关。但是，如果知道假设集$\mathcal H$的断点$k$，就可以用$B(N,k)$对$m_{\mathcal H}(N)$进一步约束，$m_{\mathcal H}(N)\leq B(N,k)$。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-B(N,k).png"><img src="/assets/images/2014-12-19-machine-learning-theory-of-generalization-B(N,k).png" alt="上限函数计算表" /></a><div class="caption">Figure 2:  上限函数计算表 [<a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-B(N,k).png">PNG</a>]</div></div></div>

<p>当$N\leq k$时$B(N,k)$的计算公式容易推导，当$N&gt;k$时$B(N,k)$的计算比较复杂，
\begin{equation}
B(N,k)=
\left\{
\begin{aligned}
&amp; 2^N &amp; N&lt;k \\
&amp; 2^N-1 &amp; N=k \\
&amp; \sum_{i=1}^{k-1}\binom{N}{i} &amp; N&gt;k
\end{aligned}
\right. 。
\end{equation}</p>

<h2 id="vc">VC界</h2>
<p>通过
\[
P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right]\leq 2m_{\mathcal H}(N)\exp\left(-2\epsilon^2N\right)
\]
证明VC界（Vapnik-Chervonenkis bound）
\begin{equation}
P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right]\leq 4m_{\mathcal H}(2N)\exp\left(-{1\over 8}\epsilon^2N\right)
\label{eq:vc-bound}
\end{equation}
分三步，对应着3个常数的变化。</p>

<h4 id="eineout">第一步：用$E’_{in}$替换$E_{out}$</h4>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-Eout_Ein.png"><img src="/assets/images/2014-12-19-machine-learning-theory-of-generalization-Eout_Ein.png" alt="替换掉out-sample误差" /></a><div class="caption">Figure 3:  替换掉out-sample误差 [<a href="/assets/images/2014-12-19-machine-learning-theory-of-generalization-Eout_Ein.png">PNG</a>]</div></div></div>

<p>另抽取$N$个点的数据集$\mathcal D’ $计算$E’_{in}$来估计$E_{out}$的值，$E’_{in}$和$E_{in}$的取值如上图，以$E_{out}$为中心分布。${1\over 2}P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right]$对应着上图粉色区域的面积（取$1\over 2$是忽略对称的左半区域），在满足该条件下，上图淡绿区域对应着$P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;\epsilon\right]$，于是显然有</p>

<p>\[
\begin{aligned}
{1\over 2}P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right] 
\leq &amp; P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;\epsilon\right]  \\
\leq &amp; P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;{\epsilon\over 2}\right] 。
\end{aligned}
\]</p>

<h4 id="section-1">第二步：利用成长函数约束二分类情况的数量</h4>

<p>上一步将问题转化为只考虑$N$个点数据集$\mathcal D $和$N$个点数据集$\mathcal D’ $的问题，最多有$m_{\mathcal H}(2N)$种二分类情况，可进一步得到坏事儿发生概率的上界</p>

<p>\[
\begin{aligned}
P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right] 
\leq &amp; 2P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;{\epsilon\over 2}\right]  \\
\leq &amp; 2m_{\mathcal H}(2N)P\left[\mbox{fixed } h\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;{\epsilon\over 2}\right] 。
\end{aligned}
\]</p>

<h4 id="hoeffding-without-replacement">第三步：利用Hoeffding without Replacement约束</h4>

<p>易知$\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;{\epsilon\over 2}$和$\left\lvert E_{in}(h)-\frac{E_{in}(h)+E’_{in}(h)}{2}\right\rvert&gt;{\epsilon\over 4}$等价，$\frac{E_{in}(h)+E’_{in}(h)}{2}$可以认为是out-sample数据的误差估计，所以直接利用Hoeffding可得</p>

<p>\[
\begin{aligned}
P\left[\exists h\in\mathcal H\mbox{ s.t. }\left\lvert E_{in}(h)-E_{out}(h)\right\rvert&gt;\epsilon\right] \leq &amp; 2m_{\mathcal H}(2N)P\left[\mbox{fixed } h\mbox{ s.t. }\left\lvert E_{in}(h)-E’_{in}(h)\right\rvert&gt;{\epsilon\over 2}\right] \\
= &amp; 2m_{\mathcal H}(2N)P\left[\mbox{fixed } h\mbox{ s.t. }\left\lvert E_{in}(h)-\frac{E_{in}(h)+E’_{in}(h)}{2}\right\rvert&gt;{\epsilon\over 4}\right] \\
\leq &amp; 2m_{\mathcal H}(2N)\cdot 2\exp\left(-2\left(\epsilon\over 4\right)^2N\right)\\
= &amp; 4m_{\mathcal H}(2N)\exp\left(-{1\over 8}\epsilon^2N\right)。
\end{aligned}
\]</p>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="lin_ml_tg_2014">[1]H.-T. Lin, “Lecture 6: Theory of Generalization.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
