<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2014-12-12T02:36:43+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>机器学习：大数据上的机器学习</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-large-scale-machine-learning" />
      <pubdate>2014-12-10T19:51:50+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-large-scale-machine-learning</guid>
      <content:encoded>&lt;![CDATA[<p>当机器学习面对大数据的时候，是否从大数据中抽取一个小的子集就可以了呢？这需要分析学习曲线，确定影响性能的关键问题是数据量、特征还是模型或其它问题。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-is-needed.png"><img src="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-is-needed.png" alt="学习曲线" /></a><div class="caption">Figure 1:  学习曲线 [<a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-is-needed.png">PNG</a>]</div></div></div>

<p>如果是上图左所示的High Variance情况，则采用大数据能提高模型效果。若从大数据中抽取$m=1000$个样本的训练，如上图右所示，这表明机器学习是High Bias，即使加入更多的数据对性能也没有大的提升，应先加入更多的新特征（若神经网络，则增加神经元），再考虑大数据上的训练是否有利。</p>

<p>如何减少学习时间提高学习效率，是大数据上的机器学习需要解决的重要问题。</p>

<h2 id="section">随机梯度下降法</h2>

<h3 id="section-1">随机梯度下降法</h3>

<p>对于梯度下降法，参数更新的方法是
\begin{equation}
\boldsymbol\theta_j := \boldsymbol\theta_j - \alpha\frac{1}{m}\sum_{i=1}^m\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}，
\end{equation}
这种方法叫做批量（batch）梯度下降法，参数更新需在整个训练集上计算一次，当$m$特别大的时候，速度就会很慢。随机梯度下降法（stochastic gradient descent）的更新方式是每次只用一个数据点更新参数。</p>

<blockquote>
  <h4 id="section-2">随机梯度下降法</h4>
  <hr />

  <ol>
    <li>数据集随机化；</li>
    <li>更新模型参数，  <br />
Repeat <sup id="fnref:sgd-cycle-times"><a href="#fn:sgd-cycle-times" class="footnote">1</a></sup> {  // 通常是$1\sim 10$轮迭。 <br />
 for $i := 1,\dots,m$ {
\begin{equation}
\boldsymbol\theta_j := \boldsymbol\theta_j - \alpha\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}~~(j=1,\ldots,n)
\end{equation}
 }  }。</li>
  </ol>

</blockquote>

<p>如下图所示，批量梯度下降法通常会向着极小值逼近，随机梯度下降法逼近道路稍显曲折，最总结果通常在极小值的某个区域内徘徊。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-illustration-gradient-descent.png"><img src="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-illustration-gradient-descent.png" alt="左：批量梯度下降法，右：随机梯度下降法" /></a><div class="caption">Figure 2:  左：批量梯度下降法，右：随机梯度下降法 [<a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-illustration-gradient-descent.png">PNG</a>]</div></div></div>

<h3 id="section-3">小批量梯度下降法</h3>

<p>随机梯度法每次更新参数只需要一个数据点，批量梯度法每次更新参数只需要整个训练集参数。小批量（mini-batch）梯度下降法间于二者之间，每次更新参数利用训练集的一个小子集的$b$个数据点（通常$b=2,\ldots,100$）。小批量梯度下降法的参数更新规则为
\begin{equation}
\boldsymbol\theta_j := \boldsymbol\theta_j - \frac{\alpha}{b}\sum_{i=k}^{k+b-1}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}，
\end{equation}
其中$k=1,b+1,2b+1,3b+1,\ldots$。</p>

<p>小批量梯度下降法可利用并行化，获得比随机梯度法更快的速度，但是又多了参数$b$需要调节。</p>

<h3 id="section-4">收敛性判断</h3>

<p>随机梯度下降法可利用代价函数曲线，判断迭代过程是否收敛。但是，随机梯度下降法不会像批量梯度下降法那样，在整个训练集上评估代价。</p>

<p>在利用$\left(\mathbf x^{(i)},y^{(i)}\right)$更新参数$\boldsymbol\theta_j$之前，计算该点的代价（误差）
\begin{equation}
\mbox{cost}\left(\boldsymbol\theta,\left(\mathbf x^{(i)},y^{(i)}\right)\right)= {1\over 2}\left(h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)-y^{(i)}\right)^2，
\end{equation}
若是在参数更新之后再计算误差，不能真实反映迭代的误差。将最近多次（比如$1000$）迭代的误差平均，作为代价函数曲线上的一个点，下图就是随机梯度下降法的代价函数曲线。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-checking-for-convergence.png"><img src="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-checking-for-convergence.png" alt="代价函数曲线" /></a><div class="caption">Figure 3:  代价函数曲线 [<a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-checking-for-convergence.png">PNG</a>]</div></div></div>

<p>小的$\alpha$能否得到更好的结果呢？随机梯度下降法的性质表明其结果会在极小值附近区域震荡，很小的学习率$\alpha$可能得到的结果也只是好一点点而已。如上图左上所示，小的$\alpha$得到稍微光滑一点的曲线，但效果提升并不明显。</p>

<p>上图右下的代价函数曲线不降反升，是因为学习率$\alpha$过大，可以适当调小学习率。</p>

<p>选择最近多少个点平均作为代价函数曲线的点合适呢？</p>

<ul>
  <li>点的数目多，代价函数曲线更光滑，需要较长时间才展示参数更新结果，不能及时的反应参数更新的情况。上图右上所示，更多数目的点平均得到的曲线更光滑。</li>
  <li>点的数目少，代价函数曲线噪声更大。上图左下所示，过少数目的点导致曲线震荡厉害，看不到变化趋势；过多的点又导致曲线过于平坦，也看不到变化趋势。</li>
</ul>

<p>因此，选择点的数目需要综合考虑这些情况，便于观察判断迭代是否收敛。</p>

<p>为了随机梯度下降法能更好逼近极小值，可动态调整学习效率$\alpha=\frac{\mbox{constant1}}{\mbox{interationNumber + constant2}}$，学习过程中不断减小$\alpha$，越是靠近极小值的地方更新步长越短。但是，这又多出两个参数$\mbox{constant1}$和$\mbox{constant2}$需要调节了，该方法也不十分常用。</p>

<h2 id="section-5">在线学习</h2>

<p>在线学习通常不需要维护一个固定的训练集，思想上和随机梯度法类似，每次新数据来，学习更新模型，然后继续接收新数据，继续更新模型……在线学习适合用于数据集动态缓慢变化的情况，模型随可随数据变换动态演进。</p>

<p>对于购物网站来说，随着经济大环境的改变，用户对价格的敏感度也会变化，用户特性会随时变迁，在线学习及时通过新样本训练模型可适应这些变化。</p>

<p>预测CTR（click-through rate）是经典的在线学习例子。比如用户在网站搜索手机，返回用户最可能点击的10个结果。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-online-example.png"><img src="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-online-example.png" alt="预测CTR" /></a><div class="caption">Figure 4:  预测CTR [<a href="/assets/images/2014-12-10-machine-learning-large-scale-machine-learning-online-example.png">PNG</a>]</div></div></div>

<p>如何计算用户点击的概率呢？将搜索匹配的单词等作为特征向量，利用logistic回归可以估计用户点击概率。推荐系统的协同过滤算法学习到的特征向量，也可作为logistic的输入特征。</p>

<p>每次用户搜索可以得到对这10个搜索结果的反馈（是否点击了），从而得到10组数据，这些数据又可以用来训练模型。</p>

<h2 id="mapreduce">MapReduce</h2>

<p>MapReduce可将学习任务分配到多台机器上，然后将这些机器的学习结果汇总得到整个学习结果。</p>

<blockquote>
  <h4 id="mapreduce-1">基于MapReduce的批量梯度学习算法</h4>
  <hr />
  <p>整个任务：$\boldsymbol\theta_j := \boldsymbol\theta_j - \alpha\frac{1}{400}\sum_{i=1}^{400}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}$。</p>

  <p>分配任务：</p>

  <ul>
    <li>Machine 1: $temp_j^{(1)}=\sum_{i=1}^{100}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}$；</li>
    <li>Machine 2: $temp_j^{(2)}=\sum_{i={101}}^{200}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}$；</li>
    <li>Machine 3: $temp_j^{(3)}=\sum_{i={201}}^{300}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}$；</li>
    <li>Machine 4: $temp_j^{(4)}=\sum_{i={301}}^{400}\left(h_\boldsymbol\theta\left(\mathbf x^{(i)}\right)-y^{(i)}\right)\mathbf x_j^{(i)}$。</li>
  </ul>

  <p>任务合并：$\boldsymbol\theta_j := \boldsymbol\theta_j - \alpha\frac{1}{400}\left(temp_j^{(1)}+temp_j^{(2)}+temp_j^{(3)}+temp_j^{(4)}\right)$。</p>
</blockquote>

<h2 id="section-6">参考资料</h2>

<h3 id="section-7">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:sgd-cycle-times">
      <p>对于大数据，通常一轮迭代（每个点参与一次）也能得到较好结果，通常进行$1\sim 10$轮迭代（这个还依赖于训练集的大小）。 <a href="#fnref:sgd-cycle-times" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>计算广告学：广告基础</title>
      <link href="http://qianjiye.de/2014/12/computational-advertising-foundation" />
      <pubdate>2014-12-09T09:02:19+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/computational-advertising-foundation</guid>
      <content:encoded>&lt;![CDATA[<p>本文的内容来自于刘鹏的网络课程《计算广告学》<a href="#liu_cad_foundations_2014">[1]</a>。</p>

<h2 id="tips">tips</h2>

<p>广告和推荐系统很相似。对广告而言，在同样位置文字链的点击率远远高于图片；对推荐而言，图片的点击率远远高于文字链。</p>

<h2 id="section">广告目的</h2>

<p>广告（advertising）是由已确定的出资人通过各种媒介进行的有关产品（商品、服务和观点）的，通常是有偿的、有组织的、综合的、劝服性的非人员的信息传播活动<a href="#arens_ad_2013">[2]</a>。</p>

<p>广告的三主体是出资人（sponsor）即广告主（advertiser）、媒介（medium）、受众（audience）<sup id="fnref:search-001"><a href="#fn:search-001" class="footnote">1</a></sup>，这是计算广告学中的三个基本变量。广告是一个三方博弈问题。</p>

<p>广告的本质功能是借助某种有广泛受众的媒介力量，完成较低成本的用户接触（不是向用户卖东西哦）。</p>

<p>品牌广告（brand awareness）：创造独特良好的品牌或产品形象，目的在于提升较长时期内的离线转化率。</p>

<p>效果广告（direct response）：有短期内明确用户转化行为诉求的广告，用户转化行为有购买、注册、投票、捐款等。</p>

<h2 id="section-1">广告的有效性模型</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-09-computational-advertising-effect-model.png"><img src="/assets/images/2014-12-09-computational-advertising-effect-model.png" alt="广告的有效性模型" /></a><div class="caption">Figure 1:  广告的有效性模型 [<a href="/assets/images/2014-12-09-computational-advertising-effect-model.png">PNG</a>]</div></div></div>

<p>曝光阶段，广告位对广告的曝光率和点击率有决定性作用，这种资源优势不是通过算法改进可以获得的。</p>

<p>关注阶段，可以通过技术手段提高用户关注度。不要打断用户而言，比如ad系统通过上下文推荐；对揭示推荐原因而言，比如租车行的广告根据用户所在地理更换背景图片。</p>

<p>信息接受阶段，广告位影响着广告的认可度，比如大的品牌广告主要确保自己的广告投放到影响力大和没有负面影响的媒介，还要关注竞争对手的广告投放情况。</p>

<p>评价在线广告的两个基本指标：点击率和转化率。一般而言，越靠前的阶段对点击率影响越大，越靠后的阶段对转化率影响越大。</p>

<p>下图展示了一些广告策略的效果，＋表示正面作用，－表示负面作用。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-09-computational-advertising-improve-ad-performance.png"><img src="/assets/images/2014-12-09-computational-advertising-improve-ad-performance.png" alt="一些广告策略的效果" /></a><div class="caption">Figure 2:  一些广告策略的效果 [<a href="/assets/images/2014-12-09-computational-advertising-improve-ad-performance.png">PNG</a>]</div></div></div>

<h2 id="section-2">广告与营销的区别</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-12-09-computational-advertising-ad-vs-sale.png"><img src="/assets/images/2014-12-09-computational-advertising-ad-vs-sale.png" alt="广告与营销的区别" /></a><div class="caption">Figure 3:  广告与营销的区别 [<a href="/assets/images/2014-12-09-computational-advertising-ad-vs-sale.png">PNG</a>]</div></div></div>

<p>就效果广告（direct response）而言，从硬广到返利网，效果越来越好<sup id="fnref:problem-001"><a href="#fn:problem-001" class="footnote">2</a></sup>。当然，各种形式的广告之间有相互作用，配合使用可提升效果。</p>

<h2 id="section-3">参考文献</h2>

<ol class="bibliography"><li><span id="liu_cad_foundations_2014">[1]刘鹏, “广告的基本知识.” 云课堂, 2014.</span>

[<a href="http://study.163.com/c/ad">Online</a>]

</li>
<li><span id="arens_ad_2013">[2]威廉·阿伦斯, <i>当代广告学</i>. 人民邮电出版社, 2013.</span>

</li></ol>

<h3 id="section-4">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:search-001">
      <p>搜索有两个主体，用户和搜索引擎。 <a href="#fnref:search-001" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:problem-001">
      <p>如何理解把你的客户卖给你，把竞争对手的客户买给你？ <a href="#fnref:problem-001" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：推荐系统</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-recommender-systems" />
      <pubdate>2014-12-09T07:20:25+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-recommender-systems</guid>
      <content:encoded>&lt;![CDATA[<p>推荐系统首先通过分析用户的使用历史获得用户的兴趣偏向，然后使用得到的用户兴趣偏向获得用户潜在感兴趣的产品或服务。鉴于产生推荐的方式不同，推荐系统通常可以分为以下三类<a href="#wu_thesis_bju_2010">[1]</a>：基于内容的过滤（content-based filtering）、协同过滤（collaborative filtering）和CBF与CF的混合过滤（hybrid filtering）。</p>

<p>以电影的推荐系统为例，相关变量定义如下<a href="#ng_ml_rs_2014">[2]</a>：</p>

<ul>
  <li>$n_u$：用户数目；</li>
  <li>$n_m$：电影数目；</li>
  <li>$\mathbf r(i, j)$：若用户$j$对电影$i$进行了评分则赋值为1；</li>
  <li>$\mathbf y^{(i, j)}$：用户$j$对电影$i$的评分；</li>
  <li>$\boldsymbol\theta^{(j)}$：用户$j$的参数向量；</li>
  <li>$\mathbf x^{(i)}$：电影$i$的特征向量；</li>
  <li>$m^{(j)}$：用户$j$评价过的电影数目。</li>
</ul>

<h2 id="section">基于内容的过滤</h2>

<p>基于内容的过滤（CBF）方法根据抽取出的用户和产品特征获得推荐。这类方法利用用户和产品的特征计算他们之间的匹配度，最终把匹配得最好的数个产品推荐给相应的用户。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-recommender-systems-recommender-data.png"><img src="/assets/images/2014-12-08-machine-learning-recommender-systems-recommender-data.png" alt="用户对电影的评价" /></a><div class="caption">Figure 1:  用户对电影的评价 [<a href="/assets/images/2014-12-08-machine-learning-recommender-systems-recommender-data.png">PNG</a>]</div></div></div>

<p>上图展示了用户对电影的评分$0\sim 5$，用属于爱情片（romance）和动作片（action）的概率表示电影特征。用户评分<code>?</code>表示用户$j$未对电影$i$作出评价，也就是$\mathbf r(i, j)=0$。</p>

<p>如何估计用户未评价电影的得分呢？这时一个线性回归问题。利用电影特征向量$\mathbf x^{(i)}$和参数$\boldsymbol\theta^{(j)}$，可估计用户$j$对电影$i$的评分
\begin{equation}
\mathbf y^{(i, j)} = \left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)}。
\label{eq:user-rating}
\end{equation}</p>

<p>因此，基于内容的的推荐需要学习参数$\boldsymbol\theta^{(1)},\boldsymbol\theta^{(2)},\dots,\boldsymbol\theta^{(n_u)}$。假设已知$\mathbf x^{(1)}, \mathbf x^{(2)}, \ldots, \mathbf x^{(n_m)}$，学习算法采用代价函数
\begin{equation}
\begin{aligned}
J\left(\boldsymbol\theta^{(1)},\dots,\boldsymbol\theta^{(n_u)}\right) = &amp;{1\over 2}\sum_{j=1}^{n_u}\sum_{i:\mathbf r(i, j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)^2 + \\
&amp;{\lambda\over 2}\sum_{j=1}^{n_u}\sum_{k=1}^n\left(\boldsymbol\theta_k^{(j)}\right)^2，
\end{aligned}
\label{eq:cf-learn-user-parameters}
\end{equation}
由于电影评分的总数目是固定的，省去平均化过程对最小化代价函数没有影响，只保留$1\over 2$方便求导，梯度下降法更新参数的规则为
\begin{equation}
\begin{aligned}
\boldsymbol\theta_k^{(j)}&amp;:=\boldsymbol\theta_k^{(j)}-\alpha\sum_{i:r(i,j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)\mathbf x_k^{(i)} &amp; (k=0)\\
\boldsymbol\theta_k^{(j)}&amp;:=\boldsymbol\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)\mathbf x_k^{(i)}+\lambda\boldsymbol\theta_k^{(j)}\right)&amp;(k\neq 0)
\end{aligned}。
\end{equation}</p>

<p>CBF就是把某用户评价高的电影推荐给该用户。为了实现推荐，需要计算用户对所有电影的评分，计算复杂度高。</p>

<p>CBF方法具有两个主要的缺陷：</p>

<ol>
  <li>CBF需要预处理产品以得到代表它们的特征，但这种预处理在实际问题中往往非常困难；</li>
  <li>CBF推荐给某个用户的产品往往和此用户已经消费过的产品很相似，它们无法发现用户并不熟悉但具有潜在兴趣的产品种类。 </li>
</ol>

<h2 id="section-1">协同过滤</h2>

<p>协同过滤（CF）方法不需要事先获得产品或用户的特征，它们只依赖于用户过去的行为（如对产品的浏览、评价或购买等）。 通过用户过去的行为企业可以收集用户对产品的显式评分（如Netflix）或隐式评分（如Google新闻）。通常 CF方法首先分析已经收集到的用户-产品评分对中所呈现的用户与产品的相互作用，然后它们使用这些相互作用为用户产生个性化产品推荐。</p>

<p>CF通常分为基于产品的（item-based）推荐和基于用户的（user-based）推荐。基于用户的推荐假设如果两个用户过去对产品有相似的喜好，那么他们现在对产品仍有相似的喜好；基于产品的推荐假设如果某个用户过去喜欢某种产品，那么该用户现在仍喜欢与此产品相似的产品。 </p>

<p>在实际应用中，电影的特征向量往往也是未知的，需要通过学习得到。协同过滤不仅可以学习到用户对电影的评价，而且能学习到电影的特征。</p>

<p>电影特征向量学习与用户评价模型参数估计\eqref{eq:cf-learn-user-parameters}类似，假设已知$\boldsymbol\theta^{(1)},\boldsymbol\theta^{(2)},\dots,\boldsymbol\theta^{(n_u)}$，学习$\mathbf x^{(1)}, \mathbf x^{(2)}, \ldots, \mathbf x^{(n_m)}$采用的代价函数为
\begin{equation}
\begin{aligned}
J\left(\mathbf x^{(1)}, \ldots, \mathbf x^{(n_m)}\right) = &amp;{1\over 2}\sum_{i=1}^{n_m}\sum_{i:\mathbf r(i, j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)^2 + \\
&amp;{\lambda\over 2}\sum_{i=1}^{n_m}\sum_{k=1}^n\left(\mathbf x_k^{(i)}\right)^2
\end{aligned}。
\label{eq:cf-learn-item-features}
\end{equation}</p>

<p>根据\eqref{eq:cf-learn-user-parameters}和\eqref{eq:cf-learn-item-features}，同时学习$\mathbf x^{(1)}, \mathbf x^{(2)}, \ldots, \mathbf x^{(n_m)}$和$\boldsymbol\theta^{(1)},\boldsymbol\theta^{(2)},\dots,\boldsymbol\theta^{(n_u)}$可以采用代价函数
\begin{equation}
\begin{aligned}
J\left(\mathbf x^{(1)},\ldots,\mathbf x^{(n_m)},\boldsymbol\theta^{(1)},\dots,\boldsymbol\theta^{(n_u)}\right) = &amp;{1\over 2}\sum_{(i,j):\mathbf r(i, j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)^2 + \\
&amp;{\lambda\over 2}\sum_{i=1}^{n_m}\sum_{k=1}^n\left(\mathbf x_k^{(i)}\right)^2+{\lambda\over 2}\sum_{j=1}^{n_u}\sum_{k=1}^n\left(\boldsymbol\theta_k^{(j)}\right)^2
\end{aligned}，
\label{eq:cf-learn-all-parameters}
\end{equation}
梯度下降法更新参数的规则为
\begin{equation}
\begin{aligned}
\mathbf x_k^{(i)}&amp;:=\mathbf x_k^{(i)}-\alpha\left(\sum_{j:r(i,j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)\boldsymbol\theta_k^{(j)}+\lambda\mathbf x_k^{(i)}\right)\\
\boldsymbol\theta_k^{(j)}&amp;:=\boldsymbol\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}\left(\left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)} - \mathbf y^{(i, j)}\right)\mathbf x_k^{(i)}+\lambda\boldsymbol\theta_k^{(j)}\right)
\end{aligned}，
\label{eq:grd-x-theta}
\end{equation}
由于电影的特征$\mathbf x$也通过学习得到，不再需要额外增加$\mathbf x_0^{(i)}=1$的项。</p>

<blockquote>
  <h4 id="section-2">协同过滤算法</h4>
  <hr />

  <ol>
    <li>用小的随机变量初始化$\mathbf x^{(1)},\ldots,\mathbf x^{(n_m)},\boldsymbol\theta^{(1)},\dots,\boldsymbol\theta^{(n_u)}$<sup id="fnref:how-item-feature-demension"><a href="#fn:how-item-feature-demension" class="footnote">1</a></sup>；</li>
    <li>最小化代价函数\eqref{eq:cf-learn-all-parameters}，若采用梯度下降法，采用\eqref{eq:grd-x-theta}更新参数；</li>
    <li>学习到参数后，利用\eqref{eq:user-rating}计算用户$j$对电影$i$的评分。</li>
  </ol>
</blockquote>

<p>协同过滤算法是一个不断进化的过程，$\mathbf x^{(i)}$和$\boldsymbol\theta^{(j)}$相互作用，$\mathbf x^{(i)}$推动$\boldsymbol\theta^{(j)}$更新，$\boldsymbol\theta^{(j)}$也推动$\mathbf x^{(i)}$更新。</p>

<p>当学习到电影的特征向量$\mathbf x^{(i)}$后，可以用$\left\lVert \mathbf x^{(i_1)}-\mathbf x^{(i_2)}\right\rVert$计算电影之间的相似度。通过电影的相似度，为用户推荐相关的电影，这就是基于物品的推荐方法。</p>

<p>CF方法存在的主要问题：</p>

<ol>
  <li>冷启动：对于一个新用户，由于缺乏其对产品的评分，CF无法为其提供可靠的 产品推荐；对于一种新的产品，CF无法确定该把它推荐给哪些用户。</li>
  <li>可扩展性：CF方法中可能涉及到数以百万计的用户为成千上万种产品提供的评分。传统的CF推荐算法通常需要计算每对用户或产品之间的相似度，然后把这些相似度存放至电脑的主存中以便高效地产生推荐。当用户或产品的数量较大时，计算复杂度很高。 </li>
</ol>

<h3 id="section-3">均值规范化</h3>

<p>针对新用户的冷启动，该用户$j$从未对任何电影做出评价，协同过滤算法会得到用户的参数向量$\boldsymbol\theta^{(j)}=\mathbf 0$，如果估计该用户对电影的评分，也将全为$0$，这显然不符合逻辑。因此，需要对数据进行适当的处理，避免这样的情况发生。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-recommender-systems-mean-normalization.png"><img src="/assets/images/2014-12-08-machine-learning-recommender-systems-mean-normalization.png" alt="均值规范化" /></a><div class="caption">Figure 2:  均值规范化 [<a href="/assets/images/2014-12-08-machine-learning-recommender-systems-mean-normalization.png">PNG</a>]</div></div></div>

<p>定义用户对所有电影评价的均值为$\boldsymbol\mu$，重新对用户对电影的评分规范化
\begin{equation}
\mathbf y^{(i,j)} := \mathbf y^{(i,j)} - \boldsymbol\mu_i。
\end{equation}
规范化评分后，公式\eqref{eq:user-rating}计算用户$j$对电影$i$的评分规则变为
\begin{equation}
\mathbf y^{(i, j)} = \left(\boldsymbol\theta^{(j)}\right)^T\mathbf x^{(i)}+\boldsymbol\mu_i，
\end{equation}
即使参数向量$\boldsymbol\theta^{(j)}=\mathbf 0$，估计新用户对电影的评分将是所有用户的平均分，合情合理。</p>

<h3 id="section-4">思考问题</h3>

<ul>
  <li>如果系统已经有了部分标注的特征$\mathbf x$，如何融入到协同过滤算法中？</li>
  <li>如何利用协同过滤提高广告点击率？</li>
  <li>协同过滤的特征学习方法可以做传感器校准么？</li>
  <li>协同过滤和广联规则挖掘有何联系？</li>
  <li>协同过滤的特征学习可以解决盲源信号分离么？</li>
</ul>

<h2 id="section-5">混合过滤</h2>

<p>混合过滤(HF)组合CBF和CF，以期在克服它们各自缺点的同时，融合它们特有的优势。通常组合的方式包括以下三种：</p>

<ol>
  <li>加权（weighted）组合：首先分别独立应用CBF和CF获得对产品的预测评分，然后组合它们的预测评分以便获得混合过滤的预测评分，最后根据混合预测评分为用户产生推荐列表。</li>
  <li>混合（mixed）组合：首先分别独立应用CBF和CF产生各自的推荐列表，然后组合这两组推荐列表以便获得最终的推荐列表。</li>
  <li>序贯（sequential）组合：当可用评分较少时使用CBF方法获得用户的特征并进行推荐；而当可用评分积聚到一定程度时，使用CF方法代替原来的CBF方法获得最终的推荐列表。 </li>
</ol>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"><li><span id="wu_thesis_bju_2010">[1]吴金龙, “Netflix Prize 中的协同过滤算法,” PhD thesis, 北京大学, 2010.</span>

</li>
<li><span id="ng_ml_rs_2014">[2]A. Ng, “Recommender Systems.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-7">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:how-item-feature-demension">
      <p>如何确定电影的特征多少维合适呢？ <a href="#fnref:how-item-feature-demension" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：异常检测</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-anomaly-detection" />
      <pubdate>2014-12-08T10:29:01+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-anomaly-detection</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<p>异常检测的基本思想：若发生了小概率事件，就认为出现了异常。</p>

<p>常用的异常检测方法是利用高斯密度函数，计算数据出现的概率，如果发现了概率小于某个阈值的数据，就认为该数据是异常的。</p>

<p>异常检测也是一种模式二分类方法，但两类数据严重不平衡，异常数据要显著少于正常数据。异常检测通常只需要对正常数据进行建模。</p>

<h2 id="section-1">基于高斯（正态）分布的异常检测</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_ad_2014">[1]</a>。</p>

<p>根据异常检测的思想，若$\mathbf x$出现的概率$p(\mathbf x) &lt; \varepsilon$，则认为$\mathbf x$是异常点。因此，异常检测的重要内容是估计概率密度函数。</p>

<h3 id="section-2">一元高斯分布</h3>

<p>基于一元高斯分布的异常检测的前提条件是假设特征之间相互独立。</p>

<p>通常假设特征分量的数据集$X$满足均值为$\mu$，方差为$\sigma^2$的正态分布，
\begin{equation}
X\sim\mathcal{N}\left(\mu, \sigma^2\right)，
\end{equation}
因此有
\begin{equation}
p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)，
\end{equation}
这需要估计均值$\mu$和方差$\sigma^2$，它们的极大似然估计为
\begin{equation}
\begin{aligned}
\mu = &amp; {1\over m}\sum_{i=1}^{m}x^{(i)}\\
\sigma^2 = &amp; {1\over m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2
\end{aligned}。
\label{eq:likehood-mu-sigma}
\end{equation}</p>

<p>得到了概率密度函数，就容易利用概率判断异常。</p>

<h4 id="section-3">一、异常检测</h4>

<blockquote>
  <h4 id="section-4">异常检测算法</h4>
  <hr />

  <ol>
    <li>选择能指示异常的特征$\mathbf x_i$；</li>
    <li>利用公式\eqref{eq:likehood-mu-sigma}，估计每维特征的均值和方差$\boldsymbol\mu_1,\ldots,\boldsymbol\mu_n,\boldsymbol\sigma_1^2,\ldots,\boldsymbol\sigma_n^2$；</li>
    <li>计算$\mathbf x$的概率，
\begin{equation}
p(\mathbf x) = \prod_{j=1}^n p\left(\mathbf x_j;\boldsymbol\mu_j,\boldsymbol\sigma_j^2\right)，
\end{equation}
通过特征分量概率密度函数乘积计算$\mathbf x$概率密度，需满足特征之间相互独立的假设；</li>
    <li>若$p(\mathbf x) &lt; \varepsilon$，则$\mathbf x$为异常点。</li>
  </ol>
</blockquote>

<p>异常检测的训练过程就是估计概率密度函数参数$\boldsymbol\mu$和$\boldsymbol\sigma^2$。通常情况，训练过程不需要异常数据。$60\%$的正常数据作为训练集，$20\%$的正常数据和$50\%$的异常数据作为交叉检验集，$20\%$的正常数据和$50\%$的异常数据作为测试集。</p>

<p>通过交叉检验集可确定判定异常的阈值$\varepsilon$，选择参数可利用<a href="/2014/11/machine-learning-advice-for-applying-machine-learning/#performance-evaluation">分类器性能评价指标</a>。</p>

<p>异常检测和监督学习存在不同的特点，应用在不同的场景：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">异常检测</th>
      <th style="text-align: left">监督学习</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">正样本（异常数据，$y=1$）少，通常$0\sim 20$个正样本，负样本（正常数据，$y=0$）多；</td>
      <td style="text-align: left">正样本和负样本都较多；</td>
    </tr>
    <tr>
      <td style="text-align: left">可能存多种不同类型的异常数据，难以通过正样本学习；</td>
      <td style="text-align: left">大量的正样本数据，能通过训练集了解正样本特点；</td>
    </tr>
    <tr>
      <td style="text-align: left">应用领域：欺诈检测、故障诊断、数据中心设备监控等</td>
      <td style="text-align: left">应用领域：垃圾邮件分类、天气预测、癌症分类等。</td>
    </tr>
  </tbody>
</table>

<h4 id="feature-transform">二、特征变换</h4>

<p>根据异常检测方法可知，运用异常检测有两个重要的前提条件：</p>

<ol>
  <li>特征满足高斯分布（特征之间的相关性下节考虑）；</li>
  <li>$p(\mathbf x)$对正常数据很大，但对异常数据很小。</li>
</ol>

<p>在实际应用中，原始特征可能并不满足这两个前提条件，需要将特征作一定变换或构造新的特征。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png"><img src="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png" alt="原始特征通过变换满足高斯分布" /></a><div class="caption">Figure 1:  原始特征通过变换满足高斯分布 [<a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png">PNG</a>]</div></div></div>

<p>上图展示了通过函数$\log x$变换原始特征以满足高斯分布。也可以通过构造新的特征，比如数据中心监控，利用特征$\mbox{CPU load}$和$\mbox{network traffic}$构造新的特征$\frac{\mbox{CPU load}}{\mbox{network traffic}}$<sup id="fnref:why-create-new-feature"><a href="#fn:why-create-new-feature" class="footnote">1</a></sup>，使其在发生异常的时数据会变得很大或者很小。</p>

<h3 id="section-5">多元高斯分布</h3>

<p>实际应用中，特征之间可能存在相关性，需要采用多元高斯分布概率密度函数进行异常检测。</p>

<p>多元高斯分布的概率密度函数定义为</p>

<p>\begin{equation}
p(\mathbf x; \boldsymbol\mu, \Sigma)=\frac{1}{(2\pi)^{n\over 2}\lvert\Sigma\rvert^{1\over 2}}\exp\left(-{1\over 2}(\mathbf x - \boldsymbol\mu)^T\Sigma^{-1}(\mathbf x - \boldsymbol\mu)\right)，
\label{eq:multi-gaussians-pdf}
\end{equation}</p>

<p>其均值向量和协方差矩阵的极大似然估计为</p>

<p>\begin{equation}
\begin{aligned}
\boldsymbol\mu = &amp; {1\over m}\sum_{i=1}^{m}\mathbf x^{(i)}\\
\Sigma = &amp; {1\over m}\sum_{i=1}^{m}\left(\mathbf x^{(i)}-\boldsymbol\mu\right)\left(\mathbf x^{(i)}-\boldsymbol\mu\right)^T
\end{aligned}。
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png"><img src="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png" alt="二元高斯分布" /></a><div class="caption">Figure 2:  二元高斯分布 [<a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png">PNG</a>]</div></div></div>

<p>上图给出了不同参数的二元高斯密度函数图。图上排的协方差矩阵为对角阵，表示特征之间独立，可用一元高斯分布的方法进行异常检测；图下排的协方差矩阵是非对角阵，表示特征之间存在相关性，需借助多元高斯分布密度函数\eqref{eq:multi-gaussians-pdf}进行异常检测。</p>

<p>基于一元高斯分布和多元高斯分布的异常检测有不同的应用场景：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">一元高斯分布</th>
      <th style="text-align: left">多元高斯分布</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">根据先验知识<a href="#feature-transform">构造新特征</a>，手动处理相关性问题；</td>
      <td style="text-align: left">自动处理样本之间的相关性，计算$\Sigma$<sup id="fnref:multi-gaussian-feature-transform"><a href="#fn:multi-gaussian-feature-transform" class="footnote">2</a></sup>；</td>
    </tr>
    <tr>
      <td style="text-align: left">计算复杂度较低；</td>
      <td style="text-align: left">计算复杂度较高；</td>
    </tr>
    <tr>
      <td style="text-align: left">能处理样本数$m$很少的情况。</td>
      <td style="text-align: left">需要$m&gt;n$（一般$m&gt;10n$），否则$\Sigma$不可逆。</td>
    </tr>
  </tbody>
</table>

<p>若$\Sigma$不可逆，原因可能是不满足条件$m&gt;n$，或者存在冗余特征，也就是特征之间有相关性（比如$\mathbf x_1=k\mathbf x_2$或$\mathbf x_1=\mathbf x_2 ＋ \mathbf x_3$等）。</p>

<p>由此可见，特征之间是否具有相关性并非利用多元还是一元高斯分布进行异常检测的唯一条件，在必要的时候需要借助一元高斯分布对具有相关性特征的数据集进行异常检测。</p>

<h2 id="section-6">参考文献</h2>

<ol class="bibliography"><li><span id="ng_ml_ad_2014">[1]A. Ng, “Anomaly detection.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-7">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:why-create-new-feature">
      <p>如何判断构造的新特征有价值？哪些特征加入会有助于提高性能？ <a href="#fnref:why-create-new-feature" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:multi-gaussian-feature-transform">
      <p>利用多元高斯分布也要将每维特征变换为高斯分布么？ <a href="#fnref:multi-gaussian-feature-transform" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：维数约减</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-dimensionality-reduction" />
      <pubdate>2014-12-08T04:53:42+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-dimensionality-reduction</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<p>维数约减的作用通常是为了数据压缩和可视化。数据压缩不仅可以节省存储空间，而且可以加速机器学习算法。高维数据需要约减到3维或2维空间，以便观测其特性。 </p>

<h2 id="pca">主成分分析（PCA）</h2>

<p>维数约减最常用的方法是主成分分析（PCA，Principal Component Analysis）。PCA可以理解为在高维空间中寻找一个低维的面，使得高维空间中的点到该面上的距离之和最小，这个距离也叫投影误差。</p>

<p>利用PCA将维数从$n$维约减到$k$维，需要寻找$n$维空间中的$k$个向量$\mathbf u^{(1)}, \mathbf u^{(2)},\ldots,\mathbf u^{(k)}\in\mathbb R^n$，使空间中的点到这$k$个向量确定的面的投影误差最小。事实上，$n$维空间中的这$k$个向量是样本协方差矩阵最大的$k$个特征值对应的特征向量。</p>

<blockquote>
  <h4 id="pca-1">PCA维数约减算法</h4>
  <hr />

  <ol>
    <li>数据作均值为$0$的规范化（mean normalization），确保每维均值为$0$，若取值范围差异过大，还需尺度规范化（feature scaling）：$\mathbf x_j^{(i)}\leftarrow\frac{\mathbf x_j^{(i)}-\boldsymbol \mu_j}{\mathbf s_j}$（$\boldsymbol \mu_j$表示均值，$\mathbf s_j$表示标准差）；</li>
    <li>计算协方差矩阵（covariance matrix）：$\Sigma = \frac{1}{m}\sum_{i=1}^m\mathbf x^{(i)}\left(\mathbf x^{(i)}\right)^T$；</li>
    <li>利用特征向量将$n$维向量$\mathbf x$映射到$k$维向量$\mathbf z$：
\begin{equation}
\mathbf z^{(i)} = U_{reduce}^T\mathbf x^{(i)}。
\end{equation}</li>
  </ol>

  <div class="highlight"><pre><code class="language-matlab"><span class="p">[</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="p">]</span> <span class="p">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">Sigma</span><span class="p">);</span>
<span class="n">Ureduce</span> <span class="p">=</span> <span class="n">U</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">);</span>
<span class="n">z</span> <span class="p">=</span> <span class="n">Ureduce</span>’ <span class="o">*</span> <span class="n">x</span><span class="p">;</span></code></pre></div>
  <p>在应用中，只需要在训练集上做PCA，交叉检验和测试集上可以直接应用训练集的均值$\boldsymbol\mu$、标准差$\mathbf s$和映射矩阵$U_{reduce}$计算约减后的向量。</p>
</blockquote>

<p>Matlab中<code>svd</code>和<code>eig</code>函数都可以得到相同的特征值和特征向量，但是<code>svd</code>更稳定。</p>

<p>从$k$维数据$\mathbf z$重构$n$维数据$\mathbf x$的方法为</p>

<p>\begin{equation}
\mathbf x_{approx}^{(i)} = U_{reduce}\mathbf z^{(i)}。
\end{equation}</p>

<p>约减后的维数$k$（主成分个数）通过方差保留的比率确定，选择满足下列条件的最小$k$</p>

<p>\begin{equation*}
\frac{\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}-\mathbf x_{approx}^{(i)}\right\rVert^2}{\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}\right\rVert^2}\leq 0.01，
\end{equation*}</p>

<p>此时方差保存比率为$99\%$。但是该方法计算复杂，可以通过特征值更简单的计算，选择满足下列条件的最小$k$</p>

<p>\begin{equation}
\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\geq 0.99，
\end{equation}</p>

<p>$S_{ii}$是SVD得到的特征值。</p>

<blockquote>
  <h4 id="pca-2">谨慎使用PCA</h4>
  <hr />

  <ol>
    <li>PCA不是解决过拟合的好方法，正则化是更好的策略（PCA或多或少损失了有助于分类的信息）；</li>
    <li>不得滥用PCA，除非有证据表明PCA的价值，比如在有训练时间和存储空间的限制的时候。</li>
  </ol>

</blockquote>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：聚类</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-clustering" />
      <pubdate>2014-12-08T02:38:59+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-clustering</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">聚类简介</h2>

<p>聚类是一种非监督学习方法。</p>

<h2 id="k-means">$k$-means聚类</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_c_2014">[1]</a>。</p>

<p>$k$-means算法主要包含两步：为样本分配类标签以及修改类中心。</p>

<blockquote>
  <h4 id="k-means-1">$k$-means算法</h4>
  <hr />
  <p>随机初始化$K$个类中心$\boldsymbol\mu_1,\boldsymbol\mu_2,\ldots,\boldsymbol\mu_K\in\mathbb R^n$。 <br />
重复 {</p>

  <ol>
    <li>for $i=1$ to $m$：将$\mathbf x^{(i)}$的类别标签$c^{(i)}$设为最靠近的类中心标签，$c^{(i)}=\arg\min_{k=1}^K\left\lVert\mathbf x^{(i)}-\boldsymbol\mu_k\right\rVert^2$；</li>
    <li>for $k=1$ to $K$：重新计算类中心$\boldsymbol\mu_k$。</li>
  </ol>

  <p>}</p>
</blockquote>

<p>$k$-means聚类的代价函数为</p>

<p>\begin{equation}
J\left(c^{(1)},\dots,c^{(m)},\boldsymbol\mu_1,\ldots,\boldsymbol\mu_K\right)=\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}-\boldsymbol\mu_{c^{(i)}}\right\rVert^2，
\label{eq:cf-k-means}
\end{equation}</p>

<p>该代价函数通常也称为distortion function。</p>

<p>从代价函数可以看出，$k$-means算法的第1步是通过修改类标签$c^{(i)}$最小化代价函数，第2步是通过修改类中心$\boldsymbol\mu_k$最小化代价函数。</p>

<p>从$k$-means算法可知，初始化类中心$\boldsymbol\mu_k$和确定类别数$k$影响着算法的性能。</p>

<h3 id="boldsymbolmuk">初始化类中心$\boldsymbol\mu_k$</h3>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png"><img src="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png" alt="不恰当的类中心初始化导致局部极值" /></a><div class="caption">Figure 1:  不恰当的类中心初始化导致局部极值 [<a href="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png">PNG</a>]</div></div></div>

<p>随机初始化类中心的方法通常是，随机从样本中选取$K$个点作为类中心。为了避免陷入局部极致，通常会进行多轮初始化，选择代价函数值最小的作为聚类结果。</p>

<blockquote>
  <h4 id="k-means-2">随机初始化选择$k$-means的类中心</h4>
  <hr />
  <p>For i = 1 to 100 {</p>

  <ol>
    <li>随机初始化类中心$\boldsymbol\mu_k$；</li>
    <li>$k$-means算法得到$c^{(1)},\dots,c^{(m)},\boldsymbol\mu_1,\ldots,\boldsymbol\mu_K$；</li>
    <li>计算代价函数\eqref{eq:cf-k-means}。</li>
  </ol>

  <p>}  <br />
选择代价函数值最小的聚类结果输出。</p>
</blockquote>

<p>当$K=2,\ldots,10$时，这种方法的代价函数变化明显，当$K$很大（$K&gt;100$）时，代价函数可能没有明显的变化。</p>

<h3 id="k">确定类别数$K$</h3>

<p>通过不停增大类别数$K$，选择代价函数曲线拐点对应的类别数，如下图左所示。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png"><img src="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png" alt="确定类别数的elbow method" /></a><div class="caption">Figure 2:  确定类别数的elbow method [<a href="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png">PNG</a>]</div></div></div>

<p>有时，代价函数曲线不存在拐点，如上图右所示。还可以根据$k$-means应用的具体场景，选择聚类数目，比如要制作XS、S、M、L、XL几种规格的服装，当然用类别数$K=5$来划分人的身高体重。</p>

<h2 id="section-1">参考文献</h2>

<ol class="bibliography"><li><span id="ng_ml_c_2014">[1]A. Ng, “Clustering.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-2">脚注</h3>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：支持向量机（SVM）</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-support-vector-machines" />
      <pubdate>2014-11-27T08:05:56+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-support-vector-machines</guid>
      <content:encoded>&lt;![CDATA[<h2 id="logisticsvm">从Logistic回归到SVM</h2>

<p>本节通过Logistic回归的代价函数，演化到SVM的代价函数，然后通过代价函数最小化推导SVM的判别界<a href="#ng_ml_svm_2014">[1, P. 3—14]</a>。</p>

<p>Logistic回归的代价函数为</p>

<p>\begin{equation}
\begin{aligned}
J(\boldsymbol\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\boldsymbol\theta_j^2
\end{aligned}，
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<p>取出代价函数中的一项如下图，并绘制逼近Logistic代价函数的两个新的代价函数项$\mbox{cost}_1(\mathbf z)$和$\mbox{cost}_0(\mathbf z)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png" alt="图解Logistic回归的代价函数" /></a><div class="caption">Figure 1:  图解Logistic回归的代价函数 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png">PNG</a>]</div></div></div>

<p>将Logistic回归的代价函数提取$\lambda\over m$，令$C={1\over\lambda}$，并用新的代价函数项替代Logistic回归的代价函数项，可得SVM的代价函数</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2 ，
\label{eq:cf-svm-r}
\end{equation}</p>

<p>新的代价函数不再以$0$作为分类边界，而是以$+1$和$-1$。若代价函数\eqref{eq:cf-svm-r}要得最小值，可得SVM的判别界</p>

<p>\begin{equation}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2}\sum_{j=1}^{n}\boldsymbol\theta_j^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\label{eq:svm-decision-boundary}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png" alt="图解SVM的判别界" /></a><div class="caption">Figure 2:  图解SVM的判别界 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png">PNG</a>]</div></div></div>

<p>根据向量间的夹角公式$\cos\theta = \frac{\boldsymbol\theta^T\mathbf x^{(i)}}{\left\lVert\boldsymbol\theta\right\rVert\left\lVert\mathbf x^{(i)}\right\rVert}$，则$p^{(i)}=\left\lVert\mathbf x^{(i)}\right\rVert\cos\theta$表示向量$\mathbf x^{(i)}$在向量$\boldsymbol\theta$上的投影，SVM的判别界\eqref{eq:svm-decision-boundary}可以改写为</p>

<p>\begin{equation*}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2} \lVert\boldsymbol\theta\rVert^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\end{equation*}</p>

<p>$\boldsymbol\theta$是判别界的法线，$p^{(i)}$的值可正可负。要满足判别界的条件，$\left\vert p^{(i)}\right\vert$越大越好，这样$\lVert\boldsymbol\theta\rVert$就可以取到很小的值。因此，上图中右下比左下有更大的$\left\vert p^{(i)}\right\vert$，是更合适的SVM判别界。</p>

<h2 id="section">核函数</h2>

<p>SVM为了解决非线性可分问题，需要引入核函数（kernel）。作为SVM的核函数须满足Mercer定理。</p>

<blockquote>
  <h4 id="mercer-a-hrefjulysvm20142a">Mercer 定理<a href="#July_svm_2014">[2]</a></h4>
  <hr />
  <p>函数$\kappa$是 $\mathbb R^n \times \mathbb R^n \to \mathbb R$ 上的映射。如果$\kappa$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$\{\mathbf x_1,\mathbf x_2,\ldots,\mathbf x_n\}$，其相应的核函数矩阵是对称半正定的。</p>
</blockquote>

<p>通过特征与参考地标（landmark）$\mathbf l^{(i)}$之间的相似性，定义高斯核为<a href="#ng_ml_svm_2014">[1, P. 17—20]</a></p>

<p>\begin{equation}
\mathbf f_i = \mbox{similarity}\left(\mathbf x, \mathbf l^{(i)} \right)
= \exp\left(-\frac{\left\lVert\mathbf x - \mathbf l^{(i)}\right\rVert}{2\sigma^2} \right)。
\end{equation}</p>

<p>核函数的作用相当于特征变换，SVM引入了核函数的代价函数为</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2。
\label{eq:cf-svm-kernel-r}
\end{equation}</p>

<p>SVM代价函数和核函数的参数对SVM分类有如下影响<a href="#ng_ml_svm_2014">[1, P. 25]</a>：</p>

<ol>
  <li>大的$C~\left(C={1\over\lambda}\right)$导致Low Bias和High Variance；</li>
  <li>小的$C~\left(C={1\over\lambda}\right)$导致High Bias和Low Variance；</li>
  <li>大的$\sigma^2$使得特征$\mathbf f_i$变化平缓，导致High Bias和Low Variance；</li>
  <li>小的$\sigma^2$使得特征$\mathbf f_i$变化较大，导致Low Bias和High Variance。 </li>
</ol>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg" alt="高斯核的SVM分类效果" /></a><div class="caption">Figure 3:  高斯核的SVM分类效果 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg">SVG</a>, <a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.png">PNG</a>]</div></div></div>

<h2 id="smo">SMO算法</h2>

<h2 id="svm">使用SVM</h2>

<p>Logistic回归强调所有点尽可能地远离中间那条线，SVM更应该关心靠近中间分割线的点，让他们尽可能地远离中间线。SVM考虑局部（不关心已经确定远离的点），Logistic回归考虑全局（已经远离的点可能通过调整中间线使其能够更加远离）<a href="#JerryLead_svm1_2011">[3]</a>。</p>

<h3 id="logisticsvm-1">Logistic回归、SVM、神经网络</h3>

<p>如何在分类器Logistic回归、SVM和神经网络之间做出选择？</p>

<p>$n$为特征数目（$x\in \mathbb{R}^{n+1} $），$m$为训练集样本数目，分类器选择的方法如下<a href="#ng_ml_svm_2014">[1, P. 31]</a>：</p>

<ol>
  <li>若$n$很大（比如$n\geq m, n=10000,m=10,\ldots,1000$），采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，用Logistic回归难以训练好非线性分类器）；</li>
  <li>若$n$很小而$m$大小适中（比如$n=1,\ldots,1000,m=10,\ldots,10000$），采用高斯核的SVM；</li>
  <li>若$n$很小而$m$很大（比如$n=1,\ldots,1000,m=50000+$），先可以增加特征，然后采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，高斯核的SVM分类器训练会慢）；</li>
  <li>神经网络在以上大多数情况都工作较好，但是可能训练速度会很慢；</li>
  <li>神经网络是非凸优化，会陷入局部极值，SVM是凸优化，不用担心陷入局部极值。</li>
</ol>

<p>选择Logistic回归或者无核函数（线性核函数）的SVM，是因为Logistic回归和无核函数（线性核函数）的SVM相似。</p>

<p>核函数的Logistic回归训练较慢，核函数的SVM可以训练较快。</p>

<h2 id="section-1">应用案例</h2>

<h3 id="a-hrefngmlsvmex20144-p-1015a">垃圾邮件分类<a href="#ng_ml_svm_ex_2014">[4, P. 10—15]</a></h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg"><img src="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg" alt="Email转换为特征向量" /></a><div class="caption">Figure 4:  Email转换为特征向量 [<a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg">SVG</a>]</div></div></div>

<p>垃圾邮件分类首先需要将Email文本转换为特征向量。如上图所示，转换方法如下：</p>

<ol>
  <li>将Email文本转换为纯单词，比如：单词小写化，移除所有HTML标签，url链接均用单词<code>httpaddr</code>代替，提取词干（例如including、includes和included都用include代替）等；</li>
  <li>利用事先准备的词典，将单词用其在词典中的索引表示，实际应用中，词典通常有10000到50000个单词；</li>
  <li>将索引转换为$\{0, 1\}$特征向量，向量长度和词典中词汇数目一样，某个单词出现用$1$表示，否者用$0$表示。  </li>
</ol>

<h2 id="section-2">相关评论</h2>

<p>事实上，支持向量机是一个具有很好数学基础的分类方法，但它本质上也只不过是一个简单的两层方法：第一层可以看作是一些单元集合（一个支持向量就是一个单元），这些单元通过核函数能够度量输入向量和每个支持向量的相似度；第二层则把这些相似度做了简单的线性累加。支持向量机第一层的训练和最简单的无监督学习基本一致：利用支持向量来表示训练样本。一般来讲，通过调整核函数的平滑性（参数）能在线性分类和模板匹配之间做出平衡。从这个角度来讲，核函数只不过是一种模板匹配方法。<a href="#lecun_dl_vs_svm_2014">[5]</a></p>

<p>［@余凯_西二旗民工］很多有关SVM的教科书都有misleading: (1)KKT条件，support vectors，quadratic programing都是浮云；（2）kernel本身对理解学习问题有帮助，但实际工程上用处为0；（3）hinge loss只是众多可选项之一，logistic效果一点不差。［@老师木］做论文时迷信kernel，margin，认为这些机制和graphical model结合不就无敌了吗？等论文出来的结论是hinge loss对比无优越性。再翻vapnik的 统计学习的本质，原来他老人家也早做过对比试验，没发现svm相对于lr的优越性。看损失函数曲线，真看不出hinge loss比log loss好，只是hinge loss能得到稀疏解，看上去很美。当然理解这些优化理论本身会给人一些享受。数据线性可分的可能性随维度升高而变大，这是Thomas cover五六十年代得出的结论。要严格一点，数据线性可分和线性无关等价，张成维度高的线性空间所需的基多，其实就是VC维了。引入kernel就是引入非线性，使变换后的样例线性无关。Rbf核等价于无穷次多项式拟合，相对于有限的样例，没有不能分开的。<a href="#mu_svm_myth_2014">[6]</a></p>

<h2 id="section-3">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_svm_2014">[1]A. Ng, “Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="July_svm_2014">[2]July, “支持向量机通俗导论（理解SVM的三层境界）.” csdn, 2014.</span>

[<a href="http://blog.csdn.net/v_july_v/article/details/7624837/">Online</a>]

</li>
<li><span id="JerryLead_svm1_2011">[3]JerryLead, “支持向量机SVM（一）.” cnblogs, 2011.</span>

[<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html">Online</a>]

</li>
<li><span id="ng_ml_svm_ex_2014">[4]A. Ng, “Programming Exercise 6: Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="lecun_dl_vs_svm_2014">[5]Y. LeCun, “深度学习与支持向量机有什么联系？.” 52cs, 2014.</span>

[<a href="http://www.52cs.org/?p=46">Online</a>]

</li>
<li><span id="mu_svm_myth_2014">[6]老师木, “SVM神话.” 52cs, 2014.</span>

[<a href="http://www.52cs.org/?p=359">Online</a>]

</li></ol>

<h3 id="section-4">脚注</h3>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：实战技能</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-advice-for-applying-machine-learning" />
      <pubdate>2014-11-25T09:44:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-advice-for-applying-machine-learning</guid>
      <content:encoded>&lt;![CDATA[<p>对于分类器而言通常需要考虑如下问题：</p>

<ul>
  <li>如何评价分类器性能？</li>
  <li>数据需要如何预处理？</li>
  <li>如何设计代价函数？</li>
  <li>代价函数是凸的么？</li>
  <li>如何用最优化方法求解模型参数？</li>
  <li>如何调整参数，在Bias和Variance之间寻求平衡？</li>
  <li>如何解决多分类问题？</li>
  <li>如何处理线性不可分问题？</li>
</ul>

<p>对于样本动态变化的情况，还需要考虑如何在线学习和动态更新模型参数；对于大规模数据集，需要考虑如何选择合适的模型，如何降低复杂度。</p>

<h2 id="section">模型选择</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_aaml_2014">[1]</a>。</p>

<p>模型选择主要是通过评估模型效果，评价模型Bias和Variance的状况，选择合适的模型和估计相应的参数。</p>

<p>数据集一般划分为训练集、交叉验证集和测试集3部分，分别占的比例大概是60%、20%和20%<sup id="fnref:only-train-test"><a href="#fn:only-train-test" class="footnote">1</a></sup>。训练集用于估计模型参数，交叉验证集用于选择模型，测试集用于估计模型的泛化误差（generalization error）；选择在交叉验证集上误差小的模型，但是用测试集上误差作为模型的误差。测试集上的数据对参数估计和模型选择都是不可见的，因此才能“公平”的评价模型的性能。</p>

<p>$J_\mbox{train}(\boldsymbol\theta)$、$J_\mbox{cv}(\boldsymbol\theta)$、$J_\mbox{test}(\boldsymbol\theta)$分别表示模型在训练集、交叉验证集和测试集上的误差</p>

<p>\begin{equation}
J_{s}(\boldsymbol\theta) = \frac{1}{2m_{s}}\sum_{i=1}^{m_s}\left(h_\boldsymbol\theta\left(\mathbf x_s^{(i)}\right)-y_s^{(i)}\right)^2~~(s = \{\mbox{train},\mbox{cv},\mbox{test}\})。
\end{equation}</p>

<h3 id="bias-vs-variance">Bias vs. Variance</h3>

<p>简单来说，Bias和Variance评价模型的拟合程度，High Bias就是欠拟合（underfit），High Variance就是过拟合（overfit）。对于欠拟合，$J_\mbox{cv}(\boldsymbol\theta)\approx J_\mbox{train}(\boldsymbol\theta)$且都较大；对于过拟合，$J_\mbox{cv}(\boldsymbol\theta)\gg J_\mbox{train}(\boldsymbol\theta)$且$J_\mbox{train}(\boldsymbol\theta)$较小。</p>

<h3 id="section-1">模型选择与正则化——以多项式回归为例</h3>

<p>模型的复杂度和参数估计时采用的正则化参数$\lambda$对模型的Bias和Variance均有影响。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png" alt="多项式回归模型" /></a><div class="caption">Figure 1:  多项式回归模型 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png">PNG</a>]</div></div></div>

<p>如果选用多项式回归模型，需要选择多项式的次数确定合适的模型。参数估计的时候，可以通过调节正则化系数，平衡Bias和Variance的关系。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png" alt="多项式次数和正则化系数与误差的关系" /></a><div class="caption">Figure 2:  多项式次数和正则化系数与误差的关系 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png">PNG</a>]</div></div></div>

<p>随着多项式次数$d$的增加， 模型从High Bias变为了High Variance；随着正则化系数$\lambda$的增加， 模型从High Variance变为了High Bias。通过评估调整模型和正则化参数时的误差，确定模型的相关系数。</p>

<h3 id="section-2">学习曲线</h3>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png" alt="High Bias和High Variance的学习曲线" /></a><div class="caption">Figure 3:  High Bias和High Variance的学习曲线 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png">PNG</a>]</div></div></div>

<p>学习曲线描绘了训练样本数目和误差之间的关系，展示了模型可能存在的问题。通过学习曲线，判断模型是否合适；如果不合适，模型是High Bias还是High Variance，从而有针对性地解决问题。</p>

<h3 id="section-3">提升模型性能的策略</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">技术手段</th>
      <th style="text-align: left">处理问题</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">加入更多的训练样本</td>
      <td style="text-align: left">High Variance<sup id="fnref:Not-good-if-high-bias"><a href="#fn:Not-good-if-high-bias" class="footnote">2</a></sup></td>
    </tr>
    <tr>
      <td style="text-align: left">抽取特征子集</td>
      <td style="text-align: left">High Variance<sup id="fnref:Not-good-if-high-bias:1"><a href="#fn:Not-good-if-high-bias" class="footnote">2</a></sup></td>
    </tr>
    <tr>
      <td style="text-align: left">增加特征</td>
      <td style="text-align: left">High Bias</td>
    </tr>
    <tr>
      <td style="text-align: left">构造多项式特征</td>
      <td style="text-align: left">High Bias</td>
    </tr>
    <tr>
      <td style="text-align: left">增大$\lambda$</td>
      <td style="text-align: left">High Variance</td>
    </tr>
    <tr>
      <td style="text-align: left">减小$\lambda$</td>
      <td style="text-align: left">High Bias</td>
    </tr>
  </tbody>
</table>

<h2 id="performance-evaluation">性能评估</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_mlsd_2014">[2]</a>。</p>

<p>如何和里评价机器学习算法，尤其是对于有偏的类别而言，单纯的正确率（Accuracy），
\begin{equation}
\mbox{Accuracy} = \frac{\mbox{true positives + true negatives}}{\mbox{total examples}}，
\end{equation}
难以合理评价分类器性能。</p>

<p>比如癌症检测，癌症的概率大概只有0.5左右，对于分类器，采用作弊的方法，即使全部输出非癌症的结果，也可以获得99.5％的正确率。因此需要合理的评估方法，通常采用的是准确率（Precision）和召回率（Recall）。</p>

<h3 id="precision--recall">Precision &amp; Recall</h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png" alt="准确率和召回率" /></a><div class="caption">Figure 4:  准确率和召回率 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png">PNG</a>, <a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.svg">SVG</a>]</div></div></div>

<p>准确率刻画的是，被正确分类的样本在分类结果中占的比率，</p>

<p>\begin{equation}
\mbox{Precision} = \frac{\mbox{true positives}}{\mbox{true positives + false positives}}。
\end{equation}</p>

<ul>
  <li>true positives：真正正确分类的样本数；</li>
  <li>false positives：混入该类别的其它类别样本数。 </li>
</ul>

<p>召回率刻画的是，被正确分类的样本在输入样本中占的比率，</p>

<p>\begin{equation}
\mbox{Recall} = \frac{\mbox{true positives}}{\mbox{true positives + false negatives}}。
\end{equation}</p>

<ul>
  <li>false negatives：本该属于该类别却被分到其它类被的样本数。</li>
</ul>

<p>根据应用的具体需求，可以在准确率和召回率之间折中。若采用Logistic回归分类器，调高分类的阈值，可以提升该类别分类的准确率和降低召回率。</p>

<h3 id="ff1-score">$F$/$F_1$ Score</h3>

<p>采用准确率和召回率可以合理评估分类器性能，但是两个数值不如单数值的评估直接明了。$F$/$F_1$ Score是基于准确率和召回率的单数值评估指标，</p>

<p>\begin{equation}
F = 2 \times \frac{\mbox{Precision}\times\mbox{Recall}}{\mbox{Precision} + \mbox{Recall}}，
\end{equation}</p>

<p>数值越高表示效果越好。</p>

<h3 id="section-4">数据为王</h3>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png" alt="数据集大小对准确率的影响" /></a><div class="caption">Figure 5:  数据集大小对准确率的影响 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png">PNG</a>]</div></div></div>

<p>通常而言，增加参数数目（比如：Logistic回归增加特征数目，神经网络增加隐藏层神经元）和训练样本数目，可以提升分类器的性能。增加参数数目可以获得Low Bias（$J_\mbox{train}(\theta)$小），增加样本数目可以获得Low Variance（$J_\mbox{test}(\theta)$小）。</p>

<blockquote>
  <p>“It’s not who has the best algorithm that wins. It’s who has the most data.”<a href="#ng_ml_mlsd_2014">[2, P. 16]</a></p>
</blockquote>

<p>在大规模数据上训练模型，取得良好效果的前提条件是：</p>

<ol>
  <li>模型具备足够多的参数，能够表示复杂的函数；</li>
  <li>特征$\mathbf x$包含了预测$y$的足够信息（例如：该领域的专家可以仅仅通过$x$的有把握地预测$y$）。</li>
</ol>

<p>Guo-Xun Yuan等撰文指出<a href="#yuan_ralll_2012">[3]</a>，对于大规模数据的应用而言，线性分类器可能获得和非线性分类器接近的性能，并且训练和测试的速度要快得多。</p>

<h2 id="section-5">获取更多的数据</h2>

<p>并非总是数据越多越好。在获取更多数据之前，先判断模型是否需要更多的数据。</p>

<p>针对Low Bias（或High Variance）的模型，获取更多的数据才能提高模型的性能。如果是High Bias（或Low Variance）的模型，先增加特征（神经网络增加神经元），使模型是Low Bias（或High Variance）的，然后更多的数据才有助于提升模型性能。获取更多数据的方法通常有人工合成、手动搜集标柱等<a href="#ng_ml_ocr_2014">[4]</a>。</p>

<p>针对OCR，可以将计算机字库中的标准字体叠加随机背景，然后进行形变，产生更多的样本集。一般而言，通过叠加高斯噪声增大样本集对提升性能帮助不大，除非需要解决的问题的就是高斯噪声下的OCR。</p>

<h2 id="section-6">提升关键步骤性能</h2>

<p>针对一个机器学习应用的流水线，找出影响性能的瓶颈，对症下药，才能更有效的改善性能<a href="#ng_ml_ocr_2014">[4]</a>。</p>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-segmentation.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-segmentation.png" alt="学习字符分割间隔的神奇方法" /></a><div class="caption">Figure 6:  学习字符分割间隔的神奇方法 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-segmentation.png">PNG</a>]</div></div></div>

<p>对OCR问题而言，通常分为文字检测、字符分割、字符识别几个步骤。提升OCR性能，需要先定量分析究竟哪个步骤对性能的影响最大，然后有针对性的解决。</p>

<div class="image_line" id="figure-7"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-ceiling-analysis.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-ceiling-analysis.png" alt="Ceiling Analysis" /></a><div class="caption">Figure 7:  Ceiling Analysis [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-ceiling-analysis.png">PNG</a>]</div></div></div>

<p>上图给出了通过Ceiling Analysis分析出关键问题的示例，系统的总体精度是$72\%$。如果手动标注（检测定位）待识别的文字区域（也就是让文字都正确定位），那么系统的精度会提升到$89\%$；如果继续将所有的字符都正确分割出，精度会提升到$90\%$。从而可得出，字符检测可以提升$17\%$的精度，字符识别可提升$10\%$的精度，首先从这两个地方改善性能，字符分割对性能的提升只有$1\%$，可先不考虑改进。</p>

<div class="image_line" id="figure-8"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-face-recognition.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-face-recognition.png" alt="人脸识别" /></a><div class="caption">Figure 8:  人脸识别 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-face-recognition.png">PNG</a>]</div></div></div>

<p>上图给出了对人脸识别的Ceiling Analysis，可以看出预处理的背景移除对提升性能影响不大，仅仅为$0.1\%$，这个步骤其实可以去掉。人脸检测才是影响性能的关键。</p>

<h2 id="section-7">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_aaml_2014">[1]A. Ng, “Advice for applying machine learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_mlsd_2014">[2]A. Ng, “Machine Learning System Design.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="yuan_ralll_2012">[3]G.-X. Yuan, C.-H. Ho, and C.-J. Lin, “Recent Advances of Large-Scale Linear Classification,” <i>Proceedings of the IEEE</i>, vol. 100, no. 9, pp. 2584–2603, 2012.</span>

</li>
<li><span id="ng_ml_ocr_2014">[4]A. Ng, “Application example: Photo OCR.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-8">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:only-train-test">
      <p>对于不需要做模型选择的情况，只需要训练集和测试集，样本的比例通常是70%和30%。 <a href="#fnref:only-train-test" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:Not-good-if-high-bias">
      <p><a href="http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html">Not good if you have high bias</a> <a href="#fnref:Not-good-if-high-bias" class="reversefootnote">&#8617;</a> <a href="#fnref:Not-good-if-high-bias:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：学习资源</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-resources" />
      <pubdate>2014-11-22T00:00:36+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-resources</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">入门资料</h2>

<ol>
  <li>Hacker’s guide to Neural Networks［<a href="http://karpathy.github.io/neuralnets/">Link</a>］</li>
  <li>Probabilistic Programming &amp; Bayesian Methods for Hackers［<a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Link</a>］</li>
  <li>Visualizing MNIST: An Exploration of Dimensionality Reduction［<a href="https://colah.github.io/posts/2014-10-Visualizing-MNIST/">Link</a>］</li>
  <li>Data Science in Python［<a href="http://blog.yhathq.com/posts/data-science-in-python-tutorial.html">Link</a>］</li>
  <li>Bayesian network与python概率编程实战入门［<a href="https://github.com/memect/hao/blob/master/awesome/bayesian-network-python.md">Link</a>］</li>
</ol>

<h3 id="deep-learning">Deep Learning</h3>

<ol>
  <li>DEEP LEARNING（An MIT Press book in preparation）［<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Link</a>］</li>
  <li>Neural Networks and Deep Learning［<a href="http://neuralnetworksanddeeplearning.com">Link</a>］</li>
  <li>Unsupervised Feature Learning and Deep Learning［<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial">Link</a>］</li>
  <li>TUTORIAL ON DEEP LEARNING FOR VISION［<a href="https://sites.google.com/site/deeplearningcvpr2014/">Link</a>］</li>
  <li>Quoc Le’s Lectures on Deep Learning［<a href="http://www.trivedigaurav.com/blog/quoc-les-lectures-on-deep-learning/">Link（视频）</a>］</li>
  <li>Deep Learning Master Class［<a href="http://www.cs.tau.ac.il/~wolf/deeplearningmeeting/home.html">Link（视频）</a>］</li>
  <li>深度学习进阶线路图［<a href="http://www.aitmr.com/?s=深度学习进阶线路图">Link</a>］</li>
  <li>深度神经网络DNN的多GPU数据并行框架及其在语音识别的应用［<a href="http://www.csdn.net/article/2014-07-11/2820628-DNN">Link</a>］</li>
  <li>解密接近人脑的智能学习机器——深度学习及并行化实现［<a href="http://www.ccf.org.cn/sites/ccf/zlcontnry.jsp?contentId=2831644844483">Link</a>］</li>
  <li>看DeepMind如何用Reinforcement learning玩游戏［<a href="http://www.infoq.com/cn/articles/atari-reinforcement-learning">Link</a>］</li>
</ol>

<h2 id="section-1">资源整合</h2>

<ol>
  <li>Some of useful machine learning resources from beginner to intermediate［<a href="https://www.quora.com/Eren-Golge/Machine-Learning/Some-of-useful-machine-learning-resources-from-beginner-to-intermediate">Link</a>］</li>
  <li>My deep learning reading list［<a href="http://blog.sina.com.cn/s/blog_bda0d2f10101fpp4.html">Link</a>］</li>
  <li>Where to Learn Deep Learning – Courses, Tutorials, Software［<a href="http://www.kdnuggets.com/2014/05/learn-deep-learning-courses-tutorials-overviews.html">Link</a>］</li>
  <li>Deep Learning – important resources for learning and understanding［<a href="http://www.kdnuggets.com/2014/08/deep-learning-important-resources-learning-understanding.html">Link</a>］</li>
  <li>A curated list of awesome Machine Learning frameworks, libraries and software［<a href="https://github.com/josephmisiti/awesome-machine-learning">Link</a>］</li>
  <li>个人阅读的Deep Learning方向的paper整理［<a href="http://hi.baidu.com/chb_seaok/item/6307c0d0363170e73cc2cb65">Link</a>］</li>
</ol>

<h2 id="section-2">竞赛与数据</h2>

<ol>
  <li>kaggle：<a href="http://www.kaggle.com">The Home of Data Science</a>［<a href="http://www.quora.com/What-are-some-alternatives-to-Kaggle">What are some alternatives to Kaggle?</a>］</li>
  <li>DRIVENDATA：<a href="http://www.drivendata.org">Data science competitions to save the world</a></li>
  <li>百度：<a href="http://openresearch.baidu.com/?locale=en_US">百度开放研究社区</a>   </li>
  <li>阿里巴巴：<a href="http://tianchi.alibaba.com/index.htm">天池大数据科研平台</a>  </li>
  <li>360：<a href="http://openlab.360.cn">360开放实验室</a> </li>
  <li>卖数据的：<a href="http://datatang.com">数据堂</a></li>
</ol>

<h3 id="section-3">竞赛经验谈</h3>

<ol>
  <li><a href="http://www.52nlp.cn/cikm-competition-topdata">CIKM Competition数据挖掘竞赛夺冠算法-陈运文</a></li>
  <li><a href="http://www.chioka.in/kaggle-competition-solutions/">Kaggle Competition Past Solutions</a></li>
  <li><a href="http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/">Winning solution of Kaggle Higgs competition: what a single model can do?</a></li>
</ol>

<h2 id="demos--codes">Demos &amp; Codes</h2>

<ol>
  <li><a href="http://deeplearning.cs.toronto.edu">Toronto Deep Learning Demos</a></li>
  <li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">ConvNetJS Deep Q Learning Demo</a></li>
  <li><a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/">100 Best GitHub: Deep Learning</a></li>
  <li><a href="https://code.google.com/p/cuda-convnet2/">cuda-convnet2</a></li>
  <li><a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">Deep Visual-Semantic Alignments for Generating Image Descriptions</a></li>
</ol>

<h2 id="section-4">视频课程</h2>

<blockquote>
  <h4 id="machine-learningstanford-universitycourserahttpswwwcourseraorgcoursemlcs229httpstudy163complanplanintroduction1200146htm">Machine Learning［Stanford University］［<a href="https://www.coursera.org/course/ml">Coursera</a>、<a href="http://study.163.com/plan/planIntroduction/1200146.htm">CS229</a>］</h4>
  <hr />
  <p>Instructors：Andrew Ng  </p>

</blockquote>

<blockquote>
  <h4 id="learning-from-datacalifornia-institute-of-technologyhttpsworkcaltechedutelecoursehtml">Learning From Data［<a href="https://work.caltech.edu/telecourse.html">California Institute of Technology</a>］</h4>
  <hr />
  <p>Instructors：Yaser S. Abu-Mostafa  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learning-foundationscourserahttpswwwcourseraorgcoursentumlone">機器學習基石 (Machine Learning Foundations)［國立台灣大學］［<a href="https://www.coursera.org/course/ntumlone">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Hsuan-Tien Lin（林軒田）  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learning-techniquescourserahttpswwwcourseraorgcoursentumltwo">機器學習技法 (Machine Learning Techniques)［國立台灣大學］［<a href="https://www.coursera.org/course/ntumltwo">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Hsuan-Tien Lin（林軒田）  </p>

</blockquote>

<blockquote>
  <h4 id="introduction-to-machine-learningcarnegie-mellon-universityhttpalexsmolaorgteachingcmu2013-10-701indexhtmlhttpwwwtudoucomplcoverpk0asp6hd4s">Introduction to Machine Learning［<a href="http://alex.smola.org/teaching/cmu2013-10-701/index.html">Carnegie Mellon University</a>］［<a href="http://www.tudou.com/plcover/pK0asp6HD4s/">土豆</a>］</h4>
  <hr />
  <p>Instructors：Barnabas Poczos and Alex Smola <br />
参考教材：<em>Introduction to Machine Learning</em>［<a href="http://alex.smola.org/drafts/thebook.pdf">下载</a>］</p>

</blockquote>

<blockquote>
  <h4 id="machine-learningcarnegie-mellon-universityhttpwwwcscmuedu7etom10701sp11lecturesshtml">Machine Learning［<a href="http://www.cs.cmu.edu/%7Etom/10701_sp11/lectures.shtml">Carnegie Mellon University</a>］</h4>
  <hr />
  <p>Instructors：Tom Mitchell  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learningthe-university-of-british-columbiahttpwwwcsubccanando540-2013indexhtmlyoutubehttpswwwyoutubecomplaylistlistple6wd9fr--edyj5lbfl8uugjecvvw66f6">Machine Learning［<a href="http://www.cs.ubc.ca/~nando/540-2013/index.html">The University of British Columbia</a>］［<a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6">Youtube</a>］</h4>
  <hr />
  <p>Instructors：Nando de Freitas  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learningcornell-universityhttpmachine-learning-coursejoachimsorg">Machine Learning［<a href="http://machine-learning-course.joachims.org">Cornell University</a>］</h4>
  <hr />
  <p>Instructors：Thorsten Joachims  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learninguniversity-of-washingtoncourserahttpswwwcourseraorgcoursemachlearning">Machine Learning［University of Washington］［<a href="https://www.coursera.org/course/machlearning">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Pedro Domingos  <br />
预览可看视频</p>

</blockquote>

<blockquote>
  <h4 id="big-data-large-scale-machine-learningnew-york-universityhttpcilvrcsnyuedudokuphpidcoursesbigdataslidesstartvideohttpcilvrcsnyuedudokuphpidcoursesbigdataslidesstart">Big Data, Large Scale Machine Learning［<a href="http://cilvr.cs.nyu.edu/doku.php?id=courses:bigdata:slides:start">New York University</a>］［<a href="http://cilvr.cs.nyu.edu/doku.php?id=courses:bigdata:slides:start">Video</a>］</h4>
  <hr />
  <p>Instructors：John Langford and Yann LeCun  </p>

</blockquote>

<blockquote>
  <h4 id="probabilistic-graphical-modelscarnegie-mellon-universityhttpwwwcscmueduepxingclass10708lecturehtml">Probabilistic Graphical Models［<a href="http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html">Carnegie Mellon University</a>］</h4>
  <hr />
  <p>Instructors：Eric Xing</p>

</blockquote>

<blockquote>
  <h4 id="probabilistic-graphical-modelsstanford-universitycourserahttpswwwcourseraorgcoursepgm">Probabilistic Graphical Models［Stanford University］［<a href="https://www.coursera.org/course/pgm">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Daphne Koller</p>

</blockquote>

<blockquote>
  <h4 id="neural-networks-for-machine-learninguniversity-of-torontocourserahttpswwwcourseraorgcourseneuralnets">Neural Networks for Machine Learning［University of Toronto］［<a href="https://www.coursera.org/course/neuralnets">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Geoffrey Hinton</p>

</blockquote>

<blockquote>
  <h4 id="discrete-inference-and-learning-in-artificial-visioncole-centrale-pariscourserahttpswwwcourseraorgcourseartificialvision">Discrete Inference and Learning in Artificial Vision［École Centrale Paris］［<a href="https://www.coursera.org/course/artificialvision">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Nikos Paragios and Pawan Kumar</p>

</blockquote>

<blockquote>
  <h4 id="intro-to-machine-learning-pattern-recognition-for-fun-and-profitstanford-universityudacityhttpswwwudacitycomcourseud120">Intro to Machine Learning: Pattern Recognition for Fun and Profit［Stanford University］［<a href="https://www.udacity.com/course/ud120">Udacity</a>］</h4>
  <hr />
  <p>Instructors：Sebastian Thrun</p>

</blockquote>

<blockquote>
  <h4 id="machine-learninggeorgia-institute-of-technologyudacitysupervised-learninghttpswwwudacitycomcourseud675-unsupervised-learninghttpswwwudacitycomcourseud741-reinforcement-learninghttpswwwudacitycomcourseud820">Machine Learning［Georgia Institute of Technology］［Udacity］［<a href="https://www.udacity.com/course/ud675">Supervised Learning</a>, <a href="https://www.udacity.com/course/ud741">Unsupervised Learning</a>, <a href="https://www.udacity.com/course/ud820">Reinforcement Learning</a>］</h4>
  <hr />
  <p>Instructors：Charles Isbell and Michael Littman   </p>

</blockquote>

<blockquote>
  <h4 id="statistical-learningstanford-universityhttpsstatlearningclassstanfordedu">Statistical Learning［<a href="https://statlearning.class.stanford.edu/">Stanford University</a>］</h4>
  <hr />
  <p>Instructors：Trevor Hastie and Rob Tibshirani     <br />
参考教材：<em>An Introduction to Statistical Learning, with Applications in R</em>［<a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf">下载</a>］</p>
</blockquote>

<blockquote>
  <h4 id="httpocwsjtueducng2socwcncoursedetailshtmid397httpocwsjtueducng2socwcncoursedetailshtmid398">机器学习导论［<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397">上海交通大学</a>］／统计机器学习［<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398">上海交通大学</a>］</h4>
  <hr />
  <p>Instructors：张志华       </p>

</blockquote>

<blockquote>
  <h4 id="introduction-to-recommender-systemsuniversity-of-minnesotacourserahttpswwwcourseraorgcourserecsys">Introduction to Recommender Systems［University of Minnesota］［<a href="https://www.coursera.org/course/recsys">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Joseph A. Konstan &amp; Michael D. Ekstrand       </p>

</blockquote>

<blockquote>
  <h4 id="httpstudy163comcad">计算广告学［<a href="http://study.163.com/c/ad">网易云课堂</a>］</h4>
  <hr />
  <p>Instructors：刘鹏       </p>

</blockquote>

<blockquote>
  <h4 id="httpbigeyeautsinghuaeducndragonstar2012">机器学习［<a href="http://bigeye.au.tsinghua.edu.cn/DragonStar2012/">龙星计划2012</a>］</h4>
  <hr />
  <p>Instructors：余凯 &amp; 张潼       </p>

</blockquote>

<blockquote>
  <h4 id="httppanbaiducomsharelinkshareid3220401770uk723014463">信息处理和人工智能的深度学习［<a href="http://pan.baidu.com/share/link?shareid=3220401770&amp;uk=723014463">龙星计划2013</a>］</h4>
  <hr />
  <p>Instructors：邓力       </p>

</blockquote>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：神经网络</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-neural-networks" />
      <pubdate>2014-11-09T11:09:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-neural-networks</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_1.png"><img src="/assets/images/2014-10-21-neural-networks_1.png" alt="神经网络结构" /></a><div class="caption">Figure 1:  神经网络结构 [<a href="/assets/images/2014-10-21-neural-networks_1.png">PNG</a>]</div></div></div>

<p>神经网络是神经元分层级联构成的网络，除输入层外每个神经元对应一个计算模型。最左边和最右边的层分别称为输入（input）和输出（output）层，中间两层为隐藏层（hidden layer）。</p>

<p>当特征数目巨大时，简单的Logistic回归无法满足需求。神经网络用于解决复杂的非线性问题，可以看成是Logistic回归的组合，上图中每个橙色的神经元（除输入层之外）都对应一个Logistic方程。</p>

<p>对于分类问题，输入层输入原始数据，隐藏层的每个神经元可视为提取一种特征，输出层的每个神经元对应所属类别的概率（不是类别标签）。输入数据所属的类别是输出层概率最大神经元对应的类别。</p>

<p>神经网络通过前向传播计算给定输入对应的输出，通过误差反向传播估计权值矩阵。</p>

<h2 id="section-1">前向传播计算</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_2.png"><img src="/assets/images/2014-10-21-neural-networks_2.png" alt="神经网络前向传播计算" /></a><div class="caption">Figure 2:  神经网络前向传播计算 [<a href="/assets/images/2014-10-21-neural-networks_2.png">PNG</a>]</div></div></div>

<p>神经网络前向传播，从输入到输出，逐层计算。上图所示<a href="#ng_ml_nnr_2014">[1, P. 23]</a>，假设权值矩阵$\Theta^{(l-1)}$已知，第$l$层可通过第$l-1$层和权值矩阵前向计算，</p>

<p>\begin{equation}
\mathbf a^{(l)} = g\left(\mathbf\Theta^{(l-1)}\mathbf a^{(l-1)}\right),
\label{eq:forward-propagation}
\end{equation}</p>

<p>$g$是<a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqsigmoid-function">Logistic函数</a>，每层额外增加了一个$\mathbf a_0^{(l)}= 1$的偏移（bias），$\mathbf\Theta^{(l-1)}$的行数为第$l$层神经元个数，列数为第$l-1$层神经元个数加$1$。</p>

<p>输出层（第$L$层）神经元的输出$\mathbf a^{(L)}$确定输入特征所属的类别。</p>

<p>如果神经网络只有输入层和含一个神经元的输出层（<a href="#figure-2">上图</a>去掉隐藏层只有输入和输出层），就相当于一个Logistic回归模型。</p>

<h2 id="section-2">反向参数估计</h2>

<p>神经网络通过反向传播估计权值矩阵$\Theta$，参数估计仍然是最小化代价函数。通过BP算法（BackPropagation algorithm），输出层的误差向输入层逐层反向传播，利用梯度下降法，估计权值矩阵。</p>

<h3 id="section-3">代价函数</h3>

<p>神经网络的神经元是Logistic模型，存在和Logistic模型类似的代价函数</p>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = &amp;-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k \right) \right) \\ 
&amp;+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2.
\end{aligned}
\label{eq:cf_nn}
\end{equation}</p>

<p>$h_\Theta (\mathbf x) \in \mathbb{R}^K$，$\left(h_\Theta (\mathbf x)\right)_k = \mathbf a_k^{(L)} $是输出层第$k$个神经元的输出，可由前向传播公式\eqref{eq:forward-propagation}计算；$s_l$表示第$l$层神经元的个数（不含bias unit）；神经网络有$L$层，$m$个样本，$K$个输出。</p>

<p>如果代价函数\eqref{eq:cf_nn}是非凸（non-convex）函数<sup id="fnref:if_no_global_minimum"><a href="#fn:if_no_global_minimum" class="footnote">1</a></sup>，理论上可能会陷入局部极值，事实上，即使不能保证取得全局极值，梯度下降法也能很好的最小化代价函数，使得神经网络工作良好<a href="#ng_ml_nnl_2014">[2, P. 30]</a>。</p>

<p>对比<a href="/2014/10/machine-learning-regularization/#mjx-eqn-eqcf-logistic-regression-r">正则化Logistic回归的代价函数</a>，由于神经网络有$K$个输出，前半部分相当于$K$个Logistic回归的代价函数之和，后半部分是非bias神经元洗漱组成的正则化项，$\mathbf a_0^{(l)} = 1$对应的系数$\mathbf \Theta_{j0}^{(l)}$和Logistic回归一样，不包含在正则化系数中。</p>

<h3 id="section-4">参数估计</h3>

<p>通过最小化代价函数$\min_\Theta J(\Theta)$估计模型的所有参数矩阵$\Theta^{(l)}$，采用梯度下降法时需计算代价函数$J(\Theta)$及其梯度$\mathbf D_{ij}^{(l)}$，
\begin{equation*}
\mathbf D_{ij}^{(l)} = \frac{\partial J(\Theta)}{\partial\Theta_{ij}^{(l)}}.
\end{equation*}</p>

<p>第$l$层的误差记为$\mathbf\delta^{(l)}$，$\mathbf\delta_j^{(l)}$表示第$l$层的第$j$个神经元的误差，对于$\mathbf a_0^{(l)} = 1$的bias节点$\boldsymbol\delta_0^{(l)}=0$。输出层（$l=L$）的误差为
\begin{equation}
\boldsymbol\delta^{(L)} = \mathbf a^{(L)} - \mathbf y, 
\label{eq:error-bp-1}
\end{equation}
对于隐藏层$(l = L-1, L-2, \ldots, 2)$，误差通过权值矩阵$\Theta^{(l)}$从输出层向各隐藏层反向传播，
\begin{equation*}
\boldsymbol\delta^{(l)} = \left(\Theta^{(l)}\right)^T\boldsymbol\delta^{(l+1)} .* g’\left(\mathbf z^{(l)}\right), 
\end{equation*}</p>

<p>其中，<code>.*</code>借用了Matlab中对应元素相乘的运算符，$\mathbf z^{(l)} = \Theta^{(l-1)}\mathbf a^{(l-1)}$，$\mathbf a^{(l)} = g\left(\mathbf z^{(l)}\right)$，$g’\left(\mathbf z^{(l)}\right) = \mathbf a^{(l)} .* \left(\mathbf 1 - \mathbf a^{(l)}\right)$<sup id="fnref:d-sigmoid-f"><a href="#fn:d-sigmoid-f" class="footnote">2</a></sup>，于是可得误差回传计算公式</p>

<p>\begin{equation}
\boldsymbol\delta^{(l)} = \left(\Theta^{(l)}\right)^T\boldsymbol\delta^{(l+1)} .* \mathbf a^{(l)} .* \left(\mathbf 1 - \mathbf a^{(l)}\right).
\label{eq:error-bp-2}
\end{equation}</p>

<p>Coursera的<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>和Michael Nielsen<a href="#nielsen_nndl_2014">[3]</a>的第二章给出了BP算法的推导过程<sup id="fnref:parameter_estimation"><a href="#fn:parameter_estimation" class="footnote">3</a></sup>。</p>

<blockquote>
  <h4 id="bp">BP算法之梯度计算</h4>
  <hr />
  <p>训练集：$\left\{\left(\mathbf x^{(1)}, \mathbf y^{(1)}\right),\ldots,\left(\mathbf x^{(m)}, \mathbf y^{(m)}\right)\right\}$。 </p>

  <p>初始化： <br />
1. $\Delta_{ij}^{(l)} = 0$； <br />
2. 随机数初始化$\Theta_{ij}^{(l)}$为$[-\epsilon, \epsilon]$的值，$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}$由神经元数目确定，其中，$L_{in} = s_l$，$L_{out}=s_{l+1}$。<sup id="fnref:initial_theta"><a href="#fn:initial_theta" class="footnote">4</a></sup>  </p>

  <p>for $k=1$ to $m$ { </p>

  <ol>
    <li>初始化输入层$\mathbf a^{(1)} = \mathbf x^{(k)}$，利用前向传播\eqref{eq:forward-propagation}，计算各层神经元$\mathbf a^{(l)}~~(l = 2,\ldots, L)$；  </li>
    <li>利用反向传播\eqref{eq:error-bp-1}和\eqref{eq:error-bp-2}，计算各层误差$\boldsymbol \delta^{(l)}~~(l = 2,\ldots, L)$；   </li>
    <li>更新$\Delta_{ij}^{(l)}~~(l = 1,\ldots, L - 1)$，$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + \mathbf a_j^{(l)}\boldsymbol\delta_i^{(l+1)}$<sup id="fnref:vector_update_Delta"><a href="#fn:vector_update_Delta" class="footnote">5</a></sup>（$\mathbf a^{(l)}$也须补$\mathbf a_0^{(l)}=1$）；   </li>
    <li>计算梯度$\mathbf D_{ij}^{(l)}~~(l = 1,\ldots, L - 1)$，
\begin{equation}
\mathbf D_{ij}^{(l)} := \left\{
\begin{aligned}
&amp; \frac{1}{m}\left(\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)}\right) &amp; (j \neq 0) \\
&amp; \frac{1}{m}\Delta_{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\right. .
\label{eq:gradient-cost-f}
\end{equation}
}</li>
  </ol>
</blockquote>

<h3 id="section-5">实现细节</h3>

<p><strong>矩阵展成（unroll）向量：</strong></p>

<ol>
  <li><code class="highlight language-matlab"><span class="n">thetaVec</span> <span class="p">=</span> <span class="p">[</span><span class="n">Theta1</span><span class="p">(:);</span> <span class="n">Theta2</span><span class="p">(:);</span> <span class="n">Theta3</span><span class="p">(:)]</span></code>；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出<code class="highlight language-matlab"><span class="n">grad</span> <span class="p">=</span> <span class="p">[</span><span class="n">D1</span><span class="p">(:);</span> <span class="n">D2</span><span class="p">(:);</span> <span class="n">D3</span><span class="p">(:)]</span></code>。</li>
</ol>

<p><strong>梯度检查（gradient checking）：</strong></p>

<p>梯度计算是梯度下降法的关键。梯度检查用以验证推导的梯度计算公式及其代码实现是否正确。通过比较代价函数［比如\eqref{eq:gradient-cost-f}］梯度公式输出结果与数值方法计算代价函数的导数是否一致，判断梯度公式是否正确。梯度检测方法也可推广到其它需要梯度计算的地方，比如Logistic回归的代价函数的参数估计。梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。具体实现方法可以参考<a href="#ng_ml_nnl_2014">[2]</a>课程的编程习题。</p>

<div class="highlight"><pre><code class="language-matlab"><span class="c">% 数值方法计算导数</span>
<span class="n">numgrad</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="n">perturb</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="n">e</span> <span class="p">=</span> <span class="mf">1e-4</span><span class="p">;</span>
<span class="k">for</span> <span class="n">p</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">numel</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="c">% Set perturbation vector</span>
    <span class="n">perturb</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="n">e</span><span class="p">;</span>
    <span class="n">loss1</span> <span class="p">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">perturb</span><span class="p">);</span>
    <span class="n">loss2</span> <span class="p">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">perturb</span><span class="p">);</span>
    <span class="c">% Compute Numerical Gradient</span>
    <span class="n">numgrad</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">loss2</span> <span class="o">-</span> <span class="n">loss1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">e</span><span class="p">);</span>
    <span class="n">perturb</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">end</span></code></pre></div>

<p>如果梯度计算公式正确，<code>numgrad</code>$\approx$<code>grad</code>，通过比较<code>numgrad</code>与BP算法所得<code>grad</code>的差距判断BP算法的代价函数及其优化算法是否有subtle bugs。</p>

<div class="highlight"><pre><code class="language-matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">numgrad</span><span class="o">-</span><span class="n">grad</span><span class="p">)</span><span class="o">/</span><span class="n">norm</span><span class="p">(</span><span class="n">numgrad</span><span class="o">+</span><span class="n">grad</span><span class="p">);</span></code></pre></div>

<p><strong>注意事项：</strong></p>

<p>不可将$\Theta_{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta_{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$。</p>

<h3 id="section-6">代价函数及其梯度计算</h3>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="err"> [J, grad] = nnCostFunction(nn_params, ...</span>
                                   <span class="n">input_layer_size</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">num_labels</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
<span class="c">%NNCOSTFUNCTION Implements the neural network cost function for a two layer</span>
<span class="c">%neural network which performs classification</span>
<span class="c">%   [J, grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span>
<span class="c">%   X, y, lambda) computes the cost and gradient of the neural network. The</span>
<span class="c">%   parameters for the neural network are &quot;unrolled&quot; into the vector</span>
<span class="c">%   nn_params and need to be converted back into the weight matrices. </span>
<span class="c">% </span>
<span class="c">%   The returned parameter grad should be a &quot;unrolled&quot; vector of the</span>
<span class="c">%   partial derivatives of the neural network.</span>
<span class="c">%</span>

<span class="c">% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices</span>
<span class="c">% for our 2 layer neural network</span>
<span class="n">Theta1</span> <span class="p">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">nn_params</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="c">...</span>
                 <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>

<span class="n">Theta2</span> <span class="p">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">nn_params</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))):</span><span class="k">end</span><span class="p">),</span> <span class="c">...</span>
                 <span class="n">num_labels</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>

<span class="c">% Setup some useful variables</span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
      
<span class="c">% Feedforward Propagation</span>
<span class="n">A1</span> <span class="p">=</span> <span class="n">X</span><span class="p">;</span>  <span class="c">% input layer, matrix size: 5000x400 (5000 samples and 400 features)</span>
<span class="n">A2</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">([</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">A1</span><span class="p">]</span> <span class="o">*</span> <span class="n">Theta1</span><span class="o">&#39;</span><span class="p">);</span> <span class="c">% hidden layer, matrix size: 5000x25</span>
<span class="n">A3</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">([</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">A2</span><span class="p">]</span> <span class="o">*</span> <span class="n">Theta2</span><span class="o">&#39;</span><span class="p">);</span> <span class="c">% output layer, matrix size: 5000x10</span>

<span class="c">% Cost Function</span>
<span class="c">% a tick to compute the cost</span>
<span class="n">J_matrix</span> <span class="p">=</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A3</span><span class="p">);</span> <span class="c">% set all cost</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">J_matrix</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">))</span> <span class="p">=</span> <span class="nb">log</span><span class="p">(</span><span class="n">A3</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">)));</span>  <span class="c">% overwrite</span>
<span class="k">end</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">sum</span><span class="p">(</span><span class="n">J_matrix</span><span class="p">(:))</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">J</span> <span class="p">=</span> <span class="n">J</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="c">...</span>
    <span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">Theta1</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">sum</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">)));</span> <span class="c">% add regular term</span>

<span class="c">% Label matrix</span>
<span class="n">y_matrix</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">A3</span><span class="p">));</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">y_matrix</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">))</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">% BP error</span>
<span class="n">Delta3</span> <span class="p">=</span> <span class="n">A3</span> <span class="o">-</span> <span class="n">y_matrix</span><span class="p">;</span> <span class="c">% 5000x10</span>
<span class="n">Delta2</span> <span class="p">=</span> <span class="p">(</span><span class="n">Delta3</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">))</span> <span class="o">.*</span> <span class="p">(</span><span class="n">A2</span> <span class="o">.*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">));</span> <span class="c">% 5000x25</span>

<span class="c">% Gradient</span>
<span class="n">DELTA1</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">DELTA2</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">Delta2</span> <span class="p">=</span> <span class="n">Delta2</span><span class="o">&#39;</span><span class="p">;</span>
<span class="n">Delta3</span> <span class="p">=</span> <span class="n">Delta3</span><span class="o">&#39;</span><span class="p">;</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">DELTA1</span> <span class="p">=</span> <span class="n">DELTA1</span> <span class="o">+</span> <span class="n">Delta2</span><span class="p">(:,</span> <span class="nb">i</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span> <span class="n">A1</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="p">:)];</span>
	<span class="n">DELTA2</span> <span class="p">=</span> <span class="n">DELTA2</span> <span class="o">+</span> <span class="n">Delta3</span><span class="p">(:,</span> <span class="nb">i</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span> <span class="n">A2</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="p">:)];</span>
<span class="k">end</span>

<span class="n">Theta1_grad</span> <span class="p">=</span> <span class="n">DELTA1</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">Theta2_grad</span> <span class="p">=</span> <span class="n">DELTA2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="c">...</span>
    <span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">Theta1</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>
<span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="c">...</span>
    <span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>

<span class="n">grad</span> <span class="p">=</span> <span class="p">[</span><span class="n">Theta1_grad</span><span class="p">(:)</span> <span class="p">;</span> <span class="n">Theta2_grad</span><span class="p">(:)];</span>

<span class="k">end</span></code></pre></div>

<h2 id="section-7">神经网络学到了什么？</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_4.png"><img src="/assets/images/2014-10-21-neural-networks_4.png" alt="待识别字符" /></a><div class="caption">Figure 3:  待识别字符 [<a href="/assets/images/2014-10-21-neural-networks_4.png">PNG</a>]</div></div></div>

<p><a href="#figure-3">上图</a>展示了部分待识别的字符，通过样本学习建立了3层神经网络的分类器。输入字符图片规格是$20 \times 20$，隐层神经元25个，25个神经元对应的参数向量可视化为<a href="#figure-4">下图</a>。从可视化的参数可以看到，每个神经元和字符的笔画相似，输入的图片将激活符合“笔画”的神经元。估计神经网络参数可视为自动学习特征。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_3.png"><img src="/assets/images/2014-10-21-neural-networks_3.png" alt="参数可视化" /></a><div class="caption">Figure 4:  参数可视化 [<a href="/assets/images/2014-10-21-neural-networks_3.png">PNG</a>]</div></div></div>

<h2 id="section-8">应用</h2>

<p>卡内基梅隆大学基于神经网络的自动驾驶系统<a href="#pomerleau_alvinn_1989">[5]</a>，一些Matlab代码和数据还可以从<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/alvinn/">这里</a>找到。</p>

<h2 id="section-9">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_nnr_2014">[1]A. Ng, “Neural Networks: Representation.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_nnl_2014">[2]A. Ng, “Neural Networks: Learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="nielsen_nndl_2014">[3]M. A. Nielsen, <i>Neural Networks and Deep Learning</i>. Determination Press, 2014.</span>

[<a href="http://neuralnetworksanddeeplearning.com">Online</a>]

</li>
<li><span id="ng_ml_nnl_pe_2014">[4]A. Ng, “Programming Exercise 4: Neural Networks Learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="pomerleau_alvinn_1989">[5]D. A. Pomerleau, “ALVINN, an autonomous land vehicle in a neural network,” Carnegie Mellon University, 1989.</span>

[<a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&amp;context=compsci">Online</a>]

</li></ol>

<h3 id="section-10">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:if_no_global_minimum">
      <p>如果非凸函数，梯度下降法不能确定取得的是全局还是局部极值，可通过<a href="https://class.coursera.org/ml-007/forum/thread?thread_id=1089#comment-3416">取不同初始值多次求解增强鲁棒性</a>。 <a href="#fnref:if_no_global_minimum" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:d-sigmoid-f">
      <p><a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqd-sigmoid-function">Sigmoid函数导数</a>为$g(z)=g(z)(1-g(z))$。 <a href="#fnref:d-sigmoid-f" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:parameter_estimation">
      <p>这两篇博客（<a href="http://blog.csdn.net/abcjennifer/article/details/7758797">1</a>、<a href="http://blog.csdn.net/sheng_ai/article/details/19931347">2</a>）也给出了BP算法的推导过程，但是采用了形如<a href="/2014/10/machine-learning-linear-regression/#mjx-eqn-eqcost_function_linear_regression">线性回归的代价函数</a>。 <a href="#fnref:parameter_estimation" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:initial_theta">
      <p>不可将$\Theta_{ij}^{(l)}$初始化为$0$。若初始化为$0$，每层的所有神经元都一样<a href="#ng_ml_nnl_2014">[2, Pp. 25-26]</a>，每层只能学习到一种特征。$\epsilon$的取值方案参考课程习题脚注<a href="#ng_ml_nnl_pe_2014">[4, P. 7]</a>或<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>。 <a href="#fnref:initial_theta" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:vector_update_Delta">
      <p>更新的向量形式$\Delta^{(l)} := \Delta^{(l)} + \boldsymbol\delta^{(l+1)}\left(\mathbf a^{(l)}\right)^T$。这一步是怎么来的？有何意义？ <a href="#fnref:vector_update_Delta" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
