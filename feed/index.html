<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2015-01-21T17:07:44+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>图像分类（2）：多分类支持向量机和softmax分类器</title>
      <link href="http://qianjiye.de/2015/01/image-classification-svm-and-softmax-based-linear-classifier" />
      <pubdate>2015-01-20T12:18:40+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/image-classification-svm-and-softmax-based-linear-classifier</guid>
      <content:encoded>&lt;![CDATA[<p>本文主要参考<em>Convolutional Neural Networks for Visual Recognition</em><a href="#lifeifei_CNN_SVM_2015">[1]</a>课程笔记。</p>

<h2 id="section">预备知识</h2>

<p>k-NN在训练时只是记住了所有的样本，占用空间大，在识别时需和训练集的所有图片比较，耗费时间长。因此，需要开发新的方法克服kNN存在的问题，新方法包含两个关键部分：</p>

<ol>
  <li><strong>评分函数</strong>（score function）：将输入图像映射为所属类别的评分；</li>
  <li><strong>损失函数</strong>（loss function）：度量预测的评分与真实结果之间的一致性，也称为代价（cost）函数或目标（objective）函数。当评分函数输出结果与真实结果之间差异越大，损失函数输出越大。</li>
</ol>

<p>训练时，通过最小化损失函数，得到评分函数的参数；分类时，通过对输入图像所属类别评分，判断图像的类别。</p>

<p>训练集图像$\mathbf x_i\in\mathbb R^D(i=1,2,\ldots,N)$对应的标签$y_i\in 1,2,\ldots,K$，表示训练集有$N$张图像，每个图像表示为一个$D$维向量，图像可分为$K$个类别。对于CIFAR-10数据集，$N=50000,D=32\times 32\times 3,K=10$。评分函数$f:\mathbb R^D\mapsto \mathbb R^K$将图像数据映射为所属类别的评分。</p>

<h2 id="section-1">线性分类器</h2>

<p>线性分类器（linear classifier）就是一个评分函数，它是一个线性映射
\begin{equation}
f(\mathbf x_i,\mathbf W, \mathbf b)=\mathbf W\mathbf x_i+\mathbf b，
\label{multiclass-linear-classifier-1}
\end{equation}
其中，$\mathbf x_i$表示图像展成的$D\times 1$维的向量，$\mathbf W$是$K\times D$维的权值矩阵，$\mathbf b$是$K\times 1$维的偏移向量（bias vector）。对CIFAR-10数据集，该映射输入的输入是3072维向量，输出属于这10个类别的评分。$\mathbf W$的用每行表示一个分类器，因此可以并行计算。一旦得到这些参数$\{\mathbf W,\mathbf b\}$之后，不必像k-NN一样存储整个训练集，只需保存参数即可<sup id="fnref:CNN-f-mapping"><a href="#fn:CNN-f-mapping" class="footnote">1</a></sup>。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg" alt="线性分类器示意" /></a><div class="caption">Figure 1:  线性分类器示意 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-imagemap.jpg">JPG</a>]</div></div></div>

<p>上图展示了线性分类器计算过程，将图像简化为4个像素代替，🐶的得分最高导致了🐱误判为🐶，这并不是一个良好的分类器。权值的正负号表示了某个位置的像素对分类结果是赞成还是反对，权值的大小表示强弱程度，从这个角度看就是投票模型。</p>

<p>线性分类器可以理解为模版匹配（template matching），$\mathbf W$的每一行对应一个模版，通过图像与模版的内积（inner product）<sup id="fnref:inner-product-similarity"><a href="#fn:inner-product-similarity" class="footnote">2</a></sup>判断图像与哪个模版最相似。从这个角度看，线性分类器也是最近邻分类器，并且只需和每类学习到的唯一模版比较，效率更高，匹配时采用内积而非$L_1$或$L_2$度量距离。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg" alt="线性分类器权值矩阵图像化" /></a><div class="caption">Figure 2:  线性分类器权值矩阵图像化 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-templates.jpg">JPG</a>]</div></div></div>

<p>上图用图像的方式展示了线性分类器在CIFAR-10上学到的权值矩阵，这些图像相当于从每类学习到的模版。</p>

<p>若将图像追加一个值为1的“特征”，可将偏移向量$\mathbf b$合并到权值矩阵$\mathbf W$中，公式\eqref{multiclass-linear-classifier-1}可改写为更简练的形式
\begin{equation}
f(\mathbf x_i,\mathbf W)=\mathbf W\mathbf x_i。
\label{multiclass-linear-classifier-2}
\end{equation}</p>

<p>在机器学习中，将输入特征归一化是常用的技术。在实际中，将图像的每维特征（像素）减去均值中心化，对图像而言就是将每张图像减去平均图像，得到新图像的像素取值范围是$[-127,127]$。还可进一步将特征取值规范化到$[-1,1]$区间。</p>

<p>在线性分类器（评分函数）基础上，通过定义不同损失函数，可得到具备不同特性的多分类支持向量机和softmax分类器。</p>

<h2 id="multiclass-SVM">多分类支持向量机</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg" alt="损失函数示意图" /></a><div class="caption">Figure 3:  损失函数示意图 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-margin.jpg">JPG</a>]</div></div></div>

<p>支持向量机期望评分函数在固定边界$\Delta$控制下，正确分类比错误分类输出更高的评分。$f(\mathbf x_i,\mathbf W)_j$表示图像$\mathbf x_i$属于类别$j$的评分，多分类支持向量机的损失函数定义为
\begin{equation}
L_i = \sum_{j\neq y_i}\max(0, f(\mathbf x_i,\mathbf W)_j-f(\mathbf x_i,\mathbf W)_{y_i}+\Delta)，
\label{eq:m-svm-loss}
\end{equation}
损失函数期望正确分类比错误分类输出更小的值。支持向量机期望正确分类比错误分类至少多得分$\Delta$，这样就没有损失，如上图所示，如果任何类别的评分落在红色区间，就会有损失累加。作出好的预测等价于最小化损失。</p>

<p>$\max(0,\cdots)$一般称为<strong>hinge损失</strong>，也称为<strong>最大边界损失</strong>（max-margin loss），$L_2$-SVM的平方损失$\max(0,\cdots)^2$惩罚力度更强。hinge损失比平方hinge损失更标准通用，但有的数据集平方hinge损失效果更好，可以通过交叉验证选择损失度量方式。</p>

<p>假设$f(\mathbf x_i,\mathbf W)＝[13, -7, 11], y_i=0,\Delta=10$，那么
\[
L_i=\max(0, -7-13+10)+\max(0,11-13+10)=8。
\]
上式右边第1项，正确分类得分（13）要高于错误分类（-7），支持向量机只关心是否二者差异至少为10（事实上二者差距为20分），因此输出为0；上式右边第2项，正确分类得分（13）高于错误分类（11），但二者差异只有2，因此损失了8。</p>

<p>将\eqref{multiclass-linear-classifier-2}代入\eqref{eq:m-svm-loss}可得
\begin{equation}
L_i = \sum_{j\neq y_i}\max(0, \mathbf w_j^T\mathbf x_i - \mathbf w_{y_i}^T\mathbf x_i +\Delta)，
\end{equation}
其中，$\mathbf w_j^T$是$\mathbf W$的第$j$行元素。</p>

<p>学习（训练）阶段的目标就是期望得到权值矩阵$\mathbf W$，使得正确分类的评分高于其它所有类别，并且使损失函数尽量小。</p>

<p>假设$\mathbf W$将一个数据集的每个样本都正确分类（对所有的$i$，$L_i=0$），$\mathbf W$却不是唯一的存在，当$\lambda&gt;0$时$\lambda\mathbf W$都能胜任。通过正则化惩罚（regularization penalty）$R(\mathbf W)$，可以消除$\mathbf W$的歧义。常用的正则化惩罚抑制$L_2$范数大的$\mathbf W$出现，
\[
R(\mathbf W)=\sum_k\sum_l\mathbf W_{k,l}^2，
\]
正则化只针对权值与输入数据无关。因此，多分类支持向量机的损失函数包括两部分，数据损失（data loss）和正则化损失（regularization loss），
\begin{equation}
L={1\over N}\sum_i\sum_{j\neq y_i}\max(0, \mathbf w_j^T\mathbf x_i - \mathbf w_{y_i}^T\mathbf x_i +\Delta)+\lambda \sum_k\sum_l\mathbf W_{k,l}^2。
\label{eq:regularization-loss-function}
\end{equation}</p>

<ol>
  <li>损失函数\eqref{eq:regularization-loss-function}采用\eqref{multiclass-linear-classifier-1}的权值$\mathbf w_j$包含偏移分量$b_j$，但正则化采用\eqref{multiclass-linear-classifier-2}的权值$\mathbf W$不包含偏移向量$\mathbf b$，实际应用中即使正则化了$\mathbf b$对结果影响也不大。</li>
  <li>由于增加了正则化项，正常情况下损失函数不可能到达0。</li>
  <li>控制正则化力度的参数$\lambda$仍然只能通过交叉验证选择。</li>
  <li>采用$L_2$正则化，可以导出<a href="\sum\_k\sum\_l\mathbf W\_{k,l}^2">支持向量机的最大边界特性</a>。</li>
</ol>

<p>正则化使得没有哪一个维度可以对评分造成很大的影响，通过惩罚大的权值提高了泛化（generalization）性能，降低了过拟合（overfitting）风险。当$\mathbf x=[1,1,1,1]^T$、$\mathbf w_1=[1,0,0,0]^T$和$\mathbf w_2=[0.25,0.25,0.25,0.25]^T$时，虽然$\mathbf w_1^T\mathbf x=\mathbf w_2^T\mathbf x=1$，但$L_2$对$\mathbf w_1$的惩罚是1对$\mathbf w_2$的惩罚只有0.25，因此结果偏爱$\mathbf w_2$。$L_2$惩罚偏爱取值小且分散的权值向量，最终得到的分类器会尽量考虑所有维度的输入，而非更偏爱哪一个维度<sup id="fnref:the-golden-mean"><a href="#fn:the-golden-mean" class="footnote">3</a></sup>。</p>

<p>$\Delta$和$\lambda$虽是两个不同的参数，但都是相同的折中效果。不需交叉验证选择$\Delta$，直接设置$\Delta=1.0$即可，只需通过交叉验证调节$\lambda$。放大$\mathbf W$可以放大评分，缩小$\mathbf W$也可缩小评分
，度量评分之间的差距的$\Delta$也同步放大或缩小，而调节$\lambda$可放缩$\mathbf W$，因此固定住$\Delta$只设置$\lambda$即可。</p>

<p>对于二分类支持向量机，样本$i$的损失表示为
\[
L_i=C\max(0, 1-y_i\mathbf w^T\mathbf x_i)+R(\mathbf w)，
\]
其中$C$是超参数，$y_i\in\{-1,+1\}$。它可视为本文多分类问题简化为二分类的一个特例，$C$和$\lambda$都有相同的折中作用，$C\propto{1\over \lambda}$。</p>

<p>本文的多分类支持向量机，只是支持向量机解决多分类问题的方法之一。除此之外还有一对多（OVA，one-vs-all）策略和实际中很少使用的多对多（AVA，all-vs-all）。简单的OVA策略能很好处理多分类问题<a href="#Rifkin04indefense">[2]</a>，本文的方法是OVA的增强版<a href="#Weston99supportvector">[3]</a>，理论上数据损失项可达0，常规的OVA方法则不行。</p>

<h2 id="softmax-classifier">softmax分类器</h2>

<p>softmax分类器是logistic回归分类器在多分类问题的推广。$f_j(\mathbf z)={e^{z_j}\over\sum_k e^{z_k}}$被称为softmax函数（softmax function），它将实数转换为$[0,1]$区间的值。softmax分类器采用同样的评分函数\eqref{multiclass-linear-classifier-2}，但采用<strong>交叉熵损失</strong>（cross-entropy loss）函数
\begin{equation}
L
=-{1\over N}\sum_{i=1}^N\log\left({e^{f_{y_i}}\over\sum_j e^{f_j}}\right)+R(\mathbf W)
=-{1\over N}\sum_{i=1}^N\left(f_{y_i}-\log\sum_j e^{f_j}\right)+R(\mathbf W)。
\end{equation}</p>

<p>“真实”分布$p$和估计所得分布$q$之间的交叉熵定义为
\[
H(p,q)=-\sum_xp(x)\log q(x)。
\]
对softmax分类器，属于类别$y_i$概率的估计值$q={e^{f_{y_i}}\over\sum_j e^{f_j}}$，属于类别$y_i$概率的“真实”值应为$p=1$。</p>

<p>交叉熵可改写为熵与<a href="http://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler散度</a>（divergence）之和的形式
\[
H(p,q)=H(p)+D_{KL}(p\| q)，
\]
其中$H(p)=-\sum_xp(x)\log p(x)$，对于本文分类问题$H(p)=0$。最小化代价函数等价于最小化两个分布之间的KL散度。通过交叉熵，期望能预测的分布能在正确分类的类别上概率达到最大。</p>

<p>softmax函数可以视为概率形式
\[
P(y_i|\mathbf x_i;\mathbf W)={e^{f_{y_i}}\over\sum_j e^{f_j}}，
\]
表示预测正确分类$y_i$的归一化概率。好的分类结果是$\prod_{i=1}^NP(y_i|\mathbf x_i;\mathbf W)$最大，这可认为是最大似然估计（MLE，maximum likelihood estimation）。正则化项$R(\mathbf W)$可认为源自权值矩阵的高斯先验（Gaussian prior），最小化损失函数相当于最大后验概率估计（maximum a posteriori）。</p>

<p>编程实现时，$e^{f_{y_i}}$和$\sum_j e^{f_j}$的值可能很大，大数值相除不稳定，所以需要采用规范化技巧（normalization trick）。softmax函数可以写为等价的形式
\[
\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}=\frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}=\frac{e^{f_{y_i}+\log C}}{\sum_j e^{f_j+\log C}}，
\]
提高计算的稳定性，通常选择$\log C=-\max_jf_j$，将评分的最大值规范化为0。</p>

<h2 id="svm-vs-softmax">SVM vs. softmax</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png"><img src="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png" alt="SVM和softmax的对比" /></a><div class="caption">Figure 4:  SVM和softmax的对比 [<a href="/assets/images/2015-01-20-image-classification-svm-and-softmax-based-linear-classifier-svmvssoftmax.png">PNG</a>]</div></div></div>

<p>支持向量机的结果不易理解，softmax分类器给出了所属类别的“概率”解释。事实上，概率分布是平坦还是尖峰依赖于正则化参数$\lambda$。</p>

<p>当线性分类器输出$[1, -2, 0]$时，softmax函数输出“概率”
\[
[1, -2, 0]\rightarrow 
[e^1, e^{-2}, e^0]=
[2.71,0.14,1]\rightarrow
[0.7,0.04,0.26]。
\]
当增大正则化系数$\lambda$，这可能导致更小的权值，线性分类器的输出假设减小到$[0.5, -1, 0]$，此时softmax函数输出“概率”
\[
[0.5, -1, 0]\rightarrow 
[e^{0.5}, e^{-1}, e^0]=
[1.65,0.37,1]\rightarrow
[0.55,0.12,0.33]，
\]
概率分布变得更均匀。大的正则化系数$\lambda$导致权值变小，类别的输出概率更趋于均匀化（uniform）。因此，softmax函数输出的“概率”之间大小比较有意义，单独看某个“概率”值的大小没有明确意义，也就是相对大小有意义，绝对大小没意义。</p>

<p>多分类支持向量机和softmax分类器，在性能表现上通常很相近。假设$\mathbf x_i$属于第1类，对于$[10, -100, -100]$和$[10, 9, 9]$，多分类支持向量机都不再增加损失函数的值；但对于softmax分类器，$[10, 9, 9]$会比$[10, -100, -100]$让损失函数增加更大的值。softmax分类器永不满足，总是试图让正确的分类概率更大，损失函数的值更小；然而，只要多分类支持向量机满足边界条件，不论评分高低，不再调整评分，这可以看作是多分类支持向量机的一个特性，比如对于一个车辆分类器，它应该关注如何分类轿车和卡车等困难问题，而不应受已经评分很低🐸🐱🐶等的影响。</p>

<h2 id="section-2">参考文献</h2>

<ol class="bibliography"><li><span id="lifeifei_CNN_SVM_2015">[1]F.-F. Li and A. Karpathy, “Linear classification: Support Vector Machine, Softmax.” GitHub, 2015.</span>

[<a href="http://cs231n.github.io/linear-classify/">Online</a>]

</li>
<li><span id="Rifkin04indefense">[2]R. Rifkin and A. Klautau, “In defense of one-vs-all classification,” <i>Journal of Machine Learning Research</i>, vol. 5, pp. 101–141, 2004.</span>

</li>
<li><span id="Weston99supportvector">[3]J. Weston and C. Watkins, “Support Vector Machines for Multi-Class Pattern Recognition,” in <i>European Symposium on Artificial Neural Networks</i>, Bruges (Belgium), 1999, pp. 219–224.</span>

[<a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:CNN-f-mapping">
      <p>CNN也会将数据像素映射为得分，只是映射$f$会更复杂，参数更多。 <a href="#fnref:CNN-f-mapping" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:inner-product-similarity">
      <p>内积就是余弦相似度？ <a href="#fnref:inner-product-similarity" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:the-golden-mean">
      <p>多么美妙的中庸之道！ <a href="#fnref:the-golden-mean" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>图像分类（1）：基于k最近邻算法的简介</title>
      <link href="http://qianjiye.de/2015/01/image-classification-knn-based-introduction" />
      <pubdate>2015-01-19T17:20:03+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/image-classification-knn-based-introduction</guid>
      <content:encoded>&lt;![CDATA[<p>本文主要参考<em>Convolutional Neural Networks for Visual Recognition</em><a href="#lifeifei_CNN_kNN_2015">[1]</a>课程笔记。</p>

<h2 id="section">简介</h2>

<p>图像分类的任务是根据已知的确定标签集，为输入图像分配一个标签（标签就是所谓的类别）。其它一些计算机视觉问题，比如目标提取、分割等，都可以被归结到图像分类。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png" alt="图像分类示例" /></a><div class="caption">Figure 1:  图像分类示例 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-classify.png">PNG</a>]</div></div></div>

<p>上图展示一个图像分类模型，通过计算属于4个标签｛🐱，🐶，🎩，🍵｝的概率，为图像分配最大概率对应的标签。彩色图像用3维矩阵表示，本例中宽248像素，高400像素的图像，用248×400×3的矩阵表示。矩阵的每个元素对应一个像素值，取值是0到255的整数。具体来说，图像分类的任务是通过这些像素值，利用计算机视觉算法，得到类别标签（本例输入图片的类别标签是🐱）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg" alt="图像分类面临的主要挑战" /></a><div class="caption">Figure 2:  图像分类面临的主要挑战 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-challenges.jpeg">JPEG</a>]</div></div></div>

<p>计算机视觉算法进行图像分类面临的主要挑战包括：视角变化（viewpoint variation）、尺度变化（scale variation）、形变（deformation）、光照影响（illumination conditions）、背景混杂（background clutter）、类内变化（intra-class variation）等，如上图所示。好的图像分类模型应当能应对这些挑战。</p>

<p>图像分类算法与传统的计算机算法（比如排序）开发不同。首先需要给计算输入供包含每类若干示例图像的<a href="http://cs231n.github.io/assets/trainset.jpg">训练集</a>；然后开发学习算法从样本集中的每类学习；最后通过预测结果评估算法性能。这种依赖已标注样本集的方法称为<strong>数据驱动的方法</strong>（data-driven approach）。</p>

<p>图像分类算法开发的流程如下：</p>

<ol>
  <li>输入：输入包含$N$张图像，已经用K个标签之一标注了每张图像，这称为训练集（training set）。</li>
  <li>学习：开发学习算法，通过训练集学习每类的样子，这称为训练分类器（training classifier）或学习模型（learning a model）。</li>
  <li>评估：输入分类算法没有学习过的图片，通过算法预测标签，根据预测和真实结果的对比评估算法的效果。</li>
</ol>

<h2 id="section-1">最近邻分类器</h2>

<p>最近邻分类器（nearest neighbor classifier）容易实现，本文通过它介绍图像分类的基本方法流程，但是在实际应用中很少使用该方法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg" alt="［左］：CIFAR-10示例图像；［右］：与第1列最相邻的10张图片" /></a><div class="caption">Figure 3:  ［左］：CIFAR-10示例图像；［右］：与第1列最相邻的10张图片 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-nn.jpg">JPG</a>]</div></div></div>

<p>图像数据集采用<a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>，它包含尺寸32×32的60000张小图，每张图片已经被｛✈️，🚗，🐦，🐱，……｝等10个标签之一标注，如上图左所示。60000张图片被分割成50000张图片（每类5000张）的训练集和10000张图片（每类1000张）的测试集。</p>

<p>最近邻分类器训练分类器的方法，就是记住训练集即可。在预测的时候直接和训练集中的每张图片比较，得到输入图像与训练集中最相邻那张图片的标签，将该标签作为预测结果。上图右是最近邻分类器的结果，其中第8行，与第1列🐎最相邻的是🚗，🐎就会被误标记为🚗。</p>

<p>最近邻算法度量两张图片的相邻程度，通常采用像素值之间的$L_1$距离或$L_2$距离：
\[
d_1(\mathbf I_1,\mathbf I_2)=\sum_p\left\lvert\mathbf I_1^p-\mathbf I_2^p\right\rvert；\qquad d_2(\mathbf I_1,\mathbf I_2)=\sqrt{\sum_p\left(\mathbf I_1^p-\mathbf I_2^p\right)^2}。
\]
在CIFAR-10数据集上，用$L_1$距离得到的分类正确率大约是38.6%，$L_2$距离得到的分类正确率大约是35.4%，高于随机猜想10%的精度，目前最先进的卷积神经网络（CNN，convolutional neural networks）<a href="http://www.kaggle.com/c/cifar-10/leaderboard">正确率在95%以上</a><sup id="fnref:what-is-kaggle-score"><a href="#fn:what-is-kaggle-score" class="footnote">1</a></sup>。</p>

<p>在度量两个向量差异时，$L_2$的效果比$L_1$糟糕（That is, the $L_2$ distance prefers many medium disagreements to one big one. ）。$L_1$和$L_2$是常用的两个<a href="http://planetmath.org/vectorpnorm">p范数</a>特例。</p>

<h2 id="kNN">k最近邻分类器</h2>

<p>k最近邻分类器（k-NN，k-nearest neighbor classifier）是对最近邻分类器的简单扩展：从训练集中选出与输入图像最相邻的k张图像的类别标签，通过这k个标签对输入图像的类别标签进行投票。k=1时，k最近邻分类器和最近邻分类器等价。直观上理解，k最近邻分类器取大的k值，对判别边界有平滑作用，能更好的抗击噪声干扰。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg" alt="k最近邻分类器的分类效果" /></a><div class="caption">Figure 4:  k最近邻分类器的分类效果 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-knn.jpeg">JPEG</a>]</div></div></div>

<p>上图的k最近邻分类器采用的是$L_2$距离。上图中可以看到，蓝色区域中的小块绿色孤岛是由于噪声点干扰导致的，这将导致预测错误；上图右的5-NN不受这些异常值的干扰，能在测试集上取得跟好的泛化（generalization）效果。上图右的灰色表示有争议的区域，在这些区域至少2个类别标签都得到了最高票。</p>

<h2 id="section-2">交叉验证</h2>

<p>在实际应用中，如何选择kNN的参数k呢？除k之外，还有度量距离的$L_1$和$L_2$等其它参数需要调节。这些候选参数称为<strong>超参数</strong>（hyperparameters）。在基于数据驱动的机器学习算法中，参数选择非常普遍。</p>

<p>在实际应用中，不能采用测试集选择参数。如果采用测试集调整参数，分类器就可能对测试集过拟合（overfit），当最终发布到应用环境，分类器的性能可能大打折扣。用测试集调整参数，相当于把测试集当训练集使用。测试集只应该在最终测试分类器泛化性能时，被使用1次。</p>

<p>合适的做法是从训练集分割出一个较小的子集，作为验证集（validation set）。对CIFAR-10数据，可以用49,000个图像作为训练集，利用剩下了1,000个图像作为验证集进行参数调节。在选择参数k的时候，在验证集上测试每个k模型的性能，从中选择使性能达到最好的k。选定k之后，在测试集上仅进行一次性能评估。</p>

<p>当训练集合测试集较小的时候，可以采用更聪明的<strong>交叉验证</strong>（cross-validation）进行参数选择。通过评估不同验证集上的平均性能，选择合适的参数。以5-fold的交叉验证为例：</p>

<ol>
  <li>将训练集分割为5等份；</li>
  <li>选择1份作为验证集，剩余的4份组成训练集，在验证集上评估参数的性能；</li>
  <li>轮流将5份作为训练集，将5次性能的平均作为最终评估结果。</li>
</ol>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png" alt="5-fold交叉验证选择参数k的正确率曲线" /></a><div class="caption">Figure 5:  5-fold交叉验证选择参数k的正确率曲线 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-cvplot.png">PNG</a>]</div></div></div>

<p>上图展示了5-fold交叉验证的效果，k=7是个不错的参数。若分割的等份大于5，上图的曲线将变得更加光滑。</p>

<p>在实际应用中，由于交叉验证计算量很大，因而会选择采用单一验证集而避免采用交叉验证。通常用50%到90%的数据作为训练集，剩下的作为验证集，候选超参数集越大，验证集越大。当验证集很小时（比如仅仅几百个数据），采用交叉验证是比较保险的方法，它能减少参数选择时的噪声干扰，常用的有3-fole、5-fold、10-fold交叉验证。</p>

<p>另一个需要考虑的问题是，在通过验证集确定了最佳参数后，是否需要利用整个训练集和最佳参数重新学习。由于放回了验证集，整个训练集上的表现必然有所不同。实际上，最终发布的分类器不需要验证集的参与。</p>

<h2 id="section-3">评价与建议</h2>

<p>k最近邻算法的主要优点是容易理解和实现，并且训练不时耗；主要缺点是测试（预测）时耗高。在实际应用中，主要关注的是测试（预测）时耗。深度神经网络（DNN，deep neural networks）相反，训练耗时但预测高效，在实际中更适用。</p>

<p>最近邻算法也是一个活跃的研究领域。近似最近邻算法（ANN，approximate nearest neighbor）通过对精度和速度（空间）耗费的折中提高效率，比如<a href="http://www.cs.ubc.ca/research/flann/">FLANN</a>。这些算法通常需要通过kd树或k均值算法预处理或建立索引。</p>

<p>k最近邻算法有时是不错的选择，尤其是数据维数较低的时候，但在图像分类中几乎很少使用。图像是高维数据，高维空间的距离度量表现有些反直觉（counter-intuitive）。</p>

<div class="image_line" id="figure-6"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png" alt="" /></a><div class="caption">Figure 6:   [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-samenorm.png">PNG</a>]</div></div></div>

<p>利用$L_2$距离度量上图中小图的相似性，结果违反直觉。其它图都是最左图变换得到的，人眼观察这些图比较相似，但是基于$L_2$的度量表明其它图和原图差异很大。实际山，像素级别的距离度量无法判断基于直觉和语义的相似性。</p>

<div class="image_line" id="figure-7"><div class="image_card"><a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg"><img src="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg" alt="利用t-SNE展示CIFAR-10图像" /></a><div class="caption">Figure 7:  利用t-SNE展示CIFAR-10图像 [<a href="/assets/images/2015-01-19-image-classification-knn-based-introduction-pixels_embed_cifar10.jpg">JPG</a>]</div></div></div>

<p>上图用<a href="http://lvdmaaten.github.io/tsne/">t-SNE</a>展示CIFAR-10的图像，在像素级的$L_2$距离度量下，相似的图像相邻排列。结果表明并所非所期望的那样，同类别的相邻排列。实际上，这种度量严重受背景和颜色分布的影响，并非真正度量图像内容的相似性。</p>

<p>k最近邻算法在实际使用中的建议：</p>

<ol>
  <li>数据预处理：特征向量均值归0化、方差单位化<sup id="fnref:why-not-here"><a href="#fn:why-not-here" class="footnote">2</a></sup>；</li>
  <li>对高维数据用<a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">PCA</a>或<a href="http://scikit-learn.org/stable/modules/random_projection.html">随机投影</a>（random projections）等算法降维处理；</li>
  <li>根据前文建议采用验证（50%到90%数据作为训练集）或交叉验证，通常fold数越多，效果越好，但也越耗时；</li>
  <li>通过验证集选择k（候选k越多越好）和距离度量方式；</li>
  <li>如果算法太耗时，尝试采用ANN加速。</li>
</ol>

<h2 id="section-4">示例代码</h2>

<p>下文中代码所需要的函数和数据<a href="http://vision.stanford.edu/teaching/cs231n/assignment1.zip">在这里获取</a>。</p>

<div class="highlight"><pre><code class="language-python"><span class="n">__author__</span> <span class="o">=</span> <span class="s">&#39;jiyeqian&#39;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">cs231n.data_utils</span> <span class="kn">import</span> <span class="n">load_CIFAR10</span>

<span class="c"># a magic function we provide</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">load_CIFAR10</span><span class="p">(</span><span class="s">&#39;cs231n/datasets/cifar-10-batches-py&#39;</span><span class="p">)</span>
<span class="c"># Subsample the data for more efficient code execution</span>
<span class="n">num_training</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training</span><span class="p">)</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">Xtr</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">Ytr</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">mask</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
<span class="n">Xte</span><span class="p">,</span> <span class="n">Yte</span> <span class="o">=</span> <span class="n">Xte</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">Yte</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="c"># flatten out all images to be one-dimensional</span>
<span class="n">Xtr_rows</span> <span class="o">=</span> <span class="n">Xtr</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xtr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c"># Xtr_rows becomes 5000 x 3072</span>
<span class="n">Xte_rows</span> <span class="o">=</span> <span class="n">Xte</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Xte</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="c"># Xte_rows becomes 500 x 3072</span>

<span class="k">class</span> <span class="nc">NearestNeighbor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span>
        <span class="c"># the nearest neighbor classifier simply remembers all the training data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span>
        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c"># lets make sure that the output type matches the input type</span>
        <span class="n">Ypred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c"># loop over all test rows</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
            <span class="c"># find the nearest training image to the i&#39;th test image</span>
            <span class="c"># using the L1 distance (sum of absolute value differences)</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
            <span class="c"># L2 distance</span>
            <span class="c"># distances = np.linalg.norm(self.Xtr - X[i,:], axis = 1)</span>
            <span class="c"># min_index = np.argmin(distances) # get the index with smallest distance</span>
            <span class="c"># Ypred[i] = self.ytr[min_index] # predict the label of the nearest example</span>
            <span class="n">fre_idx</span><span class="p">,</span> <span class="n">fre_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="p">[</span><span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]],</span> \
                                         <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">Ypred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">fre_idx</span><span class="p">[</span><span class="n">fre_num</span> <span class="o">==</span> <span class="n">fre_num</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">Ypred</span>


<span class="c"># Part 1: kNN prediction</span>

<span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span> <span class="c"># create a Nearest Neighbor classifier class</span>
<span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span> <span class="c"># train the classifier on the training images and labels</span>
<span class="n">Yte_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte_rows</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># predict labels on the test images</span>
<span class="c"># and now print the classification accuracy, which is the average number</span>
<span class="c"># of examples that are correctly predicted (i.e. label matches)</span>
<span class="k">print</span> <span class="s">&#39;accuracy: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yte_predict</span> <span class="o">==</span> <span class="n">Yte</span><span class="p">)</span> <span class="p">)</span>


<span class="c"># Part 2: validation for choosing k</span>

<span class="c"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span>
<span class="c"># recall Xtr_rows is 5,000 x 3072 matrix</span>
<span class="n">Xval_rows</span><span class="p">,</span> <span class="n">Yval</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[:</span><span class="mi">1000</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Ytr</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span><span class="c"># take first 1000 for validation</span>
<span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span> <span class="o">=</span> <span class="n">Xtr_rows</span><span class="p">[</span><span class="mi">1000</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">Ytr</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span> <span class="c"># keep last 4,000 for train</span>

<span class="c"># find hyperparameters that work best on the validation set</span>
<span class="n">validation_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>

    <span class="c"># use a particular value of k and evaluation on validation data</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="n">NearestNeighbor</span><span class="p">()</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">Xtr_rows</span><span class="p">,</span> <span class="n">Ytr</span><span class="p">)</span>
    <span class="c"># here we assume a modified NearestNeighbor class that can take a k as input</span>
    <span class="n">Yval_predict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xval_rows</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Yval_predict</span> <span class="o">==</span> <span class="n">Yval</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&#39;accuracy: </span><span class="si">%f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">,)</span>

    <span class="c"># keep track of what works on the validation set</span>
    <span class="n">validation_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span></code></pre></div>

<p>k最近邻算法预测阶段计算复杂度非常高，为了加快计算速度，上述代码只抽取了10%的数据，得到的结果如下：</p>

<div class="highlight"><pre><code># Part 1:
accuracy: 0.290000
# Part 2:
accuracy: 0.291000
accuracy: 0.269000
accuracy: 0.275000
accuracy: 0.289000
accuracy: 0.287000
accuracy: 0.285000
accuracy: 0.283000
</code></pre></div>

<h2 id="section-5">参考文献</h2>

<ol class="bibliography"><li><span id="lifeifei_CNN_kNN_2015">[1]F.-F. Li and A. Karpathy, “Image classification: data-driven approach, nearest neighbor, train/val/test splits.” GitHub, 2015.</span>

[<a href="http://cs231n.github.io/classification/">Online</a>]

</li></ol>

<h3 id="section-6">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:what-is-kaggle-score">
      <p>kaggle排名的得分（score）是如何计算的？ <a href="#fnref:what-is-kaggle-score" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-here">
      <p>We will cover this in more detail in later sections, and chose not to cover data normalization in this section because pixels in images are usually homogeneous and do not exhibit widely different distributions, alleviating the need for data normalization. <a href="#fnref:why-not-here" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>logistic回归</title>
      <link href="http://qianjiye.de/2015/01/logistic-regression" />
      <pubdate>2015-01-17T16:28:27+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<p>本节的主要参考资料是机器学习基石<a href="#lin_ml_logistic_regression_2014">[1]</a>和机器学习<a href="#ng_ml_lr_2014">[2]</a>网络课程。</p>

<h2 id="soft">soft二分类</h2>

<p>soft二分类感兴趣的不仅仅是$\{-1,+1\}$类别的判断，而是属于某个类别的可能性，比如
\begin{equation}
f(\mathbf x)=P(+1|\mathbf x)\in [0,1]。
\label{eq:f-vs-probility}
\end{equation}
但是在实际应用中，难以获取属于某个类别的概率，只能得到是否属于某个类别，也就是与二分类一样的数据集。</p>

<p>不妨将$\{-1,+1\}$类别标签的数据，看作是概率标签数据被噪声污染的结果，e.g. $(\mathbf x_1,y’_1=0.9=P(+1|\mathbf x_1))+{noise}\rightarrow(\mathbf x_1,y_1=\circ\sim P(+1|\mathbf x_1))$。</p>

<p>logistic回归解决的问题，就是在$\{-1,+1\}$类别标签的训练集上，通过soft二分类产生概率标签的输出。</p>

<h2 id="logistic">logistic回归模型</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-logistic-function.png"><img src="/assets/images/2015-01-17-logistic-regression-logistic-function.png" alt="logistic函数" /></a><div class="caption">Figure 1:  logistic函数 [<a href="/assets/images/2015-01-17-logistic-regression-logistic-function.png">PNG</a>]</div></div></div>

<p>对于特征$\mathbf x=(x_0, x_1,\ldots,x_d)$，先计算加权的risk score，$s=\mathbf w^T\mathbf x$，然后再将此得分代入如上图所示的logistic函数<sup id="fnref:logistic-function-properties"><a href="#fn:logistic-function-properties" class="footnote">1</a></sup>
\begin{equation}
\theta(s)={1\over 1 + e^{-s}}
\label{eq:sigmoid-function}
\end{equation}
转化为概率的形式。logistic函数也称为sigmoid函数，单调、光滑。也就是可用logistic回归模型
\begin{equation}
h(\mathbf x)={1\over 1+\exp(-\mathbf w^T\mathbf x)}，
\end{equation}
作为目标函数$f(\mathbf x)=P(y|\mathbf x)$的近似。</p>

<p>建立logistic回归模型的基本思路是，找到最可能产生数据集$\mathcal D$的假设$h$。考虑数据集$\mathcal D=\{(\mathbf x_1,\circ),(\mathbf x_2,\times),\ldots,(\mathbf x_N,\times)\}$，根据\eqref{eq:f-vs-probility}可得<sup id="fnref:andrew-class-label"><a href="#fn:andrew-class-label" class="footnote">2</a></sup>
\[
P(y|\mathbf x)=
\left\{
\begin{aligned}
&amp;f(\mathbf x)&amp;\mbox{for }y=+1&amp;\\
&amp;1-f(\mathbf x)&amp;\mbox{for }y=-1&amp;，
\end{aligned}
\right.
\]
那么得到这个数据集的概率为
\[
\begin{aligned}
&amp;P(\mathbf x_1)P(\circ|\mathbf x_1)\times P(\mathbf x_2)P(\times|\mathbf x_2)\times\ldots P(\mathbf x_N)P(\times|\mathbf x_N)\\
=&amp;P(\mathbf x_1)f(\mathbf x_1)\times P(\mathbf x_2)(1-f(\mathbf x_2))\times\ldots P(\mathbf x_N)(1-f(\mathbf x_N))\Rightarrow\mbox{probability using }f\\
\approx &amp;P(\mathbf x_1)h(\mathbf x_1)\times P(\mathbf x_2)(1-h(\mathbf x_2))\times\ldots P(\mathbf x_N)(1-h(\mathbf x_N))\Rightarrow\mbox{likelihood}(h)，
\end{aligned}
\]
$h$产生这笔数据的可能性（likelihood）和$f$产生这笔数据的概率（probability）差不多，最终得到了$h$产生这笔数据的可能性。期望$f$产生这笔数据的概率相当大<sup id="fnref:why-large-probability"><a href="#fn:why-large-probability" class="footnote">3</a></sup>，因此有
\[
\mbox{likelihood}(h)\approx\mbox{probability using }f\approx\mbox{large}，
\]
选择可能性最高的那个$h$
\[
g=\arg\max_h\mbox{likelihood}(h)。
\]
根据logistic函数的性质$\theta(-s)=1-\theta(s)$，可进一步得到
\[
\mbox{likelihood}(h)=P(\mathbf x_1)h(+\mathbf x_1)\times P(\mathbf x_2)(-h(\mathbf x_2))\times\ldots P(\mathbf x_N)(-h(\mathbf x_N))，
\]
对所有$h$，$P(\mathbf x_i)$都不变，那么可得
\[
\mbox{likelihood}(\mbox{logistic }h)\varpropto\prod_{n=1}^Nh(y_n\mathbf x_n)，
\]
最佳$\mathbf w$应满足条件
\[
\max_{\mathbf w}\mbox{likelihood}(\mathbf w)\varpropto\prod_{n=1}^N\theta(y_n\mathbf w^T\mathbf x_n)。
\]
利用$\ln$将乘法化为加法，得到等价的优化问题
\begin{equation}
\min_\mathbf wE_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\ln\left(1+\exp\left(-y_n\mathbf w^T\mathbf x_n\right)\right)，
\end{equation}
其中$err(\mathbf w,\mathbf x,y)=\ln\left(1+\exp\left(-y\mathbf w^T\mathbf x\right)\right)$称为<strong>交叉熵误差</strong>（cross-entropy error）。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-convex-or-non.png"><img src="/assets/images/2015-01-17-logistic-regression-convex-or-non.png" alt="［左］：平方误差代价函数；［右］交叉熵误差代价函数" /></a><div class="caption">Figure 2:  ［左］：平方误差代价函数；［右］交叉熵误差代价函数 [<a href="/assets/images/2015-01-17-logistic-regression-convex-or-non.png">PNG</a>]</div></div></div>

<p>上图展示了用不同误差度量方式的代价函数，上图左采用了线性回归基于平方误差的代价函数，非凸不利于优化。</p>

<h2 id="section">梯度下降法</h2>

<p>$E_{in}(\mathbf w)$是连续、可微、二次可微的凸函数，理论上取得最小值时$\mathbf w$满足$\nabla E_{in}(\mathbf w)=0$，其中
\begin{equation}
\nabla E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\theta(-y_n\mathbf w^T\mathbf x_n)(-y_n\mathbf x_n)。
\label{eq:gradient-logistic-object-function}
\end{equation}
由于$\nabla E_{in}(\mathbf w)=0$是非线性方程，并且$\theta(-y_n\mathbf w^T\mathbf x_n)＝0$的条件$y_n\mathbf w^T\mathbf x_n\gg 0$也难以满足，因此不存在类似线性回归的闭式解。</p>

<p>回顾<a href="/2014/10/machine-learning-perceptron-learning-algorithm/#pla-algorithm">感知器算法</a>的权值更新，稍微修改其表现形式
\[
\mathbf w_{t+1}\leftarrow\mathbf w_{t}+1\cdot\left(\left[\left[\mbox{sign}\left(\mathbf w_t^T\mathbf x_n\right)\neq y_n\right]\right]y_n\mathbf x_n\right)，
\]
简记为
\[
\mathbf w_{t+1}\leftarrow\mathbf w_{t}+\eta\mathbf v，
\]
其中$\eta=1$表示步长，$\mathbf v=\left[\left[\mbox{sign}\left(\mathbf w_t^T\mathbf x_n\right)\neq y_n\right]\right]y_n\mathbf x_n$表示方向。对$(\eta,\mathbf v)$和终止条件的不同定义，就会得到不同的迭代优化算法。考察梯度计算公式\eqref{eq:gradient-logistic-object-function}，$\theta(-y_n\mathbf w^T\mathbf x_n)$相当于梯度方向的加权值，当$y_n\mathbf w^T\mathbf x_n$越小的时候，权值越大，负值越大表示犯错越厉害。如果按照梯度方向更新，这和PLA有相同的含义，犯错越厉害的对更新贡献越大。</p>

<p>对于logistic回归的$E_{in}$，令$\eta&gt;0$，
\[
\min_{\lVert\mathbf v\rVert=1}E_{in}(\mathbf w_t+\eta\mathbf v)
\]
利用贪婪法找$\mathbf w_t$附近最好的一个方向，让$E_{in}$下降最多。上式仍然是非线性，且带约束条件，难以求解。当$\eta$足够小时，利用Taylor展式可得
\[
E_{in}(\mathbf w_t+\eta\mathbf v)\approx E_{in}(\mathbf w_t)+\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)
\]
若要实现
\[
\min_{\lVert\mathbf v\rVert=1}\left(E_{in}(\mathbf w_t)+\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)\right)，
\]
就要使得$\eta\mathbf v^T\nabla E_{in}(\mathbf w_t)$最小，此时$\eta&gt;0$是常数，因此$\mathbf v$和$\nabla E_{in}(\mathbf w_t)$反向时得到的值最小，
\[
\mathbf v=-\frac{\nabla E_{in}(\mathbf w_t)}{\lVert\nabla E_{in}(\mathbf w_t)\rVert}，
\]
那么参数的更新规则为
\begin{equation}
\mathbf w_{t+1}\leftarrow\mathbf w_{t}-\eta\frac{\nabla E_{in}(\mathbf w_t)}{\lVert\nabla E_{in}(\mathbf w_t)\rVert}。
\label{eq:gd-iterate}
\end{equation}
这种参数更新方法就是梯度下降法，是一种简单且应用广泛的优化算法。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-17-logistic-regression-eta-example.png"><img src="/assets/images/2015-01-17-logistic-regression-eta-example.png" alt="不同步长的迭代效果" /></a><div class="caption">Figure 3:  不同步长的迭代效果 [<a href="/assets/images/2015-01-17-logistic-regression-eta-example.png">PNG</a>]</div></div></div>

<p id="why-fixed-eta">上图展示了利用\eqref{eq:gd-iterate}迭代的效果，$\eta$过小导致收敛太慢，过大导致不稳定，动态调整可以得到不错的效果。对于动态调整的$\eta$，坡度$\lVert\nabla E_{in}(\mathbf w_t)\rVert$大的时候采用大的$\eta$，小的时候采用小的$\eta$。当$\eta\leftarrow{\eta\over\lVert\nabla E_{in}(\mathbf w_t)\rVert}$时，固定住新的$\eta$就会达到动态调整的效果，因此可以采用固定$\eta$的自适应步长更新方式</p>
<p>\begin{equation}
\mathbf w_{t+1}\leftarrow\mathbf w_{t}-\eta\nabla E_{in}(\mathbf w_t)。
\label{eq:gd-iterate-2}
\end{equation}</p>

<blockquote>
  <h4 id="logistic-1">梯度下降法求解logistic回归参数</h4>
  <hr />
  <p>初始化$\mathbf w_0$和$\eta$；</p>

  <p>对$t=0,1,\ldots$循环以下步骤，直到$\nabla E_{in}(\mathbf w_{t+1})=0$或达到设定迭代次数：       </p>

  <ol>
    <li>利用公式\eqref{eq:gradient-logistic-object-function}计算$\nabla E_{in}(\mathbf w_{t})$；</li>
    <li>利用公式\eqref{eq:gd-iterate-2}更新参数$\mathbf w_{t+1}$。</li>
  </ol>
</blockquote>

<p>梯度下降法的注意事项可以参考<a href="/2015/01/linear-regression/#gd-method">线性回归梯度法</a>的注意事项。</p>

<h2 id="section-1">参考文献</h2>

<ol class="bibliography"><li><span id="lin_ml_logistic_regression_2014">[1]H.-T. Lin, “Lecture 10: Logistic Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_lr_2014">[2]A. Ng, “Logistic Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-2">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:logistic-function-properties">
      <p>logistic函数的性质：（1）$\theta(-s)=1-\theta(s)$；（2）${d\theta(s)\over ds}=\theta(s)(1-\theta(s))$（${d\theta(s)\over d^Ns}=\theta(s)\prod_{n=0}^N(1-N\theta(s))$<strong>????</strong>）；（3）${\theta(s)\over 1-\theta(s)}=e^s$。 <a href="#fnref:logistic-function-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-class-label">
      <p>Andrew NG课程中采用了$\{0,1\}$类别标签<a href="#ng_ml_lr_2014">[2]</a>，因此得到的公式与本文形式不同。 <a href="#fnref:andrew-class-label" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-large-probability">
      <p><del>为什么$f$产生这笔数据的概率很大？</del>期望找到最合适的$h$，使产生数据集$\mathcal D$的可能性最大。 <a href="#fnref:why-large-probability" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>分类器融合（2）：AdaBoost</title>
      <link href="http://qianjiye.de/2015/01/adaptive-boosting" />
      <pubdate>2015-01-17T13:02:33+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/adaptive-boosting</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">基于样本加权的误差度量</h2>

<p>bootstrapping重采样数据集$\mathcal D=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),(\mathbf x_3,y_3),(\mathbf x_4,y_4)\}$，可能得到$\tilde{\mathcal D}_t=\{(\mathbf x_1,y_1),(\mathbf x_1,y_1),(\mathbf x_2,y_2),(\mathbf x_4,y_4)\}$，那么$\tilde{\mathcal D}_t$上的in-sample误差是$E_{in}^{0/1}(h)={1\over 4}\sum_{(\mathbf x,y)\in\tilde{\mathcal D}_t}[[y\neq h(\mathbf x)]]$，令$\mathbf u^{(t)}=[2,1,0,1]^T$，该误差也可以直接用$\mathcal D$上的加权误差$E_{in}$表示，$E_{in}^{\mathbf u^{(t)}}(h)={1\over 4}\sum_{n=1}^4u_n^{(t)}\cdot[[y_n\neq h(\mathbf x_n)]]$。这就是bagging通过最小化bootstrap-weighted误差得到不同$g_t$的方法。</p>

<p>通常需要最小化的加权误差为
\[
E_{in}^{\mathbf u}(h)={1\over N}\sum_{n=1}^Nu_n\cdot err(y_n, h(\mathbf x_n))，
\]
把$\mathbf u$放回算法并不困难。对于SVM，利用对偶QP最小化误差$E_{in}^{\mathbf u}\varpropto C\sum_{n=1}^Nu_n\widehat{err}_{SVM}$，可以通过调整原方法的上界为$0\leq \alpha_n\leq Cu_n$来实现；对于logistic回归，利用SGD最小化误差$E_{in}^{\mathbf u}\varpropto C\sum_{n=1}^Nu_n\widehat{err}_{CE}$，可以通过按不同倍率$u_n$的概率采样$(\mathbf x_n,y_n)$来实现。</p>

<p>这里是基于不同样本点加权的误差度量方式，与<a href="/2014/12/machine-learning-noise-and-error/#class-weighted-error">基于不同类别加权的误差度量方式</a>的加权对象不同。如何将$\mathbf u$放回原算法是这类加权算法要处理的重要问题。</p>

<h2 id="section-1">权重调整策略</h2>

<p>如果算法会根据$\mathbf u$决定$g$，那么怎样改变$\mathbf u$使得$g$越不一样越好？越不一样的$g$，通过聚合（aggregation）机制，越有可能得到更好的结果。</p>

<p>通过$u_n^{(t)}$得到$g_t$，$u_n^{(t+1)}$得到$g_{t+1}$，
\[
\left\{
\begin{aligned}
g_t&amp;\leftarrow\arg\min_{h\in\mathcal H}\left(\sum_{n=1}^Nu_n^{(t)}[[y_n\neq h(\mathbf x_n)]]\right)\\
g_{t+1}&amp;\leftarrow\arg\min_{h\in\mathcal H}\left(\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq h(\mathbf x_n)]]\right)。
\end{aligned}
\right.
\]
如果先选定$g_t$（当作$h$），调整权重$u_n^{(t+1)}$使得$g_t$效果非常差，$g_t$以及与$g_t$相似的假设都不会被当作$g_{t+1}$，这样就能选择到一个与$g_t$很不一样的$g_{t+1}$。这就是获得不一样$g$的基本思想。理想的情况就是构造$\mathbf u_n^{(t+1)}$，使得$g_t$的表现就像随机猜想一样
\[
\frac{\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t+1)}}={1\over 2}，
\]
也就是期望
\[
\frac{\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t+1)}}=\frac{\clubsuit_{t+1}}{\clubsuit_{t+1}+\spadesuit_{t+1}}={1\over 2}，
\]
其中
\[
\clubsuit_{t+1}=\sum_{n=1}^Nu_n^{(t+1)}[[y_n\neq g_t(\mathbf x_n)]]\qquad\spadesuit_{t+1}=\sum_{n=1}^Nu_n^{(t+1)}[[y_n= g_t(\mathbf x_n)]]。
\]</p>

<p>假设犯错误的样本点有$1126$个，正确的样本点有$6211$个，对于错分的样本点就可以用$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot {6211\over 7337}$更新，对于正确分类的样本点就可以用$u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot {1126\over 7337}$更新。更新权重$\mathbf u^{(t+1)}$时，设错误率为
\begin{equation}
\epsilon_t=\frac{\sum_{n=1}^Nu_n^{(t)}[[y_n\neq g_t(\mathbf x_n)]]}{\sum_{n=1}^Nu_n^{(t)}}，
\label{eq:epsilon-t}
\end{equation}
错误的点原来的权重乘以系数$\varpropto(1-\epsilon_t)$，正确的点原来的权重乘以系数$\varpropto\epsilon_t$。</p>

<p>通常的做法是定义缩放因子
\begin{equation}
\blacklozenge_t=\sqrt{1-\epsilon_t\over\epsilon_t}，
\label{eq:blacklozenge-t}
\end{equation}
其中$\epsilon_t$按\eqref{eq:epsilon-t}计算，权重更新方法为
\[
\mbox{incorrect}\leftarrow\mbox{incorrect}\cdot\blacklozenge_t\qquad\mbox{correct}\leftarrow\mbox{correct }/\blacklozenge_t。
\]
当$\epsilon\leq{1\over 2}$时，$\blacklozenge_t\geq 1$，放大错误的作用，缩小正确的影响，更关注错分的样本。</p>

<h2 id="adaboost">AdaBoost</h2>

<p>AdaBoost<sup id="fnref:pi-jiang-method"><a href="#fn:pi-jiang-method" class="footnote">1</a></sup> ＝ 弱的基础学习算法$\mathcal A$（学生）＋最优的权重调整因子$\blacklozenge_t$（老师）＋神奇的线性聚合$\alpha_t$（班级集体智慧）。</p>

<blockquote>
  <h4 id="adaboostadaptive-boosting">AdaBoost（<em>ada</em>ptive <em>boost</em>ing）算法</h4>
  <hr />

  <p>首先，初始化$\mathbf u^{(1)}=\left[{1\over N},{1\over N},\ldots,{1\over N}\right]$；</p>

  <p>其次，对于$t=1,2,\ldots,T$执行以下步骤：</p>

  <ol>
    <li>利用$\mathcal A(\mathcal D, \mathbf u^{(t)})$得到$g_t$，其中$\mathcal A$最小化$\mathbf u^{(t)}$加权的0/1误差；</li>
    <li>将$\mathbf u^{(t)}$更新为$\mathbf u^{(t＋1)}$：
\[
u_n^{(t+1)}\leftarrow\left\{
\begin{aligned}
u_n^{(t)}\cdot\blacklozenge_t&amp;\quad\mbox{if }[[y_n\neq g_t(\mathbf x_n)]]\\
u_n^{(t)}/\blacklozenge_t&amp;\quad\mbox{if }[[y_n= g_t(\mathbf x_n)]]，
\end{aligned}
\right.
\]
其中$\blacklozenge_t$按\eqref{eq:blacklozenge-t}计算；</li>
    <li>计算系数$\alpha_t=\ln(\blacklozenge_t)$；</li>
  </ol>

  <p>最后，返回$G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T\alpha_tg_t(\mathbf x)\right)$。</p>
</blockquote>

<p>好的$g_t$应该有大的$\alpha_t$。对于$\epsilon_t={1\over 2}$，近似于随机猜想，$\alpha_t=0$；对于$\epsilon_t=0$，完全正确的分类器，$\alpha_t=\infty$。</p>

<p>AdaBoost的VC界是
\[
E_{out}(G)\leq E_{in}(G)+O\left(\sqrt{O\left(d_{VC}(\mathcal H)\cdot T\log T\right)\cdot{\log N\over N}}\right)，
\]
其中$d_{VC}(\mathcal H)$是为了$g_t$所要付出的代价。当$g_t$的性能优于随机猜想$\left(\epsilon_t\leq\epsilon&lt;{1\over 2}\right)$时，经过$T=O(\log N)$轮迭代就可以达到$E_{in}(G)=0$。由于总共的$d_{VC}=O\left(d_{VC}(\mathcal H)\cdot T\log T\right)$随$T$增长缓慢，不等式右边第二项也可以做到很小。</p>

<p>AdaBoost是一种提升算法（boosting）的实现，从boosting的角度，若$\mathcal A$略优于随机猜想$\left(\epsilon_t\leq\epsilon&lt;{1\over 2}\right)$，AdaBoost＋$\mathcal A$可以达到很强大的性能（$E_{in}(G)=0$，$E_{out}$很小）。</p>

<h2 id="adaboost-stump">AdaBoost-Stump</h2>

<p>对于一个decision stump分类器
\[
h_{s,i,\theta}(\mathbf x)=s\cdot\mbox{sign}(\mathbf x_i-\theta)，
\]
$i$表示特征维，$\theta$表示阈值，$s$控制方向，在2D空间该分类器就是水平或竖直线，该算法优化的时间复杂度为$O(d\cdot N\log N)$。decision stump能够高效的最小化$E_{in}^\mathbf u$，但是自身的性能却很弱。
将decision stump作为基础分类器，可以组合出功能强大的<strong>AdaBoost-Stump</strong>。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png"><img src="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png" alt="AdaBoost-Stump示例" /></a><div class="caption">Figure 1:  AdaBoost-Stump示例 [<a href="/assets/images/2015-01-17-adaptive-boosting-adaboost-stump.png">PNG</a>]</div></div></div>

<p>上图展示了基于decision stump构造的AdaBoost-Stump，当$t=5$时就能完美的分类。AdaBoost-Stump能够比核SVM更高效地得到非线性分类器。</p>

<p>世界上第一个实时人脸识别程序就是基于AdaBoost-Stump。从$24\times 24$规格的162336张候选图片中通过decision stump挑选关键图片，在此基础上进行线性聚合（linear aggregation）<sup id="fnref:how-to-do-AdaBoost-face"><a href="#fn:how-to-do-AdaBoost-face" class="footnote">2</a></sup>。为了提高速度，人脸识别采用的$G$会尽早排除非人脸。</p>

<p>在实际应用中，特征维数可能很高，AdaBoost-Stump能够有效的进行特征选择和聚合。上例是2维的低纬度情况，进行了5次迭代，没有特征选择的功能。</p>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-3">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:pi-jiang-method">
      <p>也可称为“皮匠法”，意为“三个臭皮匠，胜过诸葛亮”。 <a href="#fnref:pi-jiang-method" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-to-do-AdaBoost-face">
      <p>这里如何构造人脸识别程序的呢？（1）<a href="https://class.coursera.org/ntumltwo-001/forum/thread?thread_id=172">论坛讨论</a>；（2）<a href="http://en.wikipedia.org/wiki/Viola–Jones_object_detection_framework">Viola–Jones object detection framework</a>。 <a href="#fnref:how-to-do-AdaBoost-face" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>分类器融合（1）：混合与自助聚合</title>
      <link href="http://qianjiye.de/2015/01/blending-and-bagging" />
      <pubdate>2015-01-15T15:12:32+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/blending-and-bagging</guid>
      <content:encoded>&lt;![CDATA[<p><strong>混合</strong>（blending）是将不同的假设用均匀、线性或非线性的方式组合起来；如果先从boostrapped数据上学习到各种不同的假设，然后再混合，就称为<strong>自助聚合</strong>（bagging, bootstrap aggregating）。</p>

<h2 id="section">聚合法简介</h2>

<p>如何将特征和假设集组合起来，让机器学习的性能更好呢？</p>

<p>假设$T$个👬朋友$g_1,\ldots,g_T$给出参考意见$g_t(\mathbf x)$预测股票是否会涨，如何决策呢？</p>

<ol>
  <li>校验法（validation）：听从最懂股票那个朋友的意见，
\begin{equation*}
G(\mathbf x)=g_{t_*}(\mathbf x)，\qquad t_*=\arg\min_{t\in\{1,2,\ldots,T\}}E_{val}\left(g_t^-\right)，
\end{equation*}
$E_{val}\left(g_{t}^-\right)$表示$g^-$是在一个较小数据集上得到的结果，选择完成之后在全验证集上得到完整的$g$。</li>
  <li>投票法（vote）：一人一票的均匀投票，听从多数人的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T1\cdot g_t(\mathbf x)\right)。
\label{eq:uniform-blending-hypothesis}
\end{equation}</li>
  <li>加权投票法：每个人的票数不一样，听从多数票的意见，
\begin{equation}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T\alpha_t\cdot g_t(\mathbf x)\right),\qquad\alpha_t\geq 0。
\label{eq:linear-blending-hypothesis}
\end{equation}
当$\alpha_t=[[E_{val}\left(g_{t}^-\right)\mbox{ smallest}]]$时，与方法1一样；当$\alpha_t=1$时，与方法2一样。</li>
  <li>有条件的组合：比如科技股听从擅长这方面的朋友，传统行业股票听从那些……
\begin{equation*}
G(\mathbf x)=\mbox{sign}\left(\sum_{t=1}^T q_t(\mathbf x)\cdot g_t(\mathbf x)\right),\qquad q_t(\mathbf x)\geq 0，
\end{equation*}
当$q_t(\mathbf x)=\alpha_t$时，与方法3一样，也就是包含了前面所有情况。</li>
</ol>

<p><strong>聚合法</strong>的目的是综合多个假设（可能是比较弱的）让效果更好。上述方法中，2～4称为<strong>聚合模型</strong>（aggregation model）。</p>

<p>对于上述1的校验（validation）方法，如果用$E_{in}(g_t)$代替$E_{val}(g_t)$进行选择，最终可能会付出很大VC维的代价<sup id="fnref:why-large-dvc"><a href="#fn:why-large-dvc" class="footnote">1</a></sup>。这种方法需要一个强大优秀的$g_t^-$，否则也只是差中择优。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png"><img src="/assets/images/2015-01-15-blending-and-bagging-vote-method.png" alt="［左］：水平垂直线投票；［右］PLA投票" /></a><div class="caption">Figure 1:  ［左］：水平垂直线投票；［右］PLA投票 [<a href="/assets/images/2015-01-15-blending-and-bagging-vote-method.png">PNG</a>]</div></div></div>

<p>上图展示了利用聚合法的投票机制，灰色的判别界表示参与投票的分类器。上图左中，水平线将平面分割成了9块区域，如果按“垂直、水平左，水平右”的顺序投票，每个区域中的投票结果如图中绿色标注，最终结果相当于组合成了黑色的判别界。上图左展示的投票方法，相当于特征变换（feature transform）的效果<sup id="fnref:why-feateure-tansform"><a href="#fn:why-feateure-tansform" class="footnote">2</a></sup>；上图右展示了感知器的投票结果，相当于正则化的效果。通过上图可看出，合理的集合方法能提升性能。</p>

<h2 id="section-1">均匀混合</h2>

<p>均匀混合（uniform blending）也就是投票(voting)方法\eqref{eq:uniform-blending-hypothesis}。如果每个$g_t$都相同，结果等价于任意的$g_t$；如果每个$g_t$千差万别，就是少数服从多数。该方法可以直接推广到多类的情况，
\[
G(\mathbf x)=\arg\max_{1\leq k\leq K}\sum_{t=1}^T[[g_t(\mathbf x)=k]]。
\]</p>

<p>均匀混合解决回归问题的方法是
\[
G(\mathbf x) = {1\over T}\sum_{t=1}^Tg_t(\mathbf x)。
\]
如果每个$g_t$都相同，结果等价于任意的$g_t$；对于不同的$g_t$，可能得到比单一判别更精确的结果。</p>

<p>由此可见，对于多个不同的假设，即使采用简单的混合法则，也可以得到比单一假设更好的结果。</p>

<p>对于均匀混合的回归，
\[
\begin{aligned}
avg\left(\left(g_t(\mathbf x)-f(\mathbf x)\right)^2\right)
=&amp;avg\left(g_t^2-2g_t^2f+f^2\right)\\
=&amp;avg\left(g_t^2\right)-2Gf+f^2\\
=&amp;avg\left(g_t^2\right)-G^2+(G-f)^2\\
=&amp;avg\left(g_t^2\right)-2G^2 + G^2 +(G-f)^2\\
=&amp;avg\left(g_t^2-2g_tG + G^2\right) +(G-f)^2\\
=&amp;avg\left(\left(g_t-G\right)^2\right) +(G-f)^2。
\end{aligned}
\]</p>

<p>若对产生$\mathbf x$分布的所有点都进行上述运算，然后取期望可得
\[
avg\left(E_{out}\left(g_t\right)\right)=avg(\varepsilon(g_t-G)^2)+E_{out}(G)\geq E_{out}(G)，
\]
也就是说，均匀混合方法的效果会比选择其中一个好。</p>

<p>从$P^N$采集大小为$N$的$T$个数据集，利用上述公式，衡量演算法$\mathcal A$的表现。通过$\mathcal A(\mathcal D_t)$获得$g_t$，对其平均
\[
\bar g=\lim_{T\rightarrow\infty}G=\lim_{T\rightarrow\infty}{1\over T}\sum_{t=1}^Tg_t=\varepsilon_{\mathcal D}\mathcal A(\mathcal D)，
\]
算法$\mathcal A$的性能期望为
\begin{equation}
avg(E_{out}(g_t))=avg(\varepsilon(g_t-\bar g)^2)+E_{out}(\bar g)。
\label{eq:bias-variance-decomposition}
\end{equation}</p>

<p>上述公式中：</p>

<ul>
  <li>$avg(E_{out}(g_t))$：算法$\mathcal A$的期望性能；</li>
  <li>$E_{out}(\bar g)$：算法共识（consensus）的性能（$\bar g$就是从$\mathcal D_t\sim P^N$期望获得的$g_t$），通常称为<strong>bias</strong>；</li>
  <li>$avg(\varepsilon(g_t-\bar g)^2)$：偏离共识的期望，通常称为<strong>variance</strong>。</li>
</ul>

<p>通过bias和variance，将演算法的表现拆分为两部分。均匀混合通过减小variance获得更稳定的性能。</p>

<h2 id="section-2">线性混合</h2>

<p>通过\eqref{eq:linear-blending-hypothesis}，赋予不同假设不同的权重就是<strong>线性混合</strong>（linear blending）。</p>

<p>线性回归的线性混合目标为
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^N\left(y_n-\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)^2，
\]
将$g(\mathbf x)$视为特征变换$\phi(\mathbf x)$，换一种表达形式
\[
\min_{\mathbf w}{1\over N}\sum_{n=1}^{N}\left(y_n-\sum_{i=1}^{\tilde d}w_i\phi_i(\mathbf x_n)\right)^2，
\]
这就类似两阶（two-level）的学习方法。</p>

<p>线性混合＝线性模型＋假设（hypothesis）视为变换＋约束条件，
\[
\min_{\alpha_t\geq 0}{1\over N}\sum_{n=1}^Nerr\left(y_n,\sum_{t=1}^T\alpha_tg_t(\mathbf x_n)\right)。
\]
对于二分类问题
\[
\alpha_tg_t(\mathbf x)=|\alpha_t|(-g_t(\mathbf x))\qquad\mbox{if }\alpha_t&lt;0，
\]
正负对分类器本质上没有差别，实际上有“线性混合＝线性模型＋假设（hypothesis）视为变换”，不需要$\alpha_t$的约束条件。</p>

<p>在实际中，$g_t$通常是用$E_{in}$从各模型中选的最优，$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\dots,g_T\in\mathcal H_T$。如果在这些$g_t$中用$E_{in}$再选最优的，就是best of best，将付出高复杂度$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。如果在这些$g_t$中用$E_{in}$再采用线性混合，就是aggregation of best，将付出<strong>高于</strong>$d_{VC}=\left(\bigcup\limits_{t=1}^T\mathcal H_t\right)$的代价。实际上通常采用$E_{val}$替代$E_{in}$，通过最小化$E_{train}$得到$g_t^-$。</p>

<blockquote>
  <h4 id="section-3">线性混合算法</h4>
  <hr />

  <ol>
    <li>从$\mathcal D_{train}$中获取$g_1^-,g_2^-,\ldots,g_T^-$；     </li>
    <li>在$\mathcal D_{val}$中将$(\mathbf x_n,y_n)$转换为$(\mathbf z_n=\phi^-(\mathbf x_n),y_n)$，其中$\phi^-(\mathbf x)＝(g_1^-(\mathbf x),g_2^-(\mathbf x),\ldots,g_T^-(\mathbf x))$；</li>
    <li>计算$\boldsymbol\alpha=\mbox{LinearModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
    <li>返回 $G_{LINB}(\mathbf x)=\mbox{LinearHypothesis}_\boldsymbol\alpha(\phi(\mathbf x))$，其中$\phi(\mathbf x)＝(g_1(\mathbf x),g_2(\mathbf x),\ldots,g_T(\mathbf x))$<sup id="fnref:how-gt-gt-x"><a href="#fn:how-gt-gt-x" class="footnote">3</a></sup>。</li>
  </ol>

</blockquote>

<p><strong>如果将3、4两步换为：</strong></p>

<ul>
  <li>计算$\tilde g=\mbox{AnyModel}\left(\{(\mathbf z_n, y_n)\}\right)$；</li>
  <li>返回$G_{ANYB}(\mathbf x)=\tilde g(\phi(\mathbf x))$。</li>
</ul>

<p>这就是any blending（stacking）的方法。any blending方法强大，可以实现conditional blending，但是也存在过拟合的风险。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png"><img src="/assets/images/2015-01-15-blending-and-bagging-blending-example.png" alt="混合方法使用实例" /></a><div class="caption">Figure 2:  混合方法使用实例 [<a href="/assets/images/2015-01-15-blending-and-bagging-blending-example.png">PNG</a>]</div></div></div>

<p>上图的流程中，Val.-Set Blending就是any blending的方法，使得$E_{test}$降低到$456.24$。然后再将这一步得到的上百个$g，G$用近似的$\tilde E_{test}$（$\tilde E_{test}$好可使真正的$E_{test}$更好）进行linear blending。</p>

<p>混合（blending）在实际中很有用，但是模型计算复杂度增大了。</p>

<h2 id="section-4">⾃助聚合</h2>

<p>混合（blending）方法的策略是先学习到$g_t$，然后再聚合（aggregate）。<del>能否在学习$g_t$的同时进行聚合呢？✅</del></p>

<p>对于均匀聚合（uniform aggregation），不同模型参与聚合是关键，获取不同模型的方法包括：</p>

<ul>
  <li>从不同假设集获取模型：$g_1\in\mathcal H_1,g_2\in\mathcal H_2,\ldots,g_T\in\mathcal H_T$；</li>
  <li>采用不同的参数：对于梯度下降法，$\eta=0.001,0.01,\ldots,10$；</li>
  <li>利用算法的随机性：从不同的初始化开始PLA；</li>
  <li>利用数据的随机性：交叉检验采用不同数据集验证$g_v^-$<sup id="fnref:how-to-gv-x"><a href="#fn:how-to-gv-x" class="footnote">4</a></sup>。</li>
</ul>

<p>回顾\eqref{eq:bias-variance-decomposition}，算法的性能被拆分为bias和variance两部分进行评估。共识的结果会比$\mathcal A(\mathcal D)$的单一$g$效果好，但是每次都需要用不同的$\mathcal D_t$获得$g_t$。</p>

<p>能否通过有限的$T$和单一的数据集$\mathcal D$得近似的$\bar g$？✅</p>

<p><strong>bootstrapping</strong>是一种通过重采样（re-sample）$\mathcal D$模拟$\mathcal D_t$的统计学工具。bootstrap得到$\tilde{\mathcal D}_t$的方法：从$\mathcal D$中随机抽取一个点，纪录该点后然后放回，重复该过程直到抽取到$N’$个数据。</p>

<blockquote>
  <h4 id="section-5">自助聚合算法</h4>
  <hr />

  <ol>
    <li>利用bootstrapping技术得到$N’$点的数据集$\tilde{\mathcal D}_t$；</li>
    <li>利用$\mathcal A(\tilde{\mathcal D}_t)$，算法$\mathcal A$在数据集$\tilde{\mathcal D}_t$上得到$g_t$；</li>
    <li>$G=\mbox{Uniform}(g_t)$。</li>
  </ol>
</blockquote>

<p><strong>⾃助聚合</strong>（<strong>b</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>）也称为<strong>打包</strong>（bagging）。像⾃助聚合这种，建立在其它基础算法（base algorithm）$\mathcal A$之上的算法称为meta algorithm。</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png"><img src="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png" alt="bagging pocket方法" /></a><div class="caption">Figure 3:  bagging pocket方法 [<a href="/assets/images/2015-01-15-blending-and-bagging-pla-bagging.png">PNG</a>]</div></div></div>

<p>上图是bagging pocket方法的效果，通过bagging得到各不相同的$g_t$，然后通过聚合得到合适的非线性分类器。</p>

<p>如果基础算法（base algorithm）对数据的随机性很敏感，bagging可以工作得相当好。</p>

<h2 id="section-6">参考资料</h2>

<ol class="bibliography"></ol>

<h3 id="section-7">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:why-large-dvc">
      <p>为什么会付出很大VC维的代价？ <a href="#fnref:why-large-dvc" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-feateure-tansform">
      <p>为什么相当于特征变换的效果？ <a href="#fnref:why-feateure-tansform" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-gt-gt-x">
      <p>$g_t^-(\mathbf x)$和$g_t(\mathbf x)$具体如何计算？ <a href="#fnref:how-gt-gt-x" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:how-to-gv-x">
      <p>这步如何操作？「機器學習基石」第十五講有關。 <a href="#fnref:how-to-gv-x" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>线性回归</title>
      <link href="http://qianjiye.de/2015/01/linear-regression" />
      <pubdate>2015-01-13T20:17:01+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/linear-regression</guid>
      <content:encoded>&lt;![CDATA[<p><img src="/assets/images/2014-10-19-linear_regression_0.png" alt="线性回归" /></p>

<p>本节的主要参考资料是机器学习基石<a href="#lin_mlf_linreg_2014">[1]</a>和机器学习<a href="#ng_ml_linreg_2014">[2]</a>网络课程。</p>

<h2 id="section">线性回归模型</h2>

<p>线性回归的假设集（hypothesis）为
\begin{equation}
h(\mathbf x) = \mathbf w^T\mathbf x，
\label{eq:linear-regression-model}
\end{equation}
$\mathbf x$是含有常数项$x_0 = 1$的$d+1$维向量$\mathbf x =\left[1,x_1,\ldots,x_d\right]^T$，$\mathbf w=\left[w_0,w_1,\ldots,w_d\right]^T$，$d$是特征维数。</p>

<p>目标代价函数采用平方误差和估计
\begin{equation}
E_{in}(\mathbf w)={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n-y_n\right)^2，
\label{eq:cost-function-linear-regression}
\end{equation}
$N$是样本数。通过机器学习算法求得线性回归的参数
\begin{equation}
\mathbf w_{LIN} = \arg\min_{\mathbf w}E_{in}(\mathbf w)，
\end{equation}
通常方法有正规方程（normal equation）的解析方法和梯度下降法（gradient descent）。</p>

<h2 id="section-1">解析方法</h2>

<p>解析方法得到的最优解也称为<strong>线性回归的最小二乘解</strong>。</p>

<p>代价函数改写为矩阵形式
\[
E_{in}(\mathbf w) 
={1\over N}\lVert\mathbf X\mathbf w-\mathbf y\rVert^2
={1\over N}\left(\mathbf w^T\mathbf X^T\mathbf X\mathbf w-2\mathbf w^T\mathbf X^T\mathbf y+\mathbf y^T\mathbf y\right)，
\]
$\mathbf X$是样本数据矩阵，每行代表一个样本点，每列代表一个特征，第一列的向量$\mathbf 1_N$对应常数偏移。$E_{in}(\mathbf w)$是连续可微的凸函数，当$\nabla E_{in}(\mathbf w)=0$时取得最小值，
\[
\nabla E_{in}(\mathbf w)={2\over N}\left(\mathbf X^T\mathbf X\mathbf w-\mathbf X^T\mathbf y\right)，
\]
取得最小值时
\begin{equation}
\mathbf w_{LIN} = \left(\mathbf X^T\mathbf X\right)^{-1}\mathbf X^T\mathbf y = \mathbf X^\dagger \mathbf y，
\end{equation}
$\mathbf X^\dagger$称为<strong>伪逆</strong>（pseudo-inverse）。通常情况$N\gg d+1$，因此$\left(\mathbf X^T\mathbf X\right)^{-1}$通常都可逆；如果不可逆，解不唯一。</p>

<p>导致$\mathbf X^T\mathbf X$不可逆的原因可能是冗余特征（redundant features）或者特征数目过多（$d$太大而$N$太少），解决的办法：   </p>

<ul>
  <li>对于冗余的线性相关特征，例如$x_1 = 2x_2$，删除线性相关特征；</li>
  <li>对于特征数目过多，例如$N&lt;d$，删除特征或正则化（regularization）<sup id="fnref:how-to-regularize"><a href="#fn:how-to-regularize" class="footnote">1</a></sup>。</li>
</ul>

<p>Matlab的<code>pinv</code>函数可以处理$\mathbf X^T\mathbf X$不可逆的情况<sup id="fnref:pinv-vs-inv"><a href="#fn:pinv-vs-inv" class="footnote">2</a></sup>。</p>

<h3 id="mathbf-wlin">解析方法求解$\mathbf w_{LIN}$是机器学习算法吗？</h3>

<p>✅通过VC维的角度分析，能得到小的$E_{out}\left(\mathbf w_{\small{LIN}}\right)$，就是学习……</p>

<ul>
  <li>能够得到最佳的$E_{in}$；</li>
  <li>$d+1$个变量，有限的$d_{VC}$，因此有好的$E_{out}$；</li>
  <li>事实上，求解伪逆的过程也是迭代逐步最优的过程（高斯消元法）。</li>
</ul>

<p>VC维考察的是个别的$E_{in}$<sup id="fnref:some-E-in"><a href="#fn:some-E-in" class="footnote">3</a></sup>，从$E_{in}$平均误差角度分析
\begin{equation}
\bar E_{in}
=\varepsilon_{\mathcal D\sim P^N}\left\{E_{in}\left(\mathbf w_{LIN}\mbox{ w.r.t }\mathcal D\right)\right\}
=\mbox{noise level}\cdot\left(1-{d+1\over N}\right)，
\label{eq:noise-level-e-in}
\end{equation}
$\mbox{noise level}$表示数据中的噪声，${d+1\over N}$表示比噪声小的比率，数据越多二者差别越小。</p>

<p>$E_{in}\left(\mathbf w_{LIN}\right)$计算方法为
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert\mathbf y-\mathbf X\mathbf X^\dagger\mathbf y\right\rVert^2
={1\over N}\left\lVert\left(\mathbf I-\mathbf X\mathbf X^\dagger\right)\mathbf y\right\rVert^2，
\]
$\mathbf X\mathbf X^\dagger$让$\mathbf y$加帽$\wedge$变成了$\hat{\mathbf y}$，也叫<strong>帽矩阵</strong><sup id="fnref:hat-matrix-properties"><a href="#fn:hat-matrix-properties" class="footnote">4</a></sup>（hat matrix），记为$\mathbf H$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-learning-curve.png"><img src="/assets/images/2015-01-13-linear-regression-learning-curve.png" alt="［左］：图解证明；［右］：学习曲线" /></a><div class="caption">Figure 1:  ［左］：图解证明；［右］：学习曲线 [<a href="/assets/images/2015-01-13-linear-regression-learning-curve.png">PNG</a>]</div></div></div>

<p>由$\hat{\mathbf y}=\mathbf X\mathbf w_{LIN}$可知，$\hat{\mathbf y}$是$\mathbf X$列向量的线性组合，也就是如上图左所示，$\hat{\mathbf y}$位于$\mathbf X$张成的线性空间中。当$\mathbf y-\hat{\mathbf y}$垂直于该生成空间时，$\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2$的值最小。$\mathbf H$将$\mathbf y$投影为$\hat{\mathbf y}$，$\mathbf I-\mathbf H$将$\mathbf y$投影为$\mathbf y-\hat{\mathbf y}$。$\mathbf I-\mathbf H$的迹为$trace(\mathbf I-\mathbf H)=N-(d+1)$，表示自由度从$N$降到$N-(d+1)$。</p>

<p>观测到的数据$\mathbf y$是理想的数据空间$f\left(\mathbf X\right)$叠加一些噪声。$\mathbf y-\hat{\mathbf y}$也可以从噪声投影得到，如上图左所示，
\[
E_{in}\left(\mathbf w_{LIN}\right)
={1\over N}\left\lVert\mathbf y-\hat{\mathbf y}\right\rVert^2
={1\over N}\left\lVert(\mathbf I-\mathbf H)\cdot\mbox{noise}\right\rVert^2
={1\over N}(N-(d+1))\lVert\mbox{noise}\rVert^2，
\]
因此可得公式\eqref{eq:noise-level-e-in}的结论。$E_{out}$的证明过程叫复杂，仍然可以得到
\[
\bar E_{out}
=\mbox{noise level}\cdot\left(1+{d+1\over N}\right)。
\]</p>

<p>$\mbox{noise level}$可以用$\sigma^2$表示，从上图右可见，$\bar E_{in}$和$\bar E_{out}$在$N\rightarrow\infty$时都会趋近于$\sigma^2$。期望的泛化误差（generalization error）可以用${2(d+1)\over N}$衡量，这里是平均情况，VC维衡量的是最坏的情况。</p>

<h2 id="gd-method">梯度下降法</h2>

<p>梯度下降法就是沿梯度下降方向更新参数，也就是对每个特征的权值$w_i$，不断迭代执行更新
\begin{equation}
w_i := w_i-\alpha{\partial E_{in}(\mathbf w)\over\partial w_i}\quad(i=0,1,\ldots,d)
\end{equation}
直至收敛，其中$\alpha$表示学习速率，梯度计算公式为
\[
{\partial E_{in}(\mathbf w)\over\partial w_i}
={1\over N}\sum_{n=1}^N\left(\mathbf w^T\mathbf x_n - y_n\right)x_{n,i}。
\]</p>

<p>参数须同时更新，也就是当每个$w_i$都更新完成后，才能用新的$\mathbf w$计算$E_{in}(\mathbf w)$，具体可参考<a href="/assets/images/2014-10-19-linear_regression_1.png">Andrew NG的讲义</a><sup id="fnref:andrew-simultaneous-update"><a href="#fn:andrew-simultaneous-update" class="footnote">5</a></sup>。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-curve.png"><img src="/assets/images/2015-01-13-linear-regression-error-curve.png" alt="［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；&lt;br/&gt;［右］：梯度下降路径" /></a><div class="caption">Figure 2:  ［左1］：未归一化梯度下降路径；［左2］：归一化梯度下降路径；<br />［右］：梯度下降路径 [<a href="/assets/images/2015-01-13-linear-regression-error-curve.png">PNG</a>]</div></div></div>

<p>梯度下降法需要将所有特征归一（feature scaling）到统一的尺度（不用归一化$x_0$），比如$-1\le x_i\le 1$，
\[
\hat{x}_i = \frac{x_i - x_{mean}}{x_{max}-x_{min}}
\qquad\mbox{or}\qquad
\hat{x}_i = \frac{x_i - x_{mean}}{x_{std}}，
\]
这样有助于提高梯度下降法的速度，如上图左2所示。</p>

<h4 id="alpha">学习率$\alpha$的注意事项：</h4>
<ol>
  <li>在迭代过程中不需调节$\alpha$大小，由于梯度会不断减小，<a href="/2015/01/logistic-regression/#why-fixed-eta">在固定$\alpha$的情况下梯度下降步长也会自动减小</a>，如上图右所示；</li>
  <li>$\alpha$太小收敛慢，太大可能错过极值点而不收敛，甚至可能导致$E_{in}(\mathbf w)$不降反升。</li>
</ol>

<p>线性回归的代价函数$E_{in}(\mathbf w)$不存在局部极值（local optima），极小值就是全局极值<sup id="fnref:no-local-optima"><a href="#fn:no-local-optima" class="footnote">6</a></sup>。</p>

<h2 id="section-2">两种方法对比</h2>

<table>
  <thead>
    <tr>
      <th>梯度下降法</th>
      <th>解析解法</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>需要$\alpha$</td>
      <td>不需要$\alpha$</td>
    </tr>
    <tr>
      <td>迭代实现，可实现在线增量学习</td>
      <td>不需要迭代</td>
    </tr>
    <tr>
      <td>当特征数$d$很大时（$10^6$）工作良好</td>
      <td>$d$很大时很慢</td>
    </tr>
    <tr>
      <td>特征需要尺度规范化</td>
      <td>特征不需要尺度规范化<sup id="fnref:why-not-scale"><a href="#fn:why-not-scale" class="footnote">7</a></sup></td>
    </tr>
  </tbody>
</table>

<h2 id="section-3">分类问题</h2>

<p>线性分类器和线性回归的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>指标</th>
      <th>线性分类器</th>
      <th>线性回归</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathcal Y$</td>
      <td>$\{-1,+1\}$</td>
      <td>$\mathbb R$</td>
    </tr>
    <tr>
      <td>$h(\mathbf x)$</td>
      <td>$\mbox{sign}\left(\mathbf w^T\mathbf x\right)$</td>
      <td>$\mathbf w^T\mathbf x$</td>
    </tr>
    <tr>
      <td>$err(\hat{y},y)$</td>
      <td>$[[\hat{y}\neq y]]$</td>
      <td>$(\hat{y}-y)^2$</td>
    </tr>
    <tr>
      <td>算法复杂度</td>
      <td>通常是NP-hard</td>
      <td>高效求解方法</td>
    </tr>
  </tbody>
</table>

<p>能否利用线性回归的高效，借助$g(\mathbf x)=\mbox{sign}\left(\mathbf w_{LIN}^T\mathbf x\right)$，用线性回归解决分类问题？✅</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-13-linear-regression-error-compare.png"><img src="/assets/images/2015-01-13-linear-regression-error-compare.png" alt="线性分类器和线性回归误差比较" /></a><div class="caption">Figure 3:  线性分类器和线性回归误差比较 [<a href="/assets/images/2015-01-13-linear-regression-error-compare.png">PNG</a>]</div></div></div>

<p>上图展示了两种方法误差的对比，$err_{0/1}\leq err_{sqr}$，平方误差是0/1误差的上限。从VC维的理论可知
\[
\mbox{classification }E_{out}(\mathbf w)
\leq \mbox{classification }E_{in}(\mathbf w)+\sqrt{\cdots}
\leq \mbox{regression }E_{in}(\mathbf w)+\sqrt{\cdots}，
\]
in-sample回归误差也是out-sample分类误差的上限，做好in-sample回归误差也是做好in-sample分类误差的一种方法，in-sample回归误差很小时能保证out-sample分类误差也很小。由此可见，可用线性回归解决分类问题。</p>

<p>也可直接将回归问题视为分类问题，只是用$err_{sqr}$当作$\widehat{err}$作为$err_{0/1}$误差的近似。为分类问题选择稍宽松的误差上界，这样容易求解参数。</p>

<p>在很多时候，用线性回归解决分类问题效果尚可。如果要让效果更好，可将$\mathbf w_{LIN}$当做PLA或pocket算法的初始值$\mathbf w_0$，加速PLA或pocket算法。</p>

<h2 id="section-4">多项式回归</h2>

<p>构造多项式特征，利用线性回归模型解决非线性问题，称为<strong>多项式回归</strong>（polynomial regression）。例如利用$x_1 = x, x_2 = x^2, x_3 = x^3, \ldots $，构造新的特征向量$\mathbf x$，带入线性回归模型\eqref{eq:linear-regression-model}求解。从另一个角度看，当特征是多项式时，可直接利用线性模型求解。</p>

<p>当对特征进行高次多项式变换后，取值范围可能急剧变化，需要对多项式特征进行尺度归一化处理<a href="#ng_ml_rlrbv_pe_2014">[3, P. 8]</a>。</p>

<h2 id="section-5">参考资料</h2>

<ol class="bibliography"><li><span id="lin_mlf_linreg_2014">[1]H.-T. Lin, “Lecture 9: Linear Regression.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ntumlone">Online</a>]

</li>
<li><span id="ng_ml_linreg_2014">[2]A. Ng, “Linear Regression with multiple variables.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_rlrbv_pe_2014">[3]A. Ng, “Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-6">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:how-to-regularize">
      <p>如何进行正则化？ <a href="#fnref:how-to-regularize" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pinv-vs-inv">
      <p>Matlab的<code>pinv</code>和<code>inv</code>有何区别？ <a href="#fnref:pinv-vs-inv" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:some-E-in">
      <p>为什么VC维考察的是个别的$E_{in}$？ <a href="#fnref:some-E-in" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:hat-matrix-properties">
      <p>帽矩阵的性质（可从文中图示的角度理解）：（1）$\mathbf H$是对称的；（2）$\mathbf H^2=\mathbf H$；（3）$(\mathbf I-\mathbf H)^2=\mathbf I-\mathbf H$。 <a href="#fnref:hat-matrix-properties" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:andrew-simultaneous-update">
      <p>事实上，不同时更新的情况很少发生，因为在更新每个$w_i$前，已经用$\mathbf w$计算过了$E_{in}(\mathbf w)$，更新过程中，不再需要重复计算$E_{in}(\mathbf w)$。 <a href="#fnref:andrew-simultaneous-update" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:no-local-optima">
      <p>这是真的吗？ <a href="#fnref:no-local-optima" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-not-scale">
      <p>为什么解析方法不需要规范化特征？ <a href="#fnref:why-not-scale" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（6）：支持向量回归</title>
      <link href="http://qianjiye.de/2015/01/svm-support-vector-regression" />
      <pubdate>2015-01-12T18:31:45+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-support-vector-regression</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">脊回归</h2>

<p>有正则化项的回归称为<strong>脊回归</strong>（ridge regression）。脊回归的核模型有解析解么？</p>

<p>脊回归的最优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\left(y_n-\mathbf w^T\mathbf z_n\right)^2\right)，
\]
根据表示定理可知，存在形如$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$的最优解，将其带入并表示为核形式
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\left(y_n-\sum_{n=1}^N\beta_mK(\mathbf x_n,\mathbf x_m)\right)\right)。
\end{equation}
<strong>脊回归的核模型</strong>就是利用表示定理将脊回归核化。目标函数写为矩阵的形式
\begin{equation}
E_{aug}(\boldsymbol\beta)={\lambda\over N}\boldsymbol\beta^T\mathbf K\boldsymbol\beta + {1\over N}\left(\boldsymbol\beta^T\mathbf K^T\mathbf K\boldsymbol\beta-2\boldsymbol\beta^T\mathbf K^T\mathbf y + \mathbf y^T\mathbf y\right)，
\end{equation}
无约束最优化问题可以通过
\[
\nabla E_{aug}(\boldsymbol\beta)={2\over N}\left(\lambda\mathbf K^T\mathbf I\boldsymbol\beta+\mathbf K^T\mathbf K\boldsymbol\beta-\mathbf K^T\right)={2\over N}\mathbf K^T\left((\lambda\mathbf I+\mathbf K)\boldsymbol\beta-\mathbf y\right)
\]
令$\nabla E_{aug}(\boldsymbol\beta)=0$求解，
\begin{equation}
\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y。
\end{equation}
根据Mercer条件可知$\mathbf K$半正定，并且$\lambda&gt;0$，因此矩阵总可逆。稠密矩阵求逆的时间复杂度为$O\left(N^3\right)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png" alt="［左］：脊回归的线性模型；［右］：脊回归的核模型" /></a><div class="caption">Figure 1:  ［左］：脊回归的线性模型；［右］：脊回归的核模型 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-ridge-regression-compare.png">PNG</a>]</div></div></div>

<p>上图中的蓝线是脊回归的效果。线性模型与核模型的选择是速度与效率的折中权衡，它们之间的对比如下表：</p>

<table>
  <thead>
    <tr>
      <th>线性模型</th>
      <th>核模型</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\mathbf w = (\lambda\mathbf I+\mathbf X^TX)^{-1}\mathbf X^T\mathbf y$</td>
      <td>$\boldsymbol\beta=(\lambda\mathbf I+\mathbf K)^{-1}\mathbf y$</td>
    </tr>
    <tr>
      <td>功能受限</td>
      <td>通过$K$实现强大的功能</td>
    </tr>
    <tr>
      <td>训练时间复杂度为$O\left(d^3+d^2N\right)$</td>
      <td>训练时间复杂度为$O\left(N^3\right)$</td>
    </tr>
    <tr>
      <td>预测时间复杂度为$O(d)$，当$N\gg d$时效率高</td>
      <td>预测时间复杂度为$O(N)$，当训练样本大时效率低</td>
    </tr>
  </tbody>
</table>

<p>当参数获得后，回归函数就可以用核表示为
\begin{equation}
g(\mathbf x)=\sum_{n=1}^N\beta_nK\left(\mathbf x_n, \mathbf x\right)。
\end{equation}</p>

<h2 id="section-1">最小二乘支持向量机</h2>

<p><strong>最小二乘支持向量机</strong>（LSSVM，least-squares SVM）就是将脊回归的核模型用于分类。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png" alt="最小二乘与soft-margin支持向量机的对比" /></a><div class="caption">Figure 2:  最小二乘与soft-margin支持向量机的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-svm-vs-lssvm.png">PNG</a>]</div></div></div>

<p>上图可以看出，最小二乘与soft-margin支持向量机的分类面很相似，但是LSSVM的支持向量要多得多，预测速度会很慢。</p>

<p>LSSVM和logistic回归的核模型得到的参数$\boldsymbol\beta$是稠密的，标准支持向量机的参数$\boldsymbol\alpha$是稀疏的。能否得到像标准支持向量机一样稀疏的$\boldsymbol\beta$呢？</p>

<h2 id="tube">tube回归</h2>

<p>定义tube回归的误差为
\begin{equation}
err(y, s) = \max(0, \lvert s-y\rvert-\epsilon)，
\end{equation}
在tube区域内不计误差，在该区域外到tube的距离记为误差。</p>

<div class="image_line" id="tube-error-illustration"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png" alt="tube与平方误差对比" /></a><div class="caption">Figure 3:  tube与平方误差对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-tube-vs-square-error.png">PNG</a>]</div></div></div>

<p>如上图所示，tube与平方误差很相似，尤其是在$\lvert s-y\rvert$很小的区域内，当$\lvert s-y\rvert$很大时，tube误差不如平方误差变化陡峭，因此受噪声影响更小。</p>

<p>基于tube误差的模型能否得到稀疏的系数呢？</p>

<h2 id="section-2">支持向量回归</h2>

<p>基于$L_2$正则化的tube回归模型为
\begin{equation*}
\min\limits_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽无约束，但$\max$导致不可微；利用表示定理可核化，但无法明确得到稀疏的系数。仿照<a href="/2015/01/svm-kernel-logistic-regression/#mjx-eqn-equniform-soft-margin-svm">无约束形式</a>的soft-margin支持向量机，分离出$b$后改写为
\begin{equation*}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w+C\sum_{n=1}^N\max\left(0, \left\lvert \mathbf w^T\mathbf z_n+b-y_n\right\rvert-\epsilon\right)\right)，
\end{equation*}
虽然不可微，但是QP问题；对偶问题可核化，KKT条件能得到系数稀疏。再对比<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">约束形式</a>的soft-margin支持向量机，改写为带约束的优化问题
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad \left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert\leq \epsilon+\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\end{equation*}
约束条件线性化
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi^\vee,\boldsymbol\xi^\wedge}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left(\xi_n^\vee+\xi_n^\wedge\right)\\
\mbox{s.t.}&amp;\quad -\epsilon-\xi_n^\vee\leq y_n - \mathbf w^T\mathbf z_n-b\leq \epsilon+\xi_n^\wedge\\
&amp;\quad\xi_n^\vee\geq 0,\xi_n^\wedge\geq 0\mbox{ for all }n，
\end{aligned}
\end{equation}
这就是标准的<strong>支持向量回归</strong>（SVR，support vector regression）原问题，$\xi_n^\vee$和$\xi_n^\wedge$分别记录tube下届和上界违规，如<a href="#tube-error-illustration">上图左</a>所示的分界线下边和上边标红的线段。通过$C$对正则化和tube违规进行折中，调节参数$\epsilon$可控制tube的高度。该QP模型有$\tilde d+1+2N$个变量，$2N+2N$个约束条件。</p>

<p>将支持向量回归的原问题转成对偶问题，可移除对$\tilde d$的依赖。</p>

<h2 id="section-3">支持向量回归的对偶模型</h2>

<p>通过拉格朗日乘子法的KKT条件${\partial\mathcal L\over\partial\mathbf w}=0$可得
\begin{equation}
\mathbf w = \sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)\mathbf z_n = \sum_{n=1}^N\beta_n\mathbf z_n，
\end{equation}
通过${\partial\mathcal L\over\partial b}=0$可得
\[
\sum_{n=1}^N\left(\alpha_n^\wedge-\alpha_n^\vee\right)＝0，
\]
互补松弛条件（complementary slackness）为
\begin{equation}
\left\{
\begin{aligned}
\alpha_n^\wedge\left(\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\right)=&amp;0\\
\alpha_n^\vee\left(\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\right)=&amp;0。
\end{aligned}
\right.
\label{eq:complementary-slackness-svm-regession}
\end{equation}</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png"><img src="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png" alt="QP原问题与对偶问题的对比" /></a><div class="caption">Figure 4:  QP原问题与对偶问题的对比 [<a href="/assets/images/2015-01-12-svm-support-vector-regression-primal-vs-dual-QP.png">PNG</a>]</div></div></div>

<p>上图左上和左下分别表示soft-margin支持向量机的原问题和对偶问题的QP模型；上图右上和右下分别表示支持向量回归的原问题和对偶问题的QP模型。上图中，相同颜色的符号展示了如何从原问题变化到对偶问题。</p>

<p>当数据点位于tube中有$\left\lvert\mathbf w^T\mathbf z_n+b-y_n\right\rvert&lt;\epsilon$，不计误差，$\xi_n^\wedge=\xi_n^\vee=0$，根据互补松弛条件\eqref{eq:complementary-slackness-svm-regession}可知
\[
\left\{
\begin{aligned}
\epsilon+\xi_n^\wedge-y_n+\mathbf w^T\mathbf z_n+b\neq &amp;0\\
\epsilon+\xi_n^\vee+y_n-\mathbf w^T\mathbf z_n-b\neq &amp;0，
\end{aligned}
\right.
\]
以及$\alpha_n^\wedge=\alpha_n^\vee＝0$，因此可得$\beta_n=0$。由此可知，支持向量回归问题中$\beta_n\neq 0$的支持向量刚好位于tube的边界上或在tube之外。</p>

<p>参考<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-complementary-slackness">soft-margin支持向量机</a>可得
\begin{equation}
b=
\left\{
\begin{aligned}
y_n-\mathbf w^T\mathbf z_n-\epsilon&amp;\quad(0&lt;\alpha_n^\wedge&lt;C)\\
y_n-\mathbf w^T\mathbf z_n+\epsilon&amp;\quad(0&lt;\alpha_n^\vee&lt;C)。
\end{aligned}
\right.
\end{equation}</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（5）：核logistic回归</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-logistic-regression" />
      <pubdate>2015-01-09T15:01:15+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-logistic-regression</guid>
      <content:encoded>&lt;![CDATA[<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png" alt="4种形式的支持向量机" /></a><div class="caption">Figure 1:  4种形式的支持向量机 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-SVMs.png">PNG</a>]</div></div></div>

<h2 id="section">支持向量机的正则化形式</h2>

<p>回顾<a href="/2015/01/svm-soft-margin-svm/#mjx-eqn-eqsoft-margin-primal-svm">soft-margin支持向量机</a>，当违反边界的时候$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n + b\right)$，当没有违反边界的时候$\xi_n = 0$，边界违法的情况可以统一定义为$\xi_n=\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)$，于是无约束形式的soft-margin支持向量机为
\begin{equation}
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\max\left(1-y_n\left(\mathbf w^T\mathbf z_n + b\right), 0\right)\right)，
\label{eq:uniform-soft-margin-svm}
\end{equation}
可以简写为
\[
\min\limits_{b,\mathbf w}\left({1\over 2}\mathbf w^T\mathbf w + C\sum\widehat{\mbox{err}}\right)。
\]
这是目标函数的正则化形式表示方法，soft-margin可以看作一种特殊的误差$\widehat{\mbox {err}}$度量。$L_2$正则化是对$\mathbf w$长度的约束
\[
\min\limits_{b,\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w + {1\over N}\sum\mbox{err}\right)。
\]</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png" alt="SVM与正则化" /></a><div class="caption">Figure 2:  SVM与正则化 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-regularized-model.png">PNG</a>]</div></div></div>

<p>加入了大分类的边界限制，可以得到更少的分类情况，也可以通过$L_2$正则化实现。
从上图对比正则化方法可知，支持向量机可以看作为特殊的正则化方法。大的$C$，对应于小的$\lambda$，更弱的正则化。</p>

<h2 id="section-1">误差度量</h2>

<p>令线性项输出$s=\mathbf w^T\mathbf z_n + b$，感知器算法、支持向量机和logistic回归的误差度量为
\begin{equation}
\left\{
\begin{aligned}
err_{0/1}(s, y)&amp;=[[\mbox{sign}(ys)\neq 1]]\\
\widehat{err}_{SVM}(s, y)&amp;=\max(1-ys, 0)\\
err_{SCE}(s, y)&amp;=\log(1+\exp(-ys))。
\end{aligned}
\right.
\end{equation}</p>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png"><img src="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png" alt="误差度量比较" /></a><div class="caption">Figure 3:  误差度量比较 [<a href="/assets/images/2015-01-09-svm-kernel-logistic-regression-error_compare.png">PNG</a>]</div></div></div>

<p>几种误差曲线对比如上图所示。其中，支持向量机的这种误差度量方式通常称为hinge error measure。支持向量机和logistic回归的误差是0/1误差的上界，对于分类问题，通过上界的最小化，间接做好0/1误差的最优化。从误差曲线还可以看出，支持向量机和$L_2$正则化的logistic的误差度量非常相似。这几种分类器的比较如下：</p>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>优化方法</th>
      <th>优势</th>
      <th>劣势</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>感知器算法</td>
      <td>最小化$err_{0/1}$</td>
      <td>线性可分时效率较高</td>
      <td>只适用线性可分，否则采用pocket算法</td>
    </tr>
    <tr>
      <td>soft-margin支持向量机</td>
      <td>QP最小化误差$err_{0/1}$</td>
      <td>容易优化，理论基础较好</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
    <tr>
      <td>正则化logistic回归</td>
      <td>GD／SGD最小化误差$err_{SCE}$</td>
      <td>容易优化，正则化控制模型</td>
      <td>对于负值很大的误差，是$err_{0/1}$很松的上界</td>
    </tr>
  </tbody>
</table>

<p>从比较可以看出，logistic回归是soft-margin支持向量机的近似。</p>

<h2 id="section-2">支持向量机的概率模型</h2>

<p>如何让支持向量机输出$[0,1]$之间的概率？</p>

<p>一种思路是将支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$作为logistic回归中线性判别部分的参数
$
g(\mathbf x) = \theta\left(\mathbf w_{SVM}^T\mathbf x + b_{SVM}\right)
$，
直接利用支持向量机和logistic回归。这在实际应用中表现尚佳，但丧失了logistic回归的优良特性（比如maxmum likehood）。</p>

<p>另一种思路将向支持向量机的参数$\mathbf w_{SVM}$和$b_{SVM}$当作logistic的起始点，最终得到logistic回归模型。这和直接利用logistic结果差不多，还丧失了支持向量机核方法等优良特性。</p>

<p>如何融合支持向量机和logistic回归的优点？</p>

<p>组合logistic回归和支持向量机
\begin{equation}
g(\mathbf x) = \theta\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x) + b_{SVM}\right)+B\right)，
\label{eq:mixture-svm-logistic-model}
\end{equation}
这样既融合了支持向量机的核特性，又通过$A$和$B$两个自由度调节分类超平面适合最大似然。如果$A&gt;0$，表示支持向量机得到的$\mathbf w_{SVM}$较好；如果$B\approx 0$，表示支持向量机得到的$b_{SVM}$较好。新的logistic问题就变为了
\begin{equation}
\min_{A, B}{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\left(A\left(\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}\right)+B\right)\right)\right)，
\end{equation}
该模型可以分为2阶段，首先利用支持向量机得到1维特征，然后采用简单的logistic模型，这称为<strong>支持向量机的Platt概率模型</strong>：</p>

<ol>
  <li>先在数据集上$\mathcal D$上利用支持向量机得到模型参数$\mathbf w_{SVM}$和$b_{SVM}$（或者$\boldsymbol\alpha$），再进行变换$\mathbf z’_n=\mathbf w_{SVM}^T\mathbf \Phi(\mathbf x_n) + b_{SVM}$；</li>
  <li>在$\{\left(\mathbf z’_n, y_n\right)\}_{n=1}^N$上得到logistic模型的参数$A,B$；</li>
  <li>将公式\eqref{eq:mixture-svm-logistic-model}的结果作为模型输出。</li>
</ol>

<p>从模型可以看出，这并不是严格的$\mathcal Z$空间logistic回归。</p>

<h2 id="logistic">核logistic回归</h2>

<p>最佳的$\mathbf w$是$\mathbf z_n$的线性组合，这是能使用核方法的关键。</p>

<p>从SGD来看，logistic回归的$\mathbf w$也是$\mathbf z_n$的线性组合
\[
\mathbf w_{LOGREG}＝\sum_{n=1}^N\left(\alpha_ny_n\right)\mathbf z_n。
\]</p>

<blockquote>
  <h4 id="representer-theorem">表示定理（representer theorem）</h4>
  <hr />
  <p>对任意的$L_2$正则化线性模型     <br />
\begin{equation*}
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^Nerr\left(y_n,\mathbf w^T\mathbf z_n\right)\right)，
\end{equation*} 
存在能用$\mathbf z_n$线性表示的最佳解$\mathbf w_*=\sum_{n=1}^N\beta_n\mathbf z_n$。</p>
</blockquote>

<p>任意$L_2$正则化的线性模型都能使用核方法。$L_2$正则化的logistic回归优化模型为
\[
\min_{\mathbf w}\left({\lambda\over N}\mathbf w^T\mathbf w+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\mathbf w^T\mathbf z_n\right)\right)\right)。
\]
由上述定理可知，最佳$\mathbf w$一定是$\mathbf z_n$的线性组合。直接将用$\mathbf z_n$表示的$\mathbf w$代入上式，再用核方法表示，可以得到基于$L_2$正则化的核logistic回归优化模型
\begin{equation}
\min_\beta\left({\lambda\over N}\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(\mathbf x_n,\mathbf x_m)+{1\over N}\sum_{n=1}^N\log\left(1+\exp\left(-y_n\sum_{m=1}^N\beta_mK(\mathbf x_m,\mathbf x_n)\right)\right)\right)，
\end{equation}
这是无约束最优化问题，GD／SGD等都可求解。与支持向量机不同，KLR的大部分$\beta_n\neq 0$。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（4）：soft-margin支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-soft-margin-svm" />
      <pubdate>2015-01-07T17:24:31+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-soft-margin-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="soft-margin">soft-margin支持向量机</h2>

<p>在求解支持向量机参数的二次规划中，约束条件要求所有点被正确分类的叫<strong>hard-margin支持向量机</strong>。这类支持向量机过拟合原因：（1）特征转换功能太强；（2）坚持所有点都被正确分类。</p>

<p>可以通过放宽条件，容忍部分噪声（容许这部分噪声数据分类错误），使得支持向量机具有更好的泛化性能。</p>

<p>对于pocket PLA，目标函数为
\[
\min\limits_{b,\mathbf w}\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\]
结合<a href="/2015/01/svm-linear-svm/#mjx-eqn-eqlinear-svm-model">线性支持向量机</a>，可以得到容忍错分的优化模型
\[
\begin{aligned}
\min\limits_{b,\mathbf w}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\infty\cdot\left[\left[y_n\neq\mbox{sign}\left(\mathbf w^T\mathbf z_n+b\right)\right]\right]，
\end{aligned}
\]
$C$是调节最大边界和噪声容忍度的参数。但是，上述模型不是QP，并且不能区别分类错误时误差的大小，进一步降模型变为<strong>soft-margin支持向量机</strong>
\begin{equation}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n\\
&amp;\quad\xi_n\geq 0 \mbox{ for all }n，
\end{aligned}
\label{eq:soft-margin-primal-svm}
\end{equation}
该QP模型有$\tilde d+1+N$个变量和$2N$个约束条件，也被称为基于$\ell_1$损失的soft-margin。如果采用$\xi_n^2$，则被称为基于$\ell_2$损失的soft-margin
\begin{equation*}
\begin{aligned}
\min\limits_{b,\mathbf w,\boldsymbol\xi}&amp;\quad\frac{1}{2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n^2\\
\mbox{s.t.}&amp;\quad y_n\left(\mathbf w^T\mathbf z_n+b\right)\geq 1-\xi_n，
\end{aligned}
\end{equation*}
此时不再需要约束条件$\xi_n\geq 0$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png" alt="边界容忍度" /></a><div class="caption">Figure 1:  边界容忍度 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-margin-violation.png">PNG</a>]</div></div></div>

<p>$C$是调节最大边界和边界容忍度的参数，$\xi_n$是容忍误差的大小，如上图所示。$C$越小，对最大边界要求越高；$C$越大，能容忍的边界误差越小。</p>

<h2 id="soft-margin-1">对偶soft-margin支持向量机</h2>

<p>soft-margin支持向量机的拉格朗日函数为
\[
\begin{aligned}
\mathcal L(b,\mathbf w,\boldsymbol\alpha, \boldsymbol\beta)=
&amp;{1\over 2}\mathbf w^T\mathbf w + C\sum_{n=1}^N\xi_n\\
&amp;+\sum_{n=1}^N\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)+\sum_{n=1}^N\beta_n\left(-\xi_n\right)，
\end{aligned}
\]
根据${\partial\mathcal L\over\partial\xi_n}=0$可得$C-\alpha_n-\beta_n=0$，利用化解<a href="/2015/01/svm-dual-svm/#lagrange-dual-problem">拉格朗日对偶问题</a>相同的方法可得
\begin{equation}
\begin{aligned}
\min\limits_{\boldsymbol\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf z_n^T\mathbf z_m-\sum_{n=1}^N\alpha_n \\
\mbox{subject to}&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\
&amp;\quad 0\leq\alpha_n\leq C,\mbox{ for }n=1,2,\ldots,N \\
\mbox{implicitly}&amp;\quad \mathbf w=\sum_{n=1}^N\alpha_ny_n\mathbf z_n\\
&amp;\quad\beta_n=C-\alpha_n,\mbox{ for }n=1,2,\ldots,N，
\end{aligned}
\label{eq:qp-soft-margin-dual-svm}
\end{equation}
与hard-margin对偶支持向量机不同的地方只是$\alpha_n$多了一个上界$C$，这是$N$个变量$2N+1$个约束条件的QP。$\alpha_n$的约束界也可以表示为矩阵形式
\begin{equation}
\mathbf 0_N\leq \mathbf I_N\boldsymbol\alpha \leq C\cdot \mathbf 1_N。
\end{equation}</p>

<h2 id="soft-margin-2">核soft-margin支持向量机</h2>

<p>soft-margin是实际中广泛应用的支持向量机。</p>

<p>soft-margin的支持向量机与<a href="/2015/01/svm-kernel-svm/#kernel-trick">hard-margin的支持向量机</a>基本相同，$\alpha_n$上界$C$的限制，导致$b$的计算不同。</p>

<p>利用complementary slackness条件可得
\begin{equation}
\begin{aligned}
\alpha_n\left(1-\xi_n-y_n\left(\mathbf w^T\mathbf z_n+b\right)\right)=&amp;0\\
\left(C-\alpha_n\right)\xi_n=&amp;0，
\end{aligned}
\label{eq:soft-margin-complementary-slackness}
\end{equation}
当$\alpha_s&gt;0$时，$b=y_s-y_s\xi_s-\mathbf w^T\mathbf z_s$；当$\alpha_s&lt;C$时，$\xi_s=0$。满足$0&lt;\alpha_s&lt;C$的点称为<strong>自由支持向量</strong>$\left(\mathbf x_s, y_s\right)$，利用这些点容易得到
\begin{equation}
b=y_s-\sum\limits_{SV}\alpha_ny_nK\left(\mathbf x_n, \mathbf x_s\right)。
\end{equation}
在极少数情况下，不存在自由支持向量，$b$通过不等式限定，只要满足KKT条件的取值都是合理的$b$。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png" alt="soft-margin高斯核支持向量机" /></a><div class="caption">Figure 2:  soft-margin高斯核支持向量机 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-soft-margin-gaussian-svm.png">PNG</a>]</div></div></div>

<p>上图展示了soft-margin高斯核支持向量机的效果，灰色的区域表示最大分类间隔。从图中可以看出，$C$越大，对误差的容忍越弱，越容易导致过拟合。</p>

<h2 id="alphan">$\alpha_n$的物理含义</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png" alt="不同类型的数据点" /></a><div class="caption">Figure 3:  不同类型的数据点 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-alpha-n.png">PNG</a>]</div></div></div>

<p>通过公式\eqref{eq:soft-margin-complementary-slackness}可知，$\alpha_n$将数据点分为如上图所示的3种类型：</p>

<ol>
  <li>当$\alpha_n=0$时，非支持向量，$\xi_n=0$，位于边界之外，极少数可能在边界上；</li>
  <li>当$0&lt;\alpha_n&lt;C$时，自由（free）支持向量$\square$，$\xi_n=0$，位于边界上，用于计算$b$；</li>
  <li>当$\alpha_n=C$时，有界（bounded）支持向量$\triangle$，$\xi_n=1-y_n\left(\mathbf w^T\mathbf z_n+b\right)$，落在边界内，可能正确分类也可能分错，极少数可能在边界上。</li>
</ol>

<h2 id="section">模型选择</h2>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png"><img src="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png" alt="［中］：交叉验证误差；［右］：支持向量个数" /></a><div class="caption">Figure 4:  ［中］：交叉验证误差；［右］：支持向量个数 [<a href="/assets/images/2015-01-07-svm-soft-margin-svm-model-select.png">PNG</a>]</div></div></div>

<p>上图左是soft-margin高斯核支持向量机的分类效果，横轴是$C$的变化，纵轴是$\gamma$的变化。由于$E_{cv}(C,\gamma)$不光滑，通常的模型选择方法是通过$C$和$\gamma$的数据网格，利用交叉验证的方法选择合适的模型，上图中所示，选择了左下角的模型。</p>

<p>交叉验证中，将数据分为$N$份的验证称为<strong>leave-one-out交叉验证</strong>，它的误差上界是
\begin{equation}
E_{loocv}\leq\frac{\#SV}{N}，
\label{eq:eloocv-upper-bound}
\end{equation}
$\#SV$表示支持向量的个数。可以通过支持向量的个数进行模型选择。由于支持向量个数的函数也是非光滑的，难以优化，也采取利用$C$和$\gamma$的数据网格，多次计算后做选择。</p>

<p>由于\eqref{eq:eloocv-upper-bound}也只是给出了$E_{loocv}$的上界，通常用于当$E_{cv}$计算量很大时模型的安全检查，剔除那些支持向量过多的危险模型，然后再在剩余模型中进一步做交叉验证选择合适的模型。</p>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>支持向量机（3）：核支持向量机</title>
      <link href="http://qianjiye.de/2015/01/svm-kernel-svm" />
      <pubdate>2015-01-06T14:25:58+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2015/01/svm-kernel-svm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">回顾对偶支持向量机</h2>

<p>当特征空间维数$\tilde d$很大时，计算$q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m$是对偶支持向量机的求解瓶颈。</p>

<p>能否找到比$O(\tilde d)$快的方法计算$\mathbf z_n^T\mathbf z_m=\Phi(\mathbf x_n)^T\Phi(\mathbf x_m)$？能否将先特征转换再计算内积的两步合为一步呢？</p>

<h2 id="kernel-trick">核技巧</h2>

<p>对于2阶多项式变换
\[
\Phi_2(\mathbf x)=\left(1, x_1,x_2,\ldots,x_d,x_1^2,x_1x_2,\ldots,x_1x_d,x_2x_1,x_2^2,\ldots,x_2x_d,\ldots,x_d^2\right)，
\]
为了简化同时包含了$x_1x_2$和$x_2x_1$这样的项。变换之后$Z$空间的内积可以可直接通过$X$空间计算
\[
\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)=1+\left(\mathbf x^T\mathbf x’\right)+\left(\mathbf x^T\mathbf x’\right)^2。
\]
这种特征转换和内积合并的方法称之为<strong>核函数</strong>，
\begin{equation*}
K_{\Phi_2}\left(\mathbf x,\mathbf x’\right)=\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)。
\end{equation*}
利用核函数，可以简化对偶支持向量机的实现，二次项的系数为
\begin{equation}
q_{n,m}=y_ny_m\mathbf z_n^T\mathbf z_m=y_ny_mK\left(\mathbf x_n,\mathbf x_m\right)，
\end{equation}
利用支持向量$\left(\mathbf x_s, y_s\right)$计算偏移量
\begin{equation}
\begin{aligned}
b
=&amp;y_s-\mathbf w^T\mathbf z_s\\
=&amp;y_s-\left(\sum_{n=1}^N\alpha_ny_n\mathbf z_n\right)^T\mathbf z_s\\
=&amp;y_s-\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x_s\right)
，
\end{aligned}
\end{equation}
对于特定的输入$\mathbf x$，判别函数为
\begin{equation}
\begin{aligned}
g_{SVM}(\mathbf x)
=&amp;\mbox{sign}\left(\mathbf w^T\Phi(\mathbf x)+b\right)\\
=&amp;\mbox{sign}\left(\sum_{n=1}^N\alpha_ny_nK\left(\mathbf x_n,\mathbf x\right)+b\right)
。
\end{aligned}
\end{equation}
从上面的公式可以看出，计算不再依赖变换后的空间，只依赖原空间，$b$和判别函数只依赖原空间的支持向量，大大简化了计算。</p>

<h2 id="section-1">多项式核</h2>

<p>仿照2阶多项式核的定义，可以推导高阶多项式核的定义为
\begin{equation}
K_Q(\mathbf x,\mathbf x’)=\left(\zeta + \gamma\mathbf x^T\mathbf x’\right)^Q\quad\zeta\geq 0,\gamma &gt; 0。
\end{equation}
事实上，系数$\zeta$和$\gamma$的取值不会改变多项式所在的空间，这些系数会被$\mathbf w$所吞噬。但是，不同的系数会得到不同的支持向量和判别函数。选择不同的核，相当于改变了边界的定义。多项式核可以在几乎不增加计算量的情况下，得到复杂的判别界。支持向量机通过large-margin控制判别界的复杂度。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png" alt="2阶多项式核SVM的效果" /></a><div class="caption">Figure 1:  2阶多项式核SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-poly2-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是2阶多项式核支持向量机的效果，不同系数下的支持向量和判别界不同。</p>

<p>当$\zeta=0,\gamma=1$时，多项式核就变为了线性核。线性核利用原始支持向量机就比较高效。因此，应当首先尝试线性核，当线性核不能满足要求时再尝试其它高阶核。</p>

<h2 id="section-2">高斯核</h2>

<p>对于1维数据的高斯核，利用Taylor展式可得
\begin{equation*}
\begin{aligned}
K(x,x’)
=&amp;\exp\left(-\left(x-x’\right)^2\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\exp\left(2xx’\right)\\
=&amp;\exp\left(-x^2\right)\exp\left(-x’^2\right)\sum_{i=0}^\infty\frac{\left(2xx’\right)^i}{i!}\\
=&amp;\sum_{i=0}^\infty\exp\left(-x^2\right)\exp\left(-x’^2\right)\sqrt{\frac{2^i}{i!}}\sqrt{\frac{2^i}{i!}}x^ix’^i\\
=&amp;\Phi(x)^T\Phi(x’)，
\end{aligned}
\end{equation*}
其中$\Phi(x)=\exp\left(-x^2\right)\cdot\left(1,\sqrt{\frac{2^1}{1!}}x,\sqrt{\frac{2^2}{2!}}x^2,\ldots\right)$，容易看出这是无穷维的特征变换。更一般的高斯核定义为
\begin{equation}
K\left(\mathbf x,\mathbf x’\right)=\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x’\right\rVert^2\right)\quad \gamma&gt;0。
\end{equation}
高斯核也成为径向基函数（RBF，Radial Basis Function）核。基于高斯核的判别函数为
\begin{equation}
g_{SVM}(\mathbf x)=\mbox{sign}\left(\sum_{SV}\alpha_ny_n\exp\left(-\gamma\left\lVert\mathbf x-\mathbf x_n\right\rVert^2\right)+b\right)，
\end{equation}
它是以支持向量为中心的高斯函数的线性组合，不再依赖$\mathbf w$，只依赖于支持向量和系数$\alpha_n$。</p>

<p>高斯核相当于进行了无限维的特征转换，可以得到复杂的判别界，泛化性能通过最大边界“保证”。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png"><img src="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png" alt="高斯核的SVM的效果" /></a><div class="caption">Figure 2:  高斯核的SVM的效果 [<a href="/assets/images/2015-01-06-svm-kernel-svm-gaussian-kernel-svm.png">PNG</a>]</div></div></div>

<p>上图是不同系数的高斯核支持向量机的效果，$\gamma$越大，高斯函数越尖，越容易过拟合。当$\gamma\rightarrow\infty$时，$K_{lim}\left(\mathbf x,\mathbf x’\right)=[[\mathbf x＝\mathbf x’]]$，相当于严格判断是否与支持向量一致。</p>

<h2 id="section-3">小结</h2>

<p>各种核函数的比较如下表：</p>

<table>
  <thead>
    <tr>
      <th>核函数</th>
      <th>优势</th>
      <th>局限性</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>线性核</td>
      <td>计算快；<br />通过$\mathbf w$和支持向量容易解释</td>
      <td>无法处理线性不可分数据</td>
    </tr>
    <tr>
      <td>多项式核</td>
      <td>可以通过次数$Q$控制复杂度</td>
      <td>当$Q$很大时数值计算困难<br />（当$\left\lvert\zeta+\gamma\mathbf x^T\mathbf x’\right\rvert&lt;1$时，$K(\mathbf x, \mathbf x’)\rightarrow 0$）；<br />3个参数难以选择</td>
    </tr>
    <tr>
      <td>高斯核</td>
      <td>强大；<br />$K(\mathbf x, \mathbf x’)$有界；<br />1个参数难以选择；</td>
      <td>没有显示的$\mathbf w$供解读；<br />计算比线性核慢；<br />太强大导致容易过拟合</td>
    </tr>
  </tbody>
</table>

<p>当采用多项式核时，如果$Q$较小，可以尝试直接利用特征变换和原始支持向量机，求解速度可能比核方法更快。</p>

<p>核是一种特殊的相似性度量，但是不是所有的相似性度量都可以作为核。有效的核必须满足<strong>Mercer条件</strong>（充要条件）：核函数矩阵
\begin{equation*}
\begin{aligned}
\mathbf K
=&amp;\begin{bmatrix}
\Phi(\mathbf x_1)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_1)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_1)^T\Phi(\mathbf x_N)\\
\Phi(\mathbf x_2)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_2)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_2)^T\Phi(\mathbf x_N)\\
\ldots&amp;\ldots&amp;\ldots&amp;\ldots\\
\Phi(\mathbf x_N)^T\Phi(\mathbf x_1) &amp; \Phi(\mathbf x_N)^T\Phi(\mathbf x_2) &amp;\ldots &amp;\Phi(\mathbf x_N)^T\Phi(\mathbf x_N)
\end{bmatrix}\\
=&amp;\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]^T\left[\mathbf z_1\quad\mathbf z_2\quad\ldots\quad\mathbf z_N\right]\\
=&amp;\mathbf Z\mathbf Z^T
\end{aligned}
\end{equation*}
必须是对称半正定。</p>

<p>若$K_1(\mathbf x,\mathbf x’)=\Phi_1(\mathbf x)^T\Phi_1(\mathbf x’)$和$K_2(\mathbf x,\mathbf x’)=\Phi_2(\mathbf x)^T\Phi_2(\mathbf x’)$是两个有效的核，那么下面生成的也是有效的核：</p>

<ul>
  <li>$K(\mathbf x,\mathbf x’) = K_1(\mathbf x,\mathbf x’)\cdot K_2(\mathbf x,\mathbf x’)$；</li>
  <li>$K(\mathbf x,\mathbf x’) = K_1(\mathbf x,\mathbf x’)+K_2(\mathbf x,\mathbf x’)$；</li>
  <li>$K(\mathbf x,\mathbf x’) = (1-K_1(\mathbf x,\mathbf x’))^{-1},\quad 0&lt;K_1(\mathbf x,\mathbf x’)&lt;1$；</li>
  <li>$K(\mathbf x,\mathbf x’) = \alpha K_1(\mathbf x,\mathbf x’),\quad\alpha\in\mathbb R$。</li>
</ul>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
