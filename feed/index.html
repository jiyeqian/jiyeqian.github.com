<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiye Qian</title>
    <link href="http://qianjiye.de/feed/" rel="self" />
    <link href="http://qianjiye.de" />
    <lastbuilddate>2014-12-08T23:15:05+08:00</lastbuilddate>
    <webmaster>ccf.developer@gmail.com</webmaster>
    
    <item>
      <title>机器学习：异常检测</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-anomaly-detection" />
      <pubdate>2014-12-08T10:29:01+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-anomaly-detection</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<p>异常检测的基本思想：若发生了小概率事件，就认为出现了异常。</p>

<p>常用的异常检测方法是利用高斯密度函数，计算数据出现的概率，如果发现了概率小于某个阈值的数据，就认为该数据是异常的。</p>

<p>异常检测也是一种模式二分类方法，但两类数据严重不平衡，异常数据要显著少于正常数据。</p>

<h2 id="section-1">基于高斯（正态）分布的异常检测</h2>

<p>根据异常检测的思想，若$\mathbf x$出现的概率$p(\mathbf x) &lt; \varepsilon$，则认为$\mathbf x$是异常点。因此，异常检测的重要内容是估计概率密度函数。</p>

<h3 id="section-2">一元高斯分布</h3>

<p>基于一元高斯分布的异常检测的前提条件是假设特征之间相互独立。</p>

<p>通常假设特征分量的数据集$X$满足均值为$\mu$，方差为$\sigma^2$的正态分布，
\begin{equation}
X\sim\mathcal{N}\left(\mu, \sigma^2\right)，
\end{equation}
因此有
\begin{equation}
p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)，
\end{equation}
这需要估计均值$\mu$和方差$\sigma^2$，它们的极大似然估计为
\begin{equation}
\begin{aligned}
\mu = &amp; {1\over m}\sum_{i=1}^{m}x^{(i)}\\
\sigma^2 = &amp; {1\over m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2
\end{aligned}。
\label{eq:likehood-mu-sigma}
\end{equation}</p>

<p>得到了概率密度函数，就容易利用概率判断异常。</p>

<h4 id="section-3">一、异常检测</h4>

<blockquote>
  <h4 id="section-4">异常检测算法</h4>
  <hr />

  <ol>
    <li>选择能指示异常的特征$\mathbf x_i$；</li>
    <li>利用公式\eqref{eq:likehood-mu-sigma}，估计每维特征的均值和方差$\boldsymbol\mu_1,\ldots,\boldsymbol\mu_n,\boldsymbol\sigma_1^2,\ldots,\boldsymbol\sigma_n^2$；</li>
    <li>计算$\mathbf x$的概率，
\begin{equation}
p(\mathbf x) = \prod_{j=1}^n p\left(\mathbf x_j;\boldsymbol\mu_j,\boldsymbol\sigma_j^2\right)，
\end{equation}
通过特征分量概率密度函数乘积计算$\mathbf x$概率密度，需满足特征之间相互独立的假设；</li>
    <li>若$p(\mathbf x) &lt; \varepsilon$，则$\mathbf x$为异常点。</li>
  </ol>
</blockquote>

<p>异常检测的训练过程就是估计概率密度函数参数$\boldsymbol\mu$和$\boldsymbol\sigma^2$。通常情况，训练过程不需要异常数据。$60\%$的正常数据作为训练集，$20\%$的正常数据和$50\%$的异常数据作为交叉检验集，$20\%$的正常数据和$50\%$的异常数据作为测试集。</p>

<p>通过交叉检验集可确定判定异常的阈值$\varepsilon$，选择参数可利用<a href="/2014/11/machine-learning-advice-for-applying-machine-learning/#performance-evaluation">分类器性能评价指标</a>。</p>

<p>异常检测和监督学习存在不同的特点，应用在不同的场景：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">异常检测</th>
      <th style="text-align: left">监督学习</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">正样本（异常数据，$y=1$）少，通常$0\sim 20$个正样本，负样本（正常数据，$y=0$）多；</td>
      <td style="text-align: left">正样本和负样本都较多；</td>
    </tr>
    <tr>
      <td style="text-align: left">可能存多种不同类型的异常数据，难以通过正样本学习；</td>
      <td style="text-align: left">大量的正样本数据，能通过训练集了解正样本特点；</td>
    </tr>
    <tr>
      <td style="text-align: left">应用领域：欺诈检测、故障诊断、数据中心设备监控等</td>
      <td style="text-align: left">应用领域：垃圾邮件分类、天气预测、癌症分类等。</td>
    </tr>
  </tbody>
</table>

<h4 id="feature-transform">二、特征变换</h4>

<p>根据异常检测方法可知，运用异常检测有两个重要的前提条件：</p>

<ol>
  <li>特征满足高斯分布（特征之间的相关性下节考虑）；</li>
  <li>$p(\mathbf x)$对正常数据很大，但对异常数据很小。</li>
</ol>

<p>在实际应用中，原始特征可能并不满足这两个前提条件，需要将特征作一定变换或构造新的特征。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png"><img src="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png" alt="原始特征通过变换满足高斯分布" /></a><div class="caption">Figure 1:  原始特征通过变换满足高斯分布 [<a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-nongaussain-features.png">PNG</a>]</div></div></div>

<p>上图展示了通过函数$\log x$变换原始特征以满足高斯分布。也可以通过构造新的特征，比如数据中心监控，利用特征$\mbox{CPU load}$和$\mbox{network traffic}$构造新的特征$\frac{\mbox{CPU load}}{\mbox{network traffic}}$<sup id="fnref:why-create-new-feature"><a href="#fn:why-create-new-feature" class="footnote">1</a></sup>，使其在发生异常的时数据会变得很大或者很小。</p>

<h3 id="section-5">多元高斯分布</h3>

<p>实际应用中，特征之间可能存在相关性，需要采用多元高斯分布概率密度函数进行异常检测。</p>

<p>多元高斯分布的概率密度函数定义为</p>

<p>\begin{equation}
p(\mathbf x; \boldsymbol\mu, \Sigma)=\frac{1}{(2\pi)^{n\over 2}\lvert\Sigma\rvert^{1\over 2}}\exp\left(-{1\over 2}(\mathbf x - \boldsymbol\mu)^T\Sigma^{-1}(\mathbf x - \boldsymbol\mu)\right)，
\label{eq:multi-gaussians-pdf}
\end{equation}</p>

<p>其均值向量和协方差矩阵的极大似然估计为</p>

<p>\begin{equation}
\begin{aligned}
\boldsymbol\mu = &amp; {1\over m}\sum_{i=1}^{m}\mathbf x^{(i)}\\
\Sigma = &amp; {1\over m}\sum_{i=1}^{m}\left(\mathbf x^{(i)}-\boldsymbol\mu\right)\left(\mathbf x^{(i)}-\boldsymbol\mu\right)^T
\end{aligned}。
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png"><img src="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png" alt="二元高斯分布" /></a><div class="caption">Figure 2:  二元高斯分布 [<a href="/assets/images/2014-12-08-machine-learning-anomaly-detection-multi-gaussians.png">PNG</a>]</div></div></div>

<p>上图给出了不同参数的二元高斯密度函数图。图上排的协方差矩阵为对角阵，表示特征之间独立，可用一元高斯分布的方法进行异常检测；图下排的协方差矩阵是非对角阵，表示特征之间存在相关性，需借助多元高斯分布密度函数\eqref{eq:multi-gaussians-pdf}进行异常检测。</p>

<p>基于一元高斯分布和多元高斯分布的异常检测有不同的应用场景：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">一元高斯分布</th>
      <th style="text-align: left">多元高斯分布</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">根据先验知识<a href="#feature-transform">构造新特征</a>，手动处理相关性问题；</td>
      <td style="text-align: left">自动处理样本之间的相关性，计算$\Sigma$<sup id="fnref:multi-gaussian-feature-transform"><a href="#fn:multi-gaussian-feature-transform" class="footnote">2</a></sup>；</td>
    </tr>
    <tr>
      <td style="text-align: left">计算复杂度较低；</td>
      <td style="text-align: left">计算复杂度较高；</td>
    </tr>
    <tr>
      <td style="text-align: left">能处理样本数$m$很少的情况。</td>
      <td style="text-align: left">需要$m&gt;n$（一般$m&gt;10n$），否则$\Sigma$不可逆。</td>
    </tr>
  </tbody>
</table>

<p>若$\Sigma$不可逆，原因可能是不满足条件$m&gt;n$，或者存在冗余特征，也就是特征之间有相关性（比如$\mathbf x_1=k\mathbf x_2$或$\mathbf x_1=\mathbf x_2 ＋ \mathbf x_3$等）。</p>

<p>由此可见，特征之间是否具有相关性并非利用多元还是一元高斯分布进行异常检测的唯一条件，在必要的时候需要借助一元高斯分布对具有相关性特征的数据集进行异常检测。</p>

<h2 id="section-6">参考文献</h2>

<h3 id="section-7">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:why-create-new-feature">
      <p>如何判断构造的新特征有价值？哪些特征加入会有助于提高性能？ <a href="#fnref:why-create-new-feature" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:multi-gaussian-feature-transform">
      <p>利用多元高斯分布也要将每维特征变换为高斯分布么？ <a href="#fnref:multi-gaussian-feature-transform" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：维数约减</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-dimensionality-reduction" />
      <pubdate>2014-12-08T04:53:42+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-dimensionality-reduction</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<p>维数约减的作用通常是为了数据压缩和可视化。数据压缩不仅可以节省存储空间，而且可以加速机器学习算法。高维数据需要约减到3维或2维空间，以便观测其特性。 </p>

<h2 id="pca">主成分分析（PCA）</h2>

<p>维数约减最常用的方法是主成分分析（PCA，Principal Component Analysis）。PCA可以理解为在高维空间中寻找一个低维的面，使得高维空间中的点到该面上的距离之和最小，这个距离也叫投影误差。</p>

<p>利用PCA将维数从$n$维约减到$k$维，需要寻找$n$维空间中的$k$个向量$\mathbf u^{(1)}, \mathbf u^{(2)},\ldots,\mathbf u^{(k)}\in\mathbb R^n$，使空间中的点到这$k$个向量确定的面的投影误差最小。事实上，$n$维空间中的这$k$个向量是样本协方差矩阵最大的$k$个特征值对应的特征向量。</p>

<blockquote>
  <h4 id="pca-1">PCA维数约减算法</h4>
  <hr />

  <ol>
    <li>数据作均值为$0$的规范化（mean normalization），确保每维均值为$0$，若取值范围差异过大，还需尺度规范化（feature scaling）：$\mathbf x_j^{(i)}\leftarrow\frac{\mathbf x_j^{(i)}-\boldsymbol \mu_j}{\mathbf s_j}$（$\boldsymbol \mu_j$表示均值，$\mathbf s_j$表示标准差）；</li>
    <li>计算协方差矩阵（covariance matrix）：$\Sigma = \frac{1}{m}\sum_{i=1}^m\mathbf x^{(i)}\left(\mathbf x^{(i)}\right)^T$；</li>
    <li>利用特征向量将$n$维向量$\mathbf x$映射到$k$维向量$\mathbf z$：
\begin{equation}
\mathbf z^{(i)} = U_{reduce}^T\mathbf x^{(i)}。
\end{equation}</li>
  </ol>

  <div class="highlight"><pre><code class="language-matlab"><span class="p">[</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="p">]</span> <span class="p">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">Sigma</span><span class="p">);</span>
<span class="n">Ureduce</span> <span class="p">=</span> <span class="n">U</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">k</span><span class="p">);</span>
<span class="n">z</span> <span class="p">=</span> <span class="n">Ureduce</span>’ <span class="o">*</span> <span class="n">x</span><span class="p">;</span></code></pre></div>
  <p>在应用中，只需要在训练集上做PCA，交叉检验和测试集上可以直接应用训练集的均值$\boldsymbol\mu$、标准差$\mathbf s$和映射矩阵$U_{reduce}$计算约减后的向量。</p>
</blockquote>

<p>Matlab中<code>svd</code>和<code>eig</code>函数都可以得到相同的特征值和特征向量，但是<code>svd</code>更稳定。</p>

<p>从$k$维数据$\mathbf z$重构$n$维数据$\mathbf x$的方法为</p>

<p>\begin{equation}
\mathbf x_{approx}^{(i)} = U_{reduce}\mathbf z^{(i)}。
\end{equation}</p>

<p>约减后的维数$k$（主成分个数）通过方差保留的比率确定，选择满足下列条件的最小$k$</p>

<p>\begin{equation*}
\frac{\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}-\mathbf x_{approx}^{(i)}\right\rVert^2}{\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}\right\rVert^2}\leq 0.01，
\end{equation*}</p>

<p>此时方差保存比率为$99\%$。但是该方法计算复杂，可以通过特征值更简单的计算，选择满足下列条件的最小$k$</p>

<p>\begin{equation}
\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\geq 0.99，
\end{equation}</p>

<p>$S_{ii}$是SVD得到的特征值。</p>

<blockquote>
  <h4 id="pca-2">谨慎使用PCA</h4>
  <hr />

  <ol>
    <li>PCA不是解决过拟合的好方法，正则化是更好的策略（PCA或多或少损失了有助于分类的信息）；</li>
    <li>不得滥用PCA，除非有证据表明PCA的价值，比如在有训练时间和存储空间的限制的时候。</li>
  </ol>

</blockquote>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：聚类</title>
      <link href="http://qianjiye.de/2014/12/machine-learning-clustering" />
      <pubdate>2014-12-08T02:38:59+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/12/machine-learning-clustering</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">聚类简介</h2>

<p>聚类是一种非监督学习方法。</p>

<h2 id="k-means">$k$-means聚类</h2>

<p>$k$-means算法主要包含两步：为样本分配类标签以及修改类中心。</p>

<blockquote>
  <h4 id="k-means-1">$k$-means算法</h4>
  <hr />
  <p>随机初始化$K$个类中心$\boldsymbol\mu_1,\boldsymbol\mu_2,\ldots,\boldsymbol\mu_K\in\mathbb R^n$。 <br />
重复 {</p>

  <ol>
    <li>for $i=1$ to $m$：将$\mathbf x^{(i)}$的类别标签$c^{(i)}$设为最靠近的类中心标签，$c^{(i)}=\arg\min_{k=1}^K\left\lVert\mathbf x^{(i)}-\boldsymbol\mu_k\right\rVert^2$；</li>
    <li>for $k=1$ to $K$：重新计算类中心$\boldsymbol\mu_k$。</li>
  </ol>

  <p>}</p>
</blockquote>

<p>$k$-means聚类的代价函数为</p>

<p>\begin{equation}
J\left(c^{(1)},\dots,c^{(m)},\boldsymbol\mu_1,\ldots,\boldsymbol\mu_K\right)=\frac{1}{m}\sum_{i=1}^m\left\lVert\mathbf x^{(i)}-\boldsymbol\mu_{c^{(i)}}\right\rVert^2，
\label{eq:cf-k-means}
\end{equation}</p>

<p>该代价函数通常也称为distortion function。</p>

<p>从代价函数可以看出，$k$-means算法的第1步是通过修改类标签$c^{(i)}$最小化代价函数，第2步是通过修改类中心$\boldsymbol\mu_k$最小化代价函数。</p>

<p>从$k$-means算法可知，初始化类中心$\boldsymbol\mu_k$和确定类别数$k$影响着算法的性能。</p>

<h3 id="boldsymbolmuk">初始化类中心$\boldsymbol\mu_k$</h3>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png"><img src="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png" alt="不恰当的类中心初始化导致局部极值" /></a><div class="caption">Figure 1:  不恰当的类中心初始化导致局部极值 [<a href="/assets/images/2014-11-27-machine-learning-k-means-local-minimum.png">PNG</a>]</div></div></div>

<p>随机初始化类中心的方法通常是，随机从样本中选取$K$个点作为类中心。为了避免陷入局部极致，通常会进行多轮初始化，选择代价函数值最小的作为聚类结果。</p>

<blockquote>
  <h4 id="k-means-2">随机初始化选择$k$-means的类中心</h4>
  <hr />
  <p>For i = 1 to 100 {</p>

  <ol>
    <li>随机初始化类中心$\boldsymbol\mu_k$；</li>
    <li>$k$-means算法得到$c^{(1)},\dots,c^{(m)},\boldsymbol\mu_1,\ldots,\boldsymbol\mu_K$；</li>
    <li>计算代价函数\eqref{eq:cf-k-means}。</li>
  </ol>

  <p>}  <br />
选择代价函数值最小的聚类结果输出。</p>
</blockquote>

<p>当$K=2,\ldots,10$时，这种方法的代价函数变化明显，当$K$很大（$K&gt;100$）时，代价函数可能没有明显的变化。</p>

<h3 id="k">确定类别数$K$</h3>

<p>通过不停增大类别数$K$，选择代价函数曲线拐点对应的类别数，如下图左所示。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png"><img src="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png" alt="确定类别数的elbow method" /></a><div class="caption">Figure 2:  确定类别数的elbow method [<a href="/assets/images/2014-11-27-machine-learning-k-means-elbow-method.png">PNG</a>]</div></div></div>

<p>有时，代价函数曲线不存在拐点，如上图右所示。还可以根据$k$-means应用的具体场景，选择聚类数目，比如要制作XS、S、M、L、XL几种规格的服装，当然用类别数$K=5$来划分人的身高体重。</p>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：支持向量机（SVM）</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-support-vector-machines" />
      <pubdate>2014-11-27T08:05:56+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-support-vector-machines</guid>
      <content:encoded>&lt;![CDATA[<h2 id="logisticsvm">从Logistic回归到SVM</h2>

<p>本节通过Logistic回归的代价函数，演化到SVM的代价函数，然后通过代价函数最小化推导SVM的判别界<a href="#ng_ml_svm_2014">[1, P. 3—14]</a>。</p>

<p>Logistic回归的代价函数为</p>

<p>\begin{equation}
\begin{aligned}
J(\boldsymbol\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\boldsymbol\theta_j^2
\end{aligned}，
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<p>取出代价函数中的一项如下图，并绘制逼近Logistic代价函数的两个新的代价函数项$\mbox{cost}_1(\mathbf z)$和$\mbox{cost}_0(\mathbf z)$。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png" alt="图解Logistic回归的代价函数" /></a><div class="caption">Figure 1:  图解Logistic回归的代价函数 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-logistic2svm.png">PNG</a>]</div></div></div>

<p>将Logistic回归的代价函数提取$\lambda\over m$，令$C={1\over\lambda}$，并用新的代价函数项替代Logistic回归的代价函数项，可得SVM的代价函数</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf x^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2 ，
\label{eq:cf-svm-r}
\end{equation}</p>

<p>新的代价函数不再以$0$作为分类边界，而是以$+1$和$-1$。若代价函数\eqref{eq:cf-svm-r}要得最小值，可得SVM的判别界</p>

<p>\begin{equation}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2}\sum_{j=1}^{n}\boldsymbol\theta_j^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ \boldsymbol\theta^T\mathbf x^{(i)} \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\label{eq:svm-decision-boundary}
\end{equation}</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png" alt="图解SVM的判别界" /></a><div class="caption">Figure 2:  图解SVM的判别界 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-svm-boundary.png">PNG</a>]</div></div></div>

<p>根据向量间的夹角公式$\cos\theta = \frac{\boldsymbol\theta^T\mathbf x^{(i)}}{\left\lVert\boldsymbol\theta\right\rVert\left\lVert\mathbf x^{(i)}\right\rVert}$，则$p^{(i)}=\left\lVert\mathbf x^{(i)}\right\rVert\cos\theta$表示向量$\mathbf x^{(i)}$在向量$\boldsymbol\theta$上的投影，SVM的判别界\eqref{eq:svm-decision-boundary}可以改写为</p>

<p>\begin{equation*}
\begin{aligned}
&amp; \min_\boldsymbol\theta\frac{1}{2} \lVert\boldsymbol\theta\rVert^2 &amp; \\
&amp;
\begin{aligned}
\mbox{s.t.} &amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \geq 1 &amp;\mbox{if} &amp; ~~ y^{(i)} = 1 \\
&amp; ~~ p^{(i)}\lVert\boldsymbol\theta\rVert \leq -1 &amp; \mbox{if} &amp; ~~ y^{(i)} = 0
\end{aligned}
\end{aligned}。
\end{equation*}</p>

<p>$\boldsymbol\theta$是判别界的法线，$p^{(i)}$的值可正可负。要满足判别界的条件，$\left\vert p^{(i)}\right\vert$越大越好，这样$\lVert\boldsymbol\theta\rVert$就可以取到很小的值。因此，上图中右下比左下有更大的$\left\vert p^{(i)}\right\vert$，是更合适的SVM判别界。</p>

<h2 id="section">核函数</h2>

<p>SVM为了解决非线性可分问题，需要引入核函数（kernel）。作为SVM的核函数须满足Mercer定理。</p>

<blockquote>
  <h4 id="mercer-a-hrefjulysvm20142a">Mercer 定理<a href="#July_svm_2014">[2]</a></h4>
  <hr />
  <p>函数$\kappa$是 $\mathbb R^n \times \mathbb R^n \to \mathbb R$ 上的映射。如果$\kappa$是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例$\{\mathbf x_1,\mathbf x_2,\ldots,\mathbf x_n\}$，其相应的核函数矩阵是对称半正定的。</p>
</blockquote>

<p>通过特征与参考地标（landmark）$\mathbf l^{(i)}$之间的相似性，定义高斯核为<a href="#ng_ml_svm_2014">[1, P. 17—20]</a></p>

<p>\begin{equation}
\mathbf f_i = \mbox{similarity}\left(\mathbf x, \mathbf l^{(i)} \right)
= \exp\left(-\frac{\left\lVert\mathbf x - \mathbf l^{(i)}\right\rVert}{2\sigma^2} \right)。
\end{equation}</p>

<p>核函数的作用相当于特征变换，SVM引入了核函数的代价函数为</p>

<p>\begin{equation}
J(\boldsymbol\theta)  =  C\sum_{i=1}^{m}\left(y^{(i)}\mbox{cost}_1\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)+\left(1-y^{(i)}\right)\mbox{cost}_0\left(\boldsymbol\theta ^T\mathbf f^{(i)}\right)\right) + \frac{1}{2}\sum_{j=1}^n\boldsymbol\theta_j^2。
\label{eq:cf-svm-kernel-r}
\end{equation}</p>

<p>SVM代价函数和核函数的参数对SVM分类有如下影响<a href="#ng_ml_svm_2014">[1, P. 25]</a>：</p>

<ol>
  <li>大的$C~\left(C={1\over\lambda}\right)$导致Low Bias和High Variance；</li>
  <li>小的$C~\left(C={1\over\lambda}\right)$导致High Bias和Low Variance；</li>
  <li>大的$\sigma^2$使得特征$\mathbf f_i$变化平缓，导致High Bias和Low Variance；</li>
  <li>小的$\sigma^2$使得特征$\mathbf f_i$变化较大，导致Low Bias和High Variance。 </li>
</ol>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg"><img src="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg" alt="高斯核的SVM分类效果" /></a><div class="caption">Figure 3:  高斯核的SVM分类效果 [<a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.svg">SVG</a>, <a href="/assets/images/2014-11-27-machine-learning-advice-for-applying-machine-learning-kernel-svm-performance.png">PNG</a>]</div></div></div>

<h2 id="smo">SMO算法</h2>

<h2 id="svm">使用SVM</h2>

<p>Logistic回归强调所有点尽可能地远离中间那条线，SVM更应该关心靠近中间分割线的点，让他们尽可能地远离中间线。SVM考虑局部（不关心已经确定远离的点），Logistic回归考虑全局（已经远离的点可能通过调整中间线使其能够更加远离）<a href="#JerryLead_svm1_2011">[3]</a>。</p>

<h3 id="logisticsvm-1">Logistic回归、SVM、神经网络</h3>

<p>如何在分类器Logistic回归、SVM和神经网络之间做出选择？</p>

<p>$n$为特征数目（$x\in \mathbb{R}^{n+1} $），$m$为训练集样本数目，分类器选择的方法如下<a href="#ng_ml_svm_2014">[1, P. 31]</a>：</p>

<ol>
  <li>若$n$很大（比如$n\geq m, n=10000,m=10,\ldots,1000$），采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，用Logistic回归难以训练好非线性分类器）；</li>
  <li>若$n$很小而$m$大小适中（比如$n=1,\ldots,1000,m=10,\ldots,10000$），采用高斯核的SVM；</li>
  <li>若$n$很小而$m$很大（比如$n=1,\ldots,1000,m=50000+$），先可以增加特征，然后采用Logistic回归或者无核函数（线性核函数）的SVM（此种情况，高斯核的SVM分类器训练会慢）；</li>
  <li>神经网络在以上大多数情况都工作较好，但是可能训练速度会很慢；</li>
  <li>神经网络是非凸优化，会陷入局部极值，SVM是凸优化，不用担心陷入局部极值。</li>
</ol>

<p>选择Logistic回归或者无核函数（线性核函数）的SVM，是因为Logistic回归和无核函数（线性核函数）的SVM相似。</p>

<p>核函数的Logistic回归训练较慢，核函数的SVM可以训练较快。</p>

<h2 id="section-1">应用案例</h2>

<h3 id="a-hrefngmlsvmex20144-p-1015a">垃圾邮件分类<a href="#ng_ml_svm_ex_2014">[4, P. 10—15]</a></h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg"><img src="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg" alt="Email转换为特征向量" /></a><div class="caption">Figure 4:  Email转换为特征向量 [<a href="/assets/images/2014-11-27-machine-learning-svm-email-processing.svg">SVG</a>]</div></div></div>

<p>垃圾邮件分类首先需要将Email文本转换为特征向量。如上图所示，转换方法如下：</p>

<ol>
  <li>将Email文本转换为纯单词，比如：单词小写化，移除所有HTML标签，url链接均用单词<code>httpaddr</code>代替，提取词干（例如including、includes和included都用include代替）等；</li>
  <li>利用事先准备的词典，将单词用其在词典中的索引表示，实际应用中，词典通常有10000到50000个单词；</li>
  <li>将索引转换为$\{0, 1\}$特征向量，向量长度和词典中词汇数目一样，某个单词出现用$1$表示，否者用$0$表示。  </li>
</ol>

<h2 id="section-2">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_svm_2014">[1]A. Ng, “Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="July_svm_2014">[2]July, “支持向量机通俗导论（理解SVM的三层境界）.” csdn, 2014.</span>

[<a href="http://blog.csdn.net/v_july_v/article/details/7624837/">Online</a>]

</li>
<li><span id="JerryLead_svm1_2011">[3]JerryLead, “支持向量机SVM（一）.” cnblogs, 2011.</span>

[<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html">Online</a>]

</li>
<li><span id="ng_ml_svm_ex_2014">[4]A. Ng, “Programming Exercise 6: Support Vector Machines.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>

<h3 id="section-3">脚注</h3>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：实战技能</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-advice-for-applying-machine-learning" />
      <pubdate>2014-11-25T09:44:20+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-advice-for-applying-machine-learning</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">模型选择</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_aaml_2014">[1]</a>。</p>

<p>模型选择主要是通过评估模型效果，评价模型Bias和Variance的状况，选择合适的模型和估计相应的参数。</p>

<p>数据集一般划分为训练集、交叉验证集和测试集3部分，分别占的比例大概是60%、20%和20%<sup id="fnref:only-train-test"><a href="#fn:only-train-test" class="footnote">1</a></sup>。训练集用于估计模型参数，交叉验证集用于选择模型，测试集用于估计模型的泛化误差（generalization error）；选择在交叉验证集上误差小的模型，但是用测试集上误差作为模型的误差。测试集上的数据对参数估计和模型选择都是不可见的，因此才能“公平”的评价模型的性能。</p>

<p>$J_\mbox{train}(\boldsymbol\theta)$、$J_\mbox{cv}(\boldsymbol\theta)$、$J_\mbox{test}(\boldsymbol\theta)$分别表示模型在训练集、交叉验证集和测试集上的误差</p>

<p>\begin{equation}
J_{s}(\boldsymbol\theta) = \frac{1}{2m_{s}}\sum_{i=1}^{m_s}\left(h_\boldsymbol\theta\left(\mathbf x_s^{(i)}\right)-y_s^{(i)}\right)^2~~(s = \{\mbox{train},\mbox{cv},\mbox{test}\})。
\end{equation}</p>

<h3 id="bias-vs-variance">Bias vs. Variance</h3>

<p>简单来说，Bias和Variance评价模型的拟合程度，High Bias就是欠拟合（underfit），High Variance就是过拟合（overfit）。对于欠拟合，$J_\mbox{cv}(\boldsymbol\theta)\approx J_\mbox{train}(\boldsymbol\theta)$且都较大；对于过拟合，$J_\mbox{cv}(\boldsymbol\theta)\gg J_\mbox{train}(\boldsymbol\theta)$且$J_\mbox{train}(\boldsymbol\theta)$较小。</p>

<h3 id="section-1">模型选择与正则化——以多项式回归为例</h3>

<p>模型的复杂度和参数估计时采用的正则化参数$\lambda$对模型的Bias和Variance均有影响。</p>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png" alt="多项式回归模型" /></a><div class="caption">Figure 1:  多项式回归模型 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models.png">PNG</a>]</div></div></div>

<p>如果选用多项式回归模型，需要选择多项式的次数确定合适的模型。参数估计的时候，可以通过调节正则化系数，平衡Bias和Variance的关系。</p>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png" alt="多项式次数和正则化系数与误差的关系" /></a><div class="caption">Figure 2:  多项式次数和正则化系数与误差的关系 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-d-lambda.png">PNG</a>]</div></div></div>

<p>随着多项式次数$d$的增加， 模型从High Bias变为了High Variance；随着正则化系数$\lambda$的增加， 模型从High Variance变为了High Bias。通过评估调整模型和正则化参数时的误差，确定模型的相关系数。</p>

<h3 id="section-2">学习曲线</h3>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png" alt="High Bias和High Variance的学习曲线" /></a><div class="caption">Figure 3:  High Bias和High Variance的学习曲线 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-learning-curves.png">PNG</a>]</div></div></div>

<p>学习曲线描绘了训练样本数目和误差之间的关系，展示了模型可能存在的问题。通过学习曲线，判断模型是否合适；如果不合适，模型是High Bias还是High Variance，从而有针对性地解决问题。</p>

<h3 id="section-3">提升模型性能的策略</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">技术手段</th>
      <th style="text-align: left">处理问题</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">加入更多的训练样本</td>
      <td style="text-align: left">High Variance<sup id="fnref:Not-good-if-high-bias"><a href="#fn:Not-good-if-high-bias" class="footnote">2</a></sup></td>
    </tr>
    <tr>
      <td style="text-align: left">抽取特征子集</td>
      <td style="text-align: left">High Variance<sup id="fnref:Not-good-if-high-bias:1"><a href="#fn:Not-good-if-high-bias" class="footnote">2</a></sup></td>
    </tr>
    <tr>
      <td style="text-align: left">增加特征</td>
      <td style="text-align: left">High Bias</td>
    </tr>
    <tr>
      <td style="text-align: left">构造多项式特征</td>
      <td style="text-align: left">High Bias</td>
    </tr>
    <tr>
      <td style="text-align: left">增大$\lambda$</td>
      <td style="text-align: left">High Variance</td>
    </tr>
    <tr>
      <td style="text-align: left">减小$\lambda$</td>
      <td style="text-align: left">High Bias</td>
    </tr>
  </tbody>
</table>

<h2 id="performance-evaluation">性能评估</h2>

<p>本节的主要内容来自Andrew NG的机器学习课程<a href="#ng_ml_mlsd_2014">[2]</a>。</p>

<p>如何和里评价机器学习算法，尤其是对于有偏的类别而言，单纯的正确率（Accuracy），
\begin{equation}
\mbox{Accuracy} = \frac{\mbox{true positives + true negatives}}{\mbox{total examples}}，
\end{equation}
难以合理评价分类器性能。</p>

<p>比如癌症检测，癌症的概率大概只有0.5左右，对于分类器，采用作弊的方法，即使全部输出非癌症的结果，也可以获得99.5％的正确率。因此需要合理的评估方法，通常采用的是准确率（Precision）和召回率（Recall）。</p>

<h3 id="precision--recall">Precision &amp; Recall</h3>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png" alt="准确率和召回率" /></a><div class="caption">Figure 4:  准确率和召回率 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.png">PNG</a>, <a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-precisionrecall.svg">SVG</a>]</div></div></div>

<p>准确率刻画的是，被正确分类的样本在分类结果中占的比率，</p>

<p>\begin{equation}
\mbox{Precision} = \frac{\mbox{true positives}}{\mbox{true positives + false positives}}。
\end{equation}</p>

<ul>
  <li>true positives：真正正确分类的样本数；</li>
  <li>false positives：混入该类别的其它类别样本数。 </li>
</ul>

<p>召回率刻画的是，被正确分类的样本在输入样本中占的比率，</p>

<p>\begin{equation}
\mbox{Recall} = \frac{\mbox{true positives}}{\mbox{true positives + false negatives}}。
\end{equation}</p>

<ul>
  <li>false negatives：本该属于该类别却被分到其它类被的样本数。</li>
</ul>

<p>根据应用的具体需求，可以在准确率和召回率之间折中。若采用Logistic回归分类器，调高分类的阈值，可以提升该类别分类的准确率和降低召回率。</p>

<h3 id="ff1-score">$F$/$F_1$ Score</h3>

<p>采用准确率和召回率可以合理评估分类器性能，但是两个数值不如单数值的评估直接明了。$F$/$F_1$ Score是基于准确率和召回率的单数值评估指标，</p>

<p>\begin{equation}
F = 2 \times \frac{\mbox{Precision}\times\mbox{Recall}}{\mbox{Precision} + \mbox{Recall}}，
\end{equation}</p>

<p>数值越高表示效果越好。</p>

<h3 id="section-4">数据为王</h3>

<div class="image_line" id="figure-5"><div class="image_card"><a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png"><img src="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png" alt="数据集大小对准确率的影响" /></a><div class="caption">Figure 5:  数据集大小对准确率的影响 [<a href="/assets/images/2014-11-25-machine-learning-advice-for-applying-machine-learning-polynomial-models-size-accuracy.png">PNG</a>]</div></div></div>

<p>通常而言，增加参数数目（比如：Logistic回归增加特征数目，神经网络增加隐藏层神经元）和训练样本数目，可以提升分类器的性能。增加参数数目可以获得Low Bias（$J_\mbox{train}(\theta)$小），增加样本数目可以获得Low Variance（$J_\mbox{test}(\theta)$小）。</p>

<blockquote>
  <p>“It’s not who has the best algorithm that wins. It’s who has the most data.”<a href="#ng_ml_mlsd_2014">[2, P. 16]</a></p>
</blockquote>

<p>在大规模数据上训练模型，取得良好效果的前提条件是：</p>

<ol>
  <li>模型具备足够多的参数，能够表示复杂的函数；</li>
  <li>特征$\mathbf x$包含了预测$y$的足够信息（例如：该领域的专家可以仅仅通过$x$的有把握地预测$y$）。</li>
</ol>

<p>Guo-Xun Yuan等撰文指出<a href="#yuan_ralll_2012">[3]</a>，对于大规模数据的应用而言，线性分类器可能获得和非线性分类器接近的性能，并且训练和测试的速度要快得多。</p>

<h2 id="section-5">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_aaml_2014">[1]A. Ng, “Advice for applying machine learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_mlsd_2014">[2]A. Ng, “Machine Learning System Design.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="yuan_ralll_2012">[3]G.-X. Yuan, C.-H. Ho, and C.-J. Lin, “Recent Advances of Large-Scale Linear Classification,” <i>Proceedings of the IEEE</i>, vol. 100, no. 9, pp. 2584–2603, 2012.</span>

</li></ol>

<h3 id="section-6">脚注</h3>

<div class="footnotes">
  <ol>
    <li id="fn:only-train-test">
      <p>对于不需要做模型选择的情况，只需要训练集和测试集，样本的比例通常是70%和30%。 <a href="#fnref:only-train-test" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:Not-good-if-high-bias">
      <p><a href="http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html">Not good if you have high bias</a> <a href="#fnref:Not-good-if-high-bias" class="reversefootnote">&#8617;</a> <a href="#fnref:Not-good-if-high-bias:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：学习资源</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-resources" />
      <pubdate>2014-11-22T00:00:36+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-resources</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">入门资料</h2>

<ol>
  <li>Hacker’s guide to Neural Networks［<a href="http://karpathy.github.io/neuralnets/">Link</a>］</li>
  <li>Probabilistic Programming &amp; Bayesian Methods for Hackers［<a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Link</a>］</li>
  <li>Visualizing MNIST: An Exploration of Dimensionality Reduction［<a href="https://colah.github.io/posts/2014-10-Visualizing-MNIST/">Link</a>］</li>
  <li>Data Science in Python［<a href="http://blog.yhathq.com/posts/data-science-in-python-tutorial.html">Link</a>］</li>
  <li>Bayesian network与python概率编程实战入门［<a href="https://github.com/memect/hao/blob/master/awesome/bayesian-network-python.md">Link</a>］</li>
</ol>

<h3 id="deep-learning">Deep Learning</h3>

<ol>
  <li>DEEP LEARNING（An MIT Press book in preparation）［<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Link</a>］</li>
  <li>Neural Networks and Deep Learning［<a href="http://neuralnetworksanddeeplearning.com">Link</a>］</li>
  <li>Unsupervised Feature Learning and Deep Learning［<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial">Link</a>］</li>
  <li>TUTORIAL ON DEEP LEARNING FOR VISION［<a href="https://sites.google.com/site/deeplearningcvpr2014/">Link</a>］</li>
  <li>Quoc Le’s Lectures on Deep Learning［<a href="http://www.trivedigaurav.com/blog/quoc-les-lectures-on-deep-learning/">Link（视频）</a>］</li>
  <li>深度学习进阶线路图［<a href="http://www.aitmr.com/?s=深度学习进阶线路图">Link</a>］</li>
  <li>深度神经网络DNN的多GPU数据并行框架及其在语音识别的应用［<a href="http://www.csdn.net/article/2014-07-11/2820628-DNN">Link</a>］</li>
  <li>解密接近人脑的智能学习机器——深度学习及并行化实现［<a href="http://www.ccf.org.cn/sites/ccf/zlcontnry.jsp?contentId=2831644844483">Link</a>］</li>
  <li>看DeepMind如何用Reinforcement learning玩游戏［<a href="http://www.infoq.com/cn/articles/atari-reinforcement-learning">Link</a>］</li>
</ol>

<h2 id="section-1">资源整合</h2>

<ol>
  <li>Some of useful machine learning resources from beginner to intermediate［<a href="https://www.quora.com/Eren-Golge/Machine-Learning/Some-of-useful-machine-learning-resources-from-beginner-to-intermediate">Link</a>］</li>
  <li>My deep learning reading list［<a href="http://blog.sina.com.cn/s/blog_bda0d2f10101fpp4.html">Link</a>］</li>
  <li>Where to Learn Deep Learning – Courses, Tutorials, Software［<a href="http://www.kdnuggets.com/2014/05/learn-deep-learning-courses-tutorials-overviews.html">Link</a>］</li>
  <li>Deep Learning – important resources for learning and understanding［<a href="http://www.kdnuggets.com/2014/08/deep-learning-important-resources-learning-understanding.html">Link</a>］</li>
  <li>A curated list of awesome Machine Learning frameworks, libraries and software［<a href="https://github.com/josephmisiti/awesome-machine-learning">Link</a>］</li>
  <li>个人阅读的Deep Learning方向的paper整理［<a href="http://hi.baidu.com/chb_seaok/item/6307c0d0363170e73cc2cb65">Link</a>］</li>
</ol>

<h2 id="section-2">竞赛与数据</h2>

<ol>
  <li>kaggle：<a href="http://www.kaggle.com">The Home of Data Science</a>［<a href="http://www.quora.com/What-are-some-alternatives-to-Kaggle">What are some alternatives to Kaggle?</a>］</li>
  <li>DRIVENDATA：<a href="http://www.drivendata.org">Data science competitions to save the world</a></li>
  <li>百度：<a href="http://openresearch.baidu.com/?locale=en_US">百度开放研究社区</a>   </li>
  <li>阿里巴巴：<a href="http://tianchi.alibaba.com/index.htm">天池大数据科研平台</a>  </li>
  <li>360：<a href="http://openlab.360.cn">360开放实验室</a> </li>
  <li>卖数据的：<a href="http://datatang.com">数据堂</a></li>
</ol>

<h3 id="section-3">竞赛经验谈</h3>

<ol>
  <li><a href="http://www.52nlp.cn/cikm-competition-topdata">CIKM Competition数据挖掘竞赛夺冠算法-陈运文</a></li>
  <li><a href="http://www.chioka.in/kaggle-competition-solutions/">Kaggle Competition Past Solutions</a></li>
  <li><a href="http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/">Winning solution of Kaggle Higgs competition: what a single model can do?</a></li>
</ol>

<h2 id="demos--codes">Demos &amp; Codes</h2>

<ol>
  <li><a href="http://deeplearning.cs.toronto.edu">Toronto Deep Learning Demos</a></li>
  <li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">ConvNetJS Deep Q Learning Demo</a></li>
  <li><a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/">100 Best GitHub: Deep Learning</a></li>
  <li><a href="https://code.google.com/p/cuda-convnet2/">cuda-convnet2</a></li>
  <li><a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">Deep Visual-Semantic Alignments for Generating Image Descriptions</a></li>
</ol>

<h2 id="section-4">视频课程</h2>

<blockquote>
  <h4 id="machine-learningstanford-universitycourserahttpswwwcourseraorgcourseml">Machine Learning［Stanford University］［<a href="https://www.coursera.org/course/ml">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Andrew Ng  </p>

</blockquote>

<blockquote>
  <h4 id="learning-from-datacalifornia-institute-of-technologyhttpsworkcaltechedutelecoursehtml">Learning From Data［<a href="https://work.caltech.edu/telecourse.html">California Institute of Technology</a>］</h4>
  <hr />
  <p>Instructors：Yaser S. Abu-Mostafa  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learning-foundationscourserahttpswwwcourseraorgcoursentumlone">機器學習基石 (Machine Learning Foundations)［國立台灣大學］［<a href="https://www.coursera.org/course/ntumlone">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Hsuan-Tien Lin（林軒田）  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learning-techniquescourserahttpswwwcourseraorgcoursentumltwo">機器學習技法 (Machine Learning Techniques)［國立台灣大學］［<a href="https://www.coursera.org/course/ntumltwo">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Hsuan-Tien Lin（林軒田）  </p>

</blockquote>

<blockquote>
  <h4 id="introduction-to-machine-learningcarnegie-mellon-universityhttpalexsmolaorgteachingcmu2013-10-701indexhtmlhttpwwwtudoucomplcoverpk0asp6hd4s">Introduction to Machine Learning［<a href="http://alex.smola.org/teaching/cmu2013-10-701/index.html">Carnegie Mellon University</a>］［<a href="http://www.tudou.com/plcover/pK0asp6HD4s/">土豆</a>］</h4>
  <hr />
  <p>Instructors：Barnabas Poczos and Alex Smola <br />
参考教材：<em>Introduction to Machine Learning</em>［<a href="http://alex.smola.org/drafts/thebook.pdf">下载</a>］</p>

</blockquote>

<blockquote>
  <h4 id="machine-learningcarnegie-mellon-universityhttpwwwcscmuedu7etom10701sp11lecturesshtml">Machine Learning［<a href="http://www.cs.cmu.edu/%7Etom/10701_sp11/lectures.shtml">Carnegie Mellon University</a>］</h4>
  <hr />
  <p>Instructors：Tom Mitchell  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learningthe-university-of-british-columbiahttpwwwcsubccanando540-2013indexhtmlyoutubehttpswwwyoutubecomplaylistlistple6wd9fr--edyj5lbfl8uugjecvvw66f6">Machine Learning［<a href="http://www.cs.ubc.ca/~nando/540-2013/index.html">The University of British Columbia</a>］［<a href="https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6">Youtube</a>］</h4>
  <hr />
  <p>Instructors：Nando de Freitas  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learningcornell-universityhttpmachine-learning-coursejoachimsorg">Machine Learning［<a href="http://machine-learning-course.joachims.org">Cornell University</a>］</h4>
  <hr />
  <p>Instructors：Thorsten Joachims  </p>

</blockquote>

<blockquote>
  <h4 id="machine-learninguniversity-of-washingtoncourserahttpswwwcourseraorgcoursemachlearning">Machine Learning［University of Washington］［<a href="https://www.coursera.org/course/machlearning">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Pedro Domingos  <br />
预览可看视频</p>

</blockquote>

<blockquote>
  <h4 id="big-data-large-scale-machine-learningnew-york-universityhttpcilvrcsnyuedudokuphpidcoursesbigdataslidesstartvideohttpcilvrcsnyuedudokuphpidcoursesbigdataslidesstart">Big Data, Large Scale Machine Learning［<a href="http://cilvr.cs.nyu.edu/doku.php?id=courses:bigdata:slides:start">New York University</a>］［<a href="http://cilvr.cs.nyu.edu/doku.php?id=courses:bigdata:slides:start">Video</a>］</h4>
  <hr />
  <p>Instructors：John Langford and Yann LeCun  </p>

</blockquote>

<blockquote>
  <h4 id="probabilistic-graphical-modelscarnegie-mellon-universityhttpwwwcscmueduepxingclass10708lecturehtml">Probabilistic Graphical Models［<a href="http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html">Carnegie Mellon University</a>］</h4>
  <hr />
  <p>Instructors：Eric Xing</p>

</blockquote>

<blockquote>
  <h4 id="probabilistic-graphical-modelsstanford-universitycourserahttpswwwcourseraorgcoursepgm">Probabilistic Graphical Models［Stanford University］［<a href="https://www.coursera.org/course/pgm">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Daphne Koller</p>

</blockquote>

<blockquote>
  <h4 id="neural-networks-for-machine-learninguniversity-of-torontocourserahttpswwwcourseraorgcourseneuralnets">Neural Networks for Machine Learning［University of Toronto］［<a href="https://www.coursera.org/course/neuralnets">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Geoffrey Hinton</p>

</blockquote>

<blockquote>
  <h4 id="discrete-inference-and-learning-in-artificial-visioncole-centrale-pariscourserahttpswwwcourseraorgcourseartificialvision">Discrete Inference and Learning in Artificial Vision［École Centrale Paris］［<a href="https://www.coursera.org/course/artificialvision">Coursera</a>］</h4>
  <hr />
  <p>Instructors：Nikos Paragios and Pawan Kumar</p>

</blockquote>

<blockquote>
  <h4 id="intro-to-machine-learning-pattern-recognition-for-fun-and-profitstanford-universityudacityhttpswwwudacitycomcourseud120">Intro to Machine Learning: Pattern Recognition for Fun and Profit［Stanford University］［<a href="https://www.udacity.com/course/ud120">Udacity</a>］</h4>
  <hr />
  <p>Instructors：Sebastian Thrun</p>

</blockquote>

<blockquote>
  <h4 id="machine-learninggeorgia-institute-of-technologyudacitysupervised-learninghttpswwwudacitycomcourseud675-unsupervised-learninghttpswwwudacitycomcourseud741-reinforcement-learninghttpswwwudacitycomcourseud820">Machine Learning［Georgia Institute of Technology］［Udacity］［<a href="https://www.udacity.com/course/ud675">Supervised Learning</a>, <a href="https://www.udacity.com/course/ud741">Unsupervised Learning</a>, <a href="https://www.udacity.com/course/ud820">Reinforcement Learning</a>］</h4>
  <hr />
  <p>Instructors：Charles Isbell and Michael Littman   </p>

</blockquote>

<blockquote>
  <h4 id="statistical-learningstanford-universityhttpsstatlearningclassstanfordedu">Statistical Learning［<a href="https://statlearning.class.stanford.edu/">Stanford University</a>］</h4>
  <hr />
  <p>Instructors：Trevor Hastie and Rob Tibshirani     <br />
参考教材：<em>An Introduction to Statistical Learning, with Applications in R</em>［<a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf">下载</a>］</p>
</blockquote>

<blockquote>
  <h4 id="httpocwsjtueducng2socwcncoursedetailshtmid397httpocwsjtueducng2socwcncoursedetailshtmid398">机器学习导论［<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397">上海交通大学</a>］／统计机器学习［<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398">上海交通大学</a>］</h4>
  <hr />
  <p>Instructors：张志华       </p>

</blockquote>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：神经网络</title>
      <link href="http://qianjiye.de/2014/11/machine-learning-neural-networks" />
      <pubdate>2014-11-09T11:09:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/11/machine-learning-neural-networks</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">简介</h2>

<div class="image_line" id="figure-1"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_1.png"><img src="/assets/images/2014-10-21-neural-networks_1.png" alt="神经网络结构" /></a><div class="caption">Figure 1:  神经网络结构 [<a href="/assets/images/2014-10-21-neural-networks_1.png">PNG</a>]</div></div></div>

<p>神经网络是神经元分层级联构成的网络，除输入层外每个神经元对应一个计算模型。最左边和最右边的层分别称为输入（input）和输出（output）层，中间两层为隐藏层（hidden layer）。</p>

<p>当特征数目巨大时，简单的Logistic回归无法满足需求。神经网络用于解决复杂的非线性问题，可以看成是Logistic回归的组合，上图中每个橙色的神经元（除输入层之外）都对应一个Logistic方程。</p>

<p>对于分类问题，输入层输入原始数据，隐藏层的每个神经元可视为提取一种特征，输出层的每个神经元对应所属类别的概率（不是类别标签）。输入数据所属的类别是输出层概率最大神经元对应的类别。</p>

<p>神经网络通过前向传播计算给定输入对应的输出，通过误差反向传播估计权值矩阵。</p>

<h2 id="section-1">前向传播计算</h2>

<div class="image_line" id="figure-2"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_2.png"><img src="/assets/images/2014-10-21-neural-networks_2.png" alt="神经网络前向传播计算" /></a><div class="caption">Figure 2:  神经网络前向传播计算 [<a href="/assets/images/2014-10-21-neural-networks_2.png">PNG</a>]</div></div></div>

<p>神经网络前向传播，从输入到输出，逐层计算。上图所示<a href="#ng_ml_nnr_2014">[1, P. 23]</a>，假设权值矩阵$\Theta^{(l-1)}$已知，第$l$层可通过第$l-1$层和权值矩阵前向计算，</p>

<p>\begin{equation}
\mathbf a^{(l)} = g\left(\mathbf\Theta^{(l-1)}\mathbf a^{(l-1)}\right),
\label{eq:forward-propagation}
\end{equation}</p>

<p>$g$是<a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqsigmoid-function">Logistic函数</a>，每层额外增加了一个$\mathbf a_0^{(l)}= 1$的偏移（bias），$\mathbf\Theta^{(l-1)}$的行数为第$l$层神经元个数，列数为第$l-1$层神经元个数加$1$。</p>

<p>输出层（第$L$层）神经元的输出$\mathbf a^{(L)}$确定输入特征所属的类别。</p>

<p>如果神经网络只有输入层和含一个神经元的输出层（<a href="#figure-2">上图</a>去掉隐藏层只有输入和输出层），就相当于一个Logistic回归模型。</p>

<h2 id="section-2">反向参数估计</h2>

<p>神经网络通过反向传播估计权值矩阵$\Theta$，参数估计仍然是最小化代价函数。通过BP算法（BackPropagation algorithm），输出层的误差向输入层逐层反向传播，利用梯度下降法，估计权值矩阵。</p>

<h3 id="section-3">代价函数</h3>

<p>神经网络的神经元是Logistic模型，存在和Logistic模型类似的代价函数</p>

<p>\begin{equation}
\begin{aligned}
J(\Theta) = &amp;-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}\left(y_k^{(i)}\log \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k + \left(1 - y_k^{(i)}\right)\log\left(1 -  \left(h_\Theta\left(\mathbf x^{(i)} \right) \right)_k \right) \right) \\ 
&amp;+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2.
\end{aligned}
\label{eq:cf_nn}
\end{equation}</p>

<p>$h_\Theta (\mathbf x) \in \mathbb{R}^K$，$\left(h_\Theta (\mathbf x)\right)_k = \mathbf a_k^{(L)} $是输出层第$k$个神经元的输出，可由前向传播公式\eqref{eq:forward-propagation}计算；$s_l$表示第$l$层神经元的个数（不含bias unit）；神经网络有$L$层，$m$个样本，$K$个输出。</p>

<p>如果代价函数\eqref{eq:cf_nn}是非凸（non-convex）函数<sup id="fnref:if_no_global_minimum"><a href="#fn:if_no_global_minimum" class="footnote">1</a></sup>，理论上可能会陷入局部极值，事实上，即使不能保证取得全局极值，梯度下降法也能很好的最小化代价函数，使得神经网络工作良好<a href="#ng_ml_nnl_2014">[2, P. 30]</a>。</p>

<p>对比<a href="/2014/10/machine-learning-regularization/#mjx-eqn-eqcf-logistic-regression-r">正则化Logistic回归的代价函数</a>，由于神经网络有$K$个输出，前半部分相当于$K$个Logistic回归的代价函数之和，后半部分是非bias神经元洗漱组成的正则化项，$\mathbf a_0^{(l)} = 1$对应的系数$\mathbf \Theta_{j0}^{(l)}$和Logistic回归一样，不包含在正则化系数中。</p>

<h3 id="section-4">参数估计</h3>

<p>通过最小化代价函数$\min_\Theta J(\Theta)$估计模型的所有参数矩阵$\Theta^{(l)}$，采用梯度下降法时需计算代价函数$J(\Theta)$及其梯度$\mathbf D_{ij}^{(l)}$，
\begin{equation*}
\mathbf D_{ij}^{(l)} = \frac{\partial J(\Theta)}{\partial\Theta_{ij}^{(l)}}.
\end{equation*}</p>

<p>第$l$层的误差记为$\mathbf\delta^{(l)}$，$\mathbf\delta_j^{(l)}$表示第$l$层的第$j$个神经元的误差，对于$\mathbf a_0^{(l)} = 1$的bias节点$\boldsymbol\delta_0^{(l)}=0$。输出层（$l=L$）的误差为
\begin{equation}
\boldsymbol\delta^{(L)} = \mathbf a^{(L)} - \mathbf y, 
\label{eq:error-bp-1}
\end{equation}
对于隐藏层$(l = L-1, L-2, \ldots, 2)$，误差通过权值矩阵$\Theta^{(l)}$从输出层向各隐藏层反向传播，
\begin{equation*}
\boldsymbol\delta^{(l)} = \left(\Theta^{(l)}\right)^T\boldsymbol\delta^{(l+1)} .* g’\left(\mathbf z^{(l)}\right), 
\end{equation*}</p>

<p>其中，<code>.*</code>借用了Matlab中对应元素相乘的运算符，$\mathbf z^{(l)} = \Theta^{(l-1)}\mathbf a^{(l-1)}$，$\mathbf a^{(l)} = g\left(\mathbf z^{(l)}\right)$，$g’\left(\mathbf z^{(l)}\right) = \mathbf a^{(l)} .* \left(\mathbf 1 - \mathbf a^{(l)}\right)$<sup id="fnref:d-sigmoid-f"><a href="#fn:d-sigmoid-f" class="footnote">2</a></sup>，于是可得误差回传计算公式</p>

<p>\begin{equation}
\boldsymbol\delta^{(l)} = \left(\Theta^{(l)}\right)^T\boldsymbol\delta^{(l+1)} .* \mathbf a^{(l)} .* \left(\mathbf 1 - \mathbf a^{(l)}\right).
\label{eq:error-bp-2}
\end{equation}</p>

<p>Coursera的<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>和Michael Nielsen<a href="#nielsen_nndl_2014">[3]</a>的第二章给出了BP算法的推导过程<sup id="fnref:parameter_estimation"><a href="#fn:parameter_estimation" class="footnote">3</a></sup>。</p>

<blockquote>
  <h4 id="bp">BP算法之梯度计算</h4>
  <hr />
  <p>训练集：$\left\{\left(\mathbf x^{(1)}, \mathbf y^{(1)}\right),\ldots,\left(\mathbf x^{(m)}, \mathbf y^{(m)}\right)\right\}$。 </p>

  <p>初始化： <br />
1. $\Delta_{ij}^{(l)} = 0$； <br />
2. 随机数初始化$\Theta_{ij}^{(l)}$为$[-\epsilon, \epsilon]$的值，$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}$由神经元数目确定，其中，$L_{in} = s_l$，$L_{out}=s_{l+1}$。<sup id="fnref:initial_theta"><a href="#fn:initial_theta" class="footnote">4</a></sup>  </p>

  <p>for $k=1$ to $m$ { </p>

  <ol>
    <li>初始化输入层$\mathbf a^{(1)} = \mathbf x^{(k)}$，利用前向传播\eqref{eq:forward-propagation}，计算各层神经元$\mathbf a^{(l)}~~(l = 2,\ldots, L)$；  </li>
    <li>利用反向传播\eqref{eq:error-bp-1}和\eqref{eq:error-bp-2}，计算各层误差$\boldsymbol \delta^{(l)}~~(l = 2,\ldots, L)$；   </li>
    <li>更新$\Delta_{ij}^{(l)}~~(l = 1,\ldots, L - 1)$，$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + \mathbf a_j^{(l)}\boldsymbol\delta_i^{(l+1)}$<sup id="fnref:vector_update_Delta"><a href="#fn:vector_update_Delta" class="footnote">5</a></sup>（$\mathbf a^{(l)}$也须补$\mathbf a_0^{(l)}=1$）；   </li>
    <li>计算梯度$\mathbf D_{ij}^{(l)}~~(l = 1,\ldots, L - 1)$，
\begin{equation}
\mathbf D_{ij}^{(l)} := \left\{
\begin{aligned}
&amp; \frac{1}{m}\left(\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)}\right) &amp; (j \neq 0) \\
&amp; \frac{1}{m}\Delta_{ij}^{(l)} &amp; (j = 0)
\end{aligned} 
\right. .
\label{eq:gradient-cost-f}
\end{equation}
}</li>
  </ol>
</blockquote>

<h3 id="section-5">实现细节</h3>

<p><strong>矩阵展成（unroll）向量：</strong></p>

<ol>
  <li><code class="highlight language-matlab"><span class="n">thetaVec</span> <span class="p">=</span> <span class="p">[</span><span class="n">Theta1</span><span class="p">(:);</span> <span class="n">Theta2</span><span class="p">(:);</span> <span class="n">Theta3</span><span class="p">(:)]</span></code>；</li>
  <li>将向量化的待估参数作为costFunction的参数；</li>
  <li>costFunction内部再将向量还原为矩阵计算梯度；</li>
  <li>梯度向量化输出<code class="highlight language-matlab"><span class="n">grad</span> <span class="p">=</span> <span class="p">[</span><span class="n">D1</span><span class="p">(:);</span> <span class="n">D2</span><span class="p">(:);</span> <span class="n">D3</span><span class="p">(:)]</span></code>。</li>
</ol>

<p><strong>梯度检查（gradient checking）：</strong></p>

<p>梯度计算是梯度下降法的关键。梯度检查用以验证推导的梯度计算公式及其代码实现是否正确。通过比较代价函数［比如\eqref{eq:gradient-cost-f}］梯度公式输出结果与数值方法计算代价函数的导数是否一致，判断梯度公式是否正确。梯度检测方法也可推广到其它需要梯度计算的地方，比如Logistic回归的代价函数的参数估计。梯度检查应当在训练神经网络之前，可以通过构造一个新的较小规模的神经网络进行检验；若每次训练都检测梯度，速度很慢。具体实现方法可以参考<a href="#ng_ml_nnl_2014">[2]</a>课程的编程习题。</p>

<div class="highlight"><pre><code class="language-matlab"><span class="c">% 数值方法计算导数</span>
<span class="n">numgrad</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="n">perturb</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">theta</span><span class="p">));</span>
<span class="n">e</span> <span class="p">=</span> <span class="mf">1e-4</span><span class="p">;</span>
<span class="k">for</span> <span class="n">p</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">numel</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="c">% Set perturbation vector</span>
    <span class="n">perturb</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="n">e</span><span class="p">;</span>
    <span class="n">loss1</span> <span class="p">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">perturb</span><span class="p">);</span>
    <span class="n">loss2</span> <span class="p">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">perturb</span><span class="p">);</span>
    <span class="c">% Compute Numerical Gradient</span>
    <span class="n">numgrad</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">loss2</span> <span class="o">-</span> <span class="n">loss1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">e</span><span class="p">);</span>
    <span class="n">perturb</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">end</span></code></pre></div>

<p>如果梯度计算公式正确，<code>numgrad</code>$\approx$<code>grad</code>，通过比较<code>numgrad</code>与BP算法所得<code>grad</code>的差距判断BP算法的代价函数及其优化算法是否有subtle bugs。</p>

<div class="highlight"><pre><code class="language-matlab"><span class="c">% If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9). </span>
<span class="n">diff</span> <span class="p">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">numgrad</span><span class="o">-</span><span class="n">grad</span><span class="p">)</span><span class="o">/</span><span class="n">norm</span><span class="p">(</span><span class="n">numgrad</span><span class="o">+</span><span class="n">grad</span><span class="p">);</span></code></pre></div>

<p><strong>注意事项：</strong></p>

<p>不可将$\Theta_{ij}^{(l)}$初始化为$0$，若初始化为$0$，每层的所有神经元都是一样的。随机数初始化$-\epsilon\leq\Theta_{ij}^{(l)}\leq\epsilon$，选择$\epsilon$的有效策略是根据每层神经元的数目取$\epsilon=\frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}~(L_{in} = s_l,L_{out}=s_{l+1})$。</p>

<h3 id="section-6">代价函数及其梯度计算</h3>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="err"> [J, grad] = nnCostFunction(nn_params, ...</span>
                                   <span class="n">input_layer_size</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">num_labels</span><span class="p">,</span> <span class="c">...</span>
                                   <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
<span class="c">%NNCOSTFUNCTION Implements the neural network cost function for a two layer</span>
<span class="c">%neural network which performs classification</span>
<span class="c">%   [J, grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span>
<span class="c">%   X, y, lambda) computes the cost and gradient of the neural network. The</span>
<span class="c">%   parameters for the neural network are &quot;unrolled&quot; into the vector</span>
<span class="c">%   nn_params and need to be converted back into the weight matrices. </span>
<span class="c">% </span>
<span class="c">%   The returned parameter grad should be a &quot;unrolled&quot; vector of the</span>
<span class="c">%   partial derivatives of the neural network.</span>
<span class="c">%</span>

<span class="c">% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices</span>
<span class="c">% for our 2 layer neural network</span>
<span class="n">Theta1</span> <span class="p">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">nn_params</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="c">...</span>
                 <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>

<span class="n">Theta2</span> <span class="p">=</span> <span class="nb">reshape</span><span class="p">(</span><span class="n">nn_params</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_layer_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))):</span><span class="k">end</span><span class="p">),</span> <span class="c">...</span>
                 <span class="n">num_labels</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_layer_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>

<span class="c">% Setup some useful variables</span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
      
<span class="c">% Feedforward Propagation</span>
<span class="n">A1</span> <span class="p">=</span> <span class="n">X</span><span class="p">;</span>  <span class="c">% input layer, matrix size: 5000x400 (5000 samples and 400 features)</span>
<span class="n">A2</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">([</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">A1</span><span class="p">]</span> <span class="o">*</span> <span class="n">Theta1</span><span class="o">&#39;</span><span class="p">);</span> <span class="c">% hidden layer, matrix size: 5000x25</span>
<span class="n">A3</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">([</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">A2</span><span class="p">]</span> <span class="o">*</span> <span class="n">Theta2</span><span class="o">&#39;</span><span class="p">);</span> <span class="c">% output layer, matrix size: 5000x10</span>

<span class="c">% Cost Function</span>
<span class="c">% a tick to compute the cost</span>
<span class="n">J_matrix</span> <span class="p">=</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A3</span><span class="p">);</span> <span class="c">% set all cost</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">J_matrix</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">))</span> <span class="p">=</span> <span class="nb">log</span><span class="p">(</span><span class="n">A3</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">)));</span>  <span class="c">% overwrite</span>
<span class="k">end</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">sum</span><span class="p">(</span><span class="n">J_matrix</span><span class="p">(:))</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">J</span> <span class="p">=</span> <span class="n">J</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="c">...</span>
    <span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">Theta1</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">sum</span><span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">)));</span> <span class="c">% add regular term</span>

<span class="c">% Label matrix</span>
<span class="n">y_matrix</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">A3</span><span class="p">));</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">y_matrix</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="nb">i</span><span class="p">))</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">% BP error</span>
<span class="n">Delta3</span> <span class="p">=</span> <span class="n">A3</span> <span class="o">-</span> <span class="n">y_matrix</span><span class="p">;</span> <span class="c">% 5000x10</span>
<span class="n">Delta2</span> <span class="p">=</span> <span class="p">(</span><span class="n">Delta3</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">))</span> <span class="o">.*</span> <span class="p">(</span><span class="n">A2</span> <span class="o">.*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">));</span> <span class="c">% 5000x25</span>

<span class="c">% Gradient</span>
<span class="n">DELTA1</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">DELTA2</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">Delta2</span> <span class="p">=</span> <span class="n">Delta2</span><span class="o">&#39;</span><span class="p">;</span>
<span class="n">Delta3</span> <span class="p">=</span> <span class="n">Delta3</span><span class="o">&#39;</span><span class="p">;</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">m</span>
	<span class="n">DELTA1</span> <span class="p">=</span> <span class="n">DELTA1</span> <span class="o">+</span> <span class="n">Delta2</span><span class="p">(:,</span> <span class="nb">i</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span> <span class="n">A1</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="p">:)];</span>
	<span class="n">DELTA2</span> <span class="p">=</span> <span class="n">DELTA2</span> <span class="o">+</span> <span class="n">Delta3</span><span class="p">(:,</span> <span class="nb">i</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span> <span class="n">A2</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span> <span class="p">:)];</span>
<span class="k">end</span>

<span class="n">Theta1_grad</span> <span class="p">=</span> <span class="n">DELTA1</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">Theta2_grad</span> <span class="p">=</span> <span class="n">DELTA2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="c">...</span>
    <span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">Theta1</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>
<span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">=</span> <span class="c">...</span>
    <span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>

<span class="n">grad</span> <span class="p">=</span> <span class="p">[</span><span class="n">Theta1_grad</span><span class="p">(:)</span> <span class="p">;</span> <span class="n">Theta2_grad</span><span class="p">(:)];</span>

<span class="k">end</span></code></pre></div>

<h2 id="section-7">神经网络学到了什么？</h2>

<div class="image_line" id="figure-3"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_4.png"><img src="/assets/images/2014-10-21-neural-networks_4.png" alt="待识别字符" /></a><div class="caption">Figure 3:  待识别字符 [<a href="/assets/images/2014-10-21-neural-networks_4.png">PNG</a>]</div></div></div>

<p><a href="#figure-3">上图</a>展示了部分待识别的字符，通过样本学习建立了3层神经网络的分类器。输入字符图片规格是$20 \times 20$，隐层神经元25个，25个神经元对应的参数向量可视化为<a href="#figure-4">下图</a>。从可视化的参数可以看到，每个神经元和字符的笔画相似，输入的图片将激活符合“笔画”的神经元。估计神经网络参数可视为自动学习特征。</p>

<div class="image_line" id="figure-4"><div class="image_card"><a href="/assets/images/2014-10-21-neural-networks_3.png"><img src="/assets/images/2014-10-21-neural-networks_3.png" alt="参数可视化" /></a><div class="caption">Figure 4:  参数可视化 [<a href="/assets/images/2014-10-21-neural-networks_3.png">PNG</a>]</div></div></div>

<h2 id="section-8">应用</h2>

<p>卡内基梅隆大学基于神经网络的自动驾驶系统<a href="#pomerleau_alvinn_1989">[5]</a>，一些Matlab代码和数据还可以从<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/alvinn/">这里</a>找到。</p>

<h2 id="section-9">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_nnr_2014">[1]A. Ng, “Neural Networks: Representation.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="ng_ml_nnl_2014">[2]A. Ng, “Neural Networks: Learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="nielsen_nndl_2014">[3]M. A. Nielsen, <i>Neural Networks and Deep Learning</i>. Determination Press, 2014.</span>

[<a href="http://neuralnetworksanddeeplearning.com">Online</a>]

</li>
<li><span id="ng_ml_nnl_pe_2014">[4]A. Ng, “Programming Exercise 4: Neural Networks Learning.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li>
<li><span id="pomerleau_alvinn_1989">[5]D. A. Pomerleau, “ALVINN, an autonomous land vehicle in a neural network,” Carnegie Mellon University, 1989.</span>

[<a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&amp;context=compsci">Online</a>]

</li></ol>

<h3 id="section-10">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:if_no_global_minimum">
      <p>如果非凸函数，梯度下降法不能确定取得的是全局还是局部极值，可通过<a href="https://class.coursera.org/ml-007/forum/thread?thread_id=1089#comment-3416">取不同初始值多次求解增强鲁棒性</a>。 <a href="#fnref:if_no_global_minimum" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:d-sigmoid-f">
      <p><a href="/2014/10/machine-learning-logistic-regression/#mjx-eqn-eqd-sigmoid-function">Sigmoid函数导数</a>为$g(z)=g(z)(1-g(z))$。 <a href="#fnref:d-sigmoid-f" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:parameter_estimation">
      <p>这两篇博客（<a href="http://blog.csdn.net/abcjennifer/article/details/7758797">1</a>、<a href="http://blog.csdn.net/sheng_ai/article/details/19931347">2</a>）也给出了BP算法的推导过程，但是采用了形如<a href="/2014/10/machine-learning-linear-regression/#mjx-eqn-eqcost_function_linear_regression">线性回归的代价函数</a>。 <a href="#fnref:parameter_estimation" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:initial_theta">
      <p>不可将$\Theta_{ij}^{(l)}$初始化为$0$。若初始化为$0$，每层的所有神经元都一样<a href="#ng_ml_nnl_2014">[2, Pp. 25-26]</a>，每层只能学习到一种特征。$\epsilon$的取值方案参考课程习题脚注<a href="#ng_ml_nnl_pe_2014">[4, P. 7]</a>或<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning">课程Wiki</a>。 <a href="#fnref:initial_theta" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:vector_update_Delta">
      <p>更新的向量形式$\Delta^{(l)} := \Delta^{(l)} + \boldsymbol\delta^{(l+1)}\left(\mathbf a^{(l)}\right)^T$。这一步是怎么来的？有何意义？ <a href="#fnref:vector_update_Delta" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>CSS Essential</title>
      <link href="http://qianjiye.de/2014/10/css-essential" />
      <pubdate>2014-10-25T15:37:41+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/10/css-essential</guid>
      <content:encoded>&lt;![CDATA[<blockquote>
  <h4 id="what-is-css">What is CSS?</h4>
  <hr />
  <p>The basic goal of the Cascading Stylesheet (CSS) language is to allow a browser engine to paint elements of the page with specific features, like colors, positioning, or decorations. </p>

  <p>CSS主要作用是设定元素（通常称为盒子）的属性，并将设定这些元素在页面的位置关系。CSS解决的主要问题是：    </p>

  <ol>
    <li>如何选择指定的元素？选择器（Selector）   </li>
    <li>如何设定元素的属性？盒子模型（Box Model）   </li>
    <li>如何设置元素间的位置关系？定位机制（Positioning Scheme）</li>
  </ol>
</blockquote>

<p>本文主要介绍CSS的基本用法，包括如何使用选择器，如何设定样式以及如何布局定位，这些涉及到选择器、盒子模型、普通流、可视化格式模型和块格式化环境等核心概念，理解这些核心概念是熟练运用CSS的基础。</p>

<p>若要详细了解相关内容，可参考文中链接，以及<a href="https://developer.mozilla.org/en-US/docs/Web/CSS" title="Cascading Style Sheets">MDN</a>、<a href="http://www.w3.org/Style/CSS/" title="Cascading Style Sheets home page">W3</a>和<a href="http://www.w3school.com.cn/css/index.asp" title="CSS 教程">w3school</a>关于CSS的详细介绍。</p>

<h2 id="section">基础知识</h2>

<h3 id="section-1">基本语法</h3>

<p>CCS语法元素主要由选择器，属性和值构成，通过规则给选择器范围内的属性赋值。   </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_css-syntax-ruleset.png" alt="CSS语法" />
<div class="caption">CSS语法（规则）</div>
</div>
</div>

<!---
<div class="image_card">
<img src="/assets/images/2014-10-21-pla_1.png" alt="CSS语法">
<div class="caption">CSS语法（规则）</div>
</div>
--->

<h4 id="section-2">各部分元素含义：</h4>

<ul>
  <li><code>div p</code>：属性选择器</li>
  <li><code>#id</code>：id选择器</li>
  <li><code>first_line</code>：伪元素（pseudo-element）</li>
  <li><code>background-color</code>：属性（property）</li>
  <li><code>red</code>：值（value）</li>
</ul>

<h3 id="section-3">基本选择器</h3>

<p>选择器相当于作用域规则，选择属性设置起作用的范围。另一个相关概念是<a href="#block-formatting-context">块格式化环境</a>。</p>

<h4 id="section-4">常用选择器</h4>

<table>
  <tbody>
    <tr>
      <td>*</td>
      <td>通用选择器，匹配任何元素。</td>
    </tr>
    <tr>
      <td>E</td>
      <td>标签选择器，匹配所有使用E标签的元素。</td>
    </tr>
    <tr>
      <td>.info</td>
      <td>class选择器，匹配所有class属性中包含info的元素。</td>
    </tr>
    <tr>
      <td>#footer</td>
      <td>id选择器，匹配所有id属性等于footer的元素。</td>
    </tr>
  </tbody>
</table>

<p><strong>id选择器和class选择器的区别：</strong></p>

<ul>
  <li>在CSS文件中，id加前缀<code>#</code>，class用<code>.</code>；</li>
  <li>同一个页面id只可使用一次，class可多次引用：id是一个标签，用于区分不同的结构和内容，就象名字；class是一个样式，可套在任何结构和内容上，就象衣服，同一内容也可以套多个class的样式；</li>
  <li>从概念上说不一样：id是先找到结构/内容，再给它定义样式；class是先定义好一种样式，再套给多个结构/内容。</li>
</ul>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content&quot;</span> <span class="na">id=</span><span class="s">&quot;lavender&quot;</span><span class="nt">&gt;</span>The air was fragrant with lavender.<span class="nt">&lt;/div&gt;</span></code></pre></div>

<h4 id="section-5">多选择器组合</h4>

<table>
  <tbody>
    <tr>
      <td>E,F</td>
      <td>多元素选择器，同时匹配所有E元素或F元素，E和F之间用逗号分隔。</td>
    </tr>
    <tr>
      <td>E F</td>
      <td>后代元素选择器，匹配所有属于E元素后代的F元素，E和F之间用空格分隔。</td>
    </tr>
    <tr>
      <td>E &gt; F</td>
      <td>子元素选择器，匹配所有E元素的子元素F。</td>
    </tr>
    <tr>
      <td>E + F</td>
      <td>毗邻元素选择器，匹配所有紧随E元素之后的同级元素F。</td>
    </tr>
  </tbody>
</table>

<p>如果两个选择器紧连，表示同时满足两个条件的内容。</p>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nt">div</span><span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nt">div</span><span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>
The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.
<span class="nt">&lt;/div&gt;</span></code></pre></div>

<p>上例中，<code>div.emphasis</code>表示套用<code>.emphasis</code>样式的<code>div</code>标签。</p>

<h3 id="section-6">优先级别</h3>

<p>基本规则是<code>行内样式 &gt; id样式 &gt; class样式 &gt; 标签名样式</code>，也就是，选择越具体优先级越高。如下元素：   </p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="o">&lt;</span><span class="nt">div</span> <span class="nt">id</span><span class="o">=</span><span class="s2">&quot;ID&quot;</span> <span class="nt">class</span><span class="o">=</span><span class="s2">&quot;CLASS&quot;</span> <span class="nt">style</span><span class="o">=</span><span class="s2">&quot;color:black;&quot;</span><span class="o">&gt;&lt;/</span><span class="nt">div</span><span class="o">&gt;</span></code></pre></div>

<p>作用在其上样式的优先级从低到高是<code>div &lt; .CLASS &lt; div.CLASS &lt; #ID &lt; div#ID &lt; #ID.CLASS &lt; div#ID.CLASS</code>。</p>

<p><strong>继承与覆盖：</strong></p>

<p>子元素没有设置的属性会从父元素继承而来。同一元素的同一属性如有多次设置，优先级高的覆盖优先级低的；若是同一优先级，后设设置覆盖前设置。</p>

<div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
<span class="nt">div</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1em</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.content</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.2em</span><span class="p">;</span> <span class="k">line-height</span><span class="o">:</span> <span class="m">200%</span><span class="p">}</span>
<span class="nt">div</span><span class="nc">.emphasis</span> <span class="p">{</span><span class="k">font-size</span><span class="o">:</span> <span class="m">1.4em</span><span class="p">;</span> <span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">}</span>
<span class="nt">div</span><span class="nf">#lily</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">white</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span> <span class="nb">black</span><span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;content emphasis&quot;</span> <span class="na">id=</span><span class="s">&quot;lily&quot;</span><span class="nt">&gt;</span>
The flower of lily of the valley is like tinkler, be born at spending cauline top to show raceme.
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;water_lily&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>

<p>上例中，<code>div</code>设定的<code>font-size</code>优先级太低，不会生效；<code>.emphasis</code>覆盖<code>.content</code>的<code>font-size</code>，<code>#lily</code>的<code>font-size</code>最终为<code>1.4em</code>；<code>#water_lily</code>继承了<code>#lily</code>的所有属性。</p>

<h3 id="section-7">属性赋值</h3>

<p>确定属性最终取值要<a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">经历3步</a>（<a href="http://www.w3.org/TR/CSS2/cascade.html#value-stages" title="Specified, computed, and actual values">CSS2经历4步</a>）：首先获取CSS样式中的指定值（specified value），然后如有必要则转换为绝对值或计算值（computed value），最后根据局部环境的约束再转换为实际值（actual value）。</p>

<p>指定值可能是绝对值（例如：<code>2mm</code>、<code>red</code>等），也可能是相对值（例如：<code>auto</code>、<code>1.2em</code>、<code>12%</code>等）。对于绝对值而言不需要计算即可获得计算值，相对值需要再借助参考值计算获得计算值。如果属性没有指定值，它的取值继承父元素。</p>

<p>在特定的客户端环境，可能还需要将计算值转换为实际值，比如可能需要将小数的边界近似取整。</p>

<blockquote>
  <h4 id="section-8">常用的赋值规则</h4>
  <hr />
  <ol>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color_value" title="&lt;color&gt;">颜色</a>赋值的三种形式：关键字、RGB空间、HSL空间；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/length" title="&lt;length&gt;">长度</a>赋值的两种形式：以<code>em</code>、<code>ex</code>、<code>ch</code>、<code>rem</code>、<code>vw</code>、<code>vh</code>、<code>vmin</code>、<code>vmax</code>为单位的相对赋值，以<code>cm</code>、<code>mm</code>、<code>in</code>、<code>pt</code>、<code>pc</code>、<code>px</code>为单位的绝对赋值；</li>
    <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">百分数</a>赋值：<code>width</code>、<code>margin</code>和<code>padding</code>等接受百分数赋值。</li>
  </ol>

  <p>示例：    </p>

  <p><code>em</code>等单位和百分数都是相对赋值，需要继承父属性的参考值。<code>em</code>是针对字体大小的值，$w$ <code>em</code> = $w$ $\times$ <code>font-size</code>。</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;font-size:18px;&quot;</span><span class="nt">&gt;</span>
  Full size text (18px)
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:50%;&quot;</span><span class="nt">&gt;</span>50%<span class="nt">&lt;/span&gt;</span>
  <span class="nt">&lt;span</span> <span class="na">style=</span><span class="s">&quot;font-size:200%;&quot;</span><span class="nt">&gt;</span>200%<span class="nt">&lt;/span&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>上述代码中，<code>50%</code>从父属性继承而来的值是<code>18px</code>，然后再乘以<code>50%</code>（等价于<code>0.5em</code>），实际大小是<code>9px</code>（<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/percentage" title="&lt;percentage&gt;">查看效果</a>）。</p>
</blockquote>

<p>当一个属性可以有多个方向可设置值时，存在形如以下简写的赋值规则：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">位置</th>
      <th style="text-align: left">赋值规则</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border1.png" alt="1" /></td>
      <td style="text-align: left"><code>border-width: 1em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border2.png" alt="2" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border3.png" alt="3" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/assets/images/2014-10-25-css-essential_border4.png" alt="4" /></td>
      <td style="text-align: left"><code>border-width: 1em 2em 3em 4em</code></td>
    </tr>
  </tbody>
</table>

<h2 id="section-9">盒子模型</h2>

<p>CSS将HTML的元素（可认为是标签）定义为适合CSS处理的矩形盒（rectangular box）。<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/box_model">盒子模型（box model）</a>描述了这些矩形盒的尺寸、属性（颜色、背景和边框等）和位置特性，浏览器根据盒子模型实现页面的渲染与显示。</p>

<p>按照HTML的元素是否新开一行可分为块（block-level）元素和内联（inline）元素，这一特性对设置元素的布局至关重要。</p>

<blockquote>
  <h4 id="htmlblock-levelinline">HTML的块（block-level）元素和内联（inline）元素</h4>
  <hr />
  <p><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Block-level_elements">块元素</a>充满父元素的所有空间，每个块元素都会另起一个新行显示。块元素之内还可以包含块元素和行内元素。HTML定义的块元素包括<code>&lt;div&gt;, &lt;span&gt;, &lt;p&gt;</code>等。<a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Inline_elemente">内联元素</a>只占据标签包含内容的空间，不会另起新行显示。内联元素通常只包含内联元素。HTML定义的内联元素包括<code>&lt;span&gt;, &lt;code&gt;, &lt;textarea&gt;</code>等。   </p>

  <p><img src="/assets/images/2014-10-25-css-essential_css_box-intro.png" alt="d" /> </p>

  <p>上例中，<code>&lt;p&gt;</code>是块元素，新起一行显示，并且撑满了行，<code>&lt;em&gt;</code>是内联元素，紧接上一元素显示。</p>
</blockquote>

<p>CSS的<code>display</code>属性可设定HTML元素生成盒子的类型是块还是内联，常用的属性值有<code>block, inline, inline-block, none</code>。将<code>display</code>属性设为<code>block</code>，可将内联元素转为块元素。盒子模型有四类边：margin edge，border edge，padding edge，content edge。    </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_box_model.gif" alt="CSS语法" />
<div class="caption">盒子模型的四类边</div>
</div>
</div>

<p><code>.box {with: ...; height: ...}</code>设置的是只是<code>Content</code>尺寸。相关边的的设置方法是：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nc">.box</span> <span class="p">{</span>
    <span class="k">width</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">padding</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">border</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
    <span class="k">margin</span><span class="o">:</span> <span class="o">...</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>

<p>内联元素设置属性<code>height</code>和<code>width</code>是没有用的，致使它变宽变大的原因是内部元素的宽高<code>+padding</code>。</p>

<p>满足一定条件俩个盒子的外边届（margin）会叠加，使得估计这俩个盒子的位置关系变得复杂。</p>

<blockquote>
  <h4 id="margin-collapsing">外边叠加（margin collapsing）</h4>
  <hr />
  <p>上边距（top margin）和下边距（bottom margin）有时会叠加，叠加后两者最大值作为两者间的边距。发生外边叠加的3种情况（<a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html#CSS 外边距合并">图解</a>）<sup id="fnref:ccs_master"><a href="#fn:ccs_master" class="footnote">1</a></sup>：</p>

  <ol>
    <li>兄弟（Adjacent siblings）块：相邻块中，兄的下边界和弟的上边界会叠加；</li>
    <li>父与首尾孩子（Parent and first/last child）块：父块和首个孩子的margin-top之间不被任何东西分隔，则它们会叠加；父块和最后孩子的margin-bottom之间不被任何东西分隔，则它们会叠加；</li>
    <li>空块（Empty blocks）：不存在border、padding、inline content、height或min-height分隔块的margin-top和margin-bottom，上下边界会叠加。</li>
  </ol>

  <p>注意事项：</p>

  <ul>
    <li>当与负边界叠加时，叠加后的边界是最大正边界和最小边界之和；</li>
    <li>浮动的（floating）和绝对定位的（absolutely positioned）元素不参与外边叠加（这是因为创建了新的<a href="#block-formatting-context">块格式化环境</a>，而边界叠加只发生在同一块格式化环境）。</li>
  </ul>

</blockquote>

<h2 id="section-10">可视化格式模型</h2>

<blockquote>
  <h4 id="section-11">可视化格式模型解决的主要问题</h4>
  <hr />
  <ol>
    <li>如何生成盒子？</li>
    <li>盒子如何在页面布局？</li>
  </ol>
</blockquote>

<p><a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Visual_formatting_model">可视化格式模型（Visual formatting model）</a>是用于处理网页文档并显示到虚拟设备上的算法，该模型将文档中的每个元素生成符合盒子模型的盒子，然后对这些模型进行布局。可视化格式模型包括盒子的生成和定位两部分。</p>

<h3 id="box-generation">生成机制（Box Generation）</h3>

<p>生成盒子的类型通过CSS属性<code>display</code>设定，当设定属性为<code>block</code>、<code>list-item</code>和<code>table</code>时，生成块盒子，当设定属性为<code>inline</code>、<code>inline-block</code>和<code>inline-table</code>时，生成内联盒子。<a href="#htmlblock-levelinline">HTML元素的属性</a>决定了该元素生成盒子类型的默认值，块元素默认生成块盒子，内联元素默认生成内联盒子。不同类型盒子的定位机制不同。</p>

<h3 id="positioning-scheme">定位机制（Positioning Scheme）</h3>

<p>CSS的盒子有3种基本定位机制：普通流（normal flow）、浮动（floats）和绝对定位（absolute position），默认定位机制是普通流。在普通流中，盒子一个接一个排列，floats算法可以将盒子从普通流中抽出来，绝对定位通过包含它盒子的坐标系统来定位。</p>

<h4 id="normal-flow">Normal Flow</h4>

<blockquote>
  <h4 id="normal-flow-1">如何进入普通流（normal flow）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>static</code>或<code>relative</code>，并且将<code>float</code>属性设置为<code>none</code>。默认值<code>position: static</code>，<code>float: none</code>。</p>

  <p>普通流可以理解为盒子按照读入的先后次序依次处理，就像水流一样连续有序，每个页面对应一个流。浮动和绝对定位，可将读入序列中的某些盒子从这个流中抽取出来，单独处理。</p>
</blockquote>

<p>在普通流中，块盒子（block-level boxes ）和内联盒子（inline boxes）分别从纵向和横向对元素进行布局。块盒子通过垂直的方式一个接一个的排列，盒子之间的距离通过垂直方向的边界（margin-top和margin-bottom）控制（计算距离时要注意盒子<a href="#margin-collapsing">外边叠加问题</a>）。内联盒子通过水平方式排列，设置垂自方向的padding、borders和margins无效。水平排列的内联盒子通过行盒子（line box）组织在一起，行盒子的高度总是足够容纳包含在其中的所有盒子，可以通过设置行高（line height）控制行盒子的高度。因此，改变内联盒子尺寸的参数只有水平方向上的borders、padding、margins以及line height。    </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_css-syntax-ruleset_line_box.jpeg" alt="CSS语法" />
<div class="caption">包含在行盒子中的内联元素</div>
</div>
</div>

<p>CSS2.1可以设置<code>display</code>属性为<code>inline-block</code>，融合inline和block的属性。在水平方向按内联盒子的方式布局，同时可以像块盒子一样设置widths、heights和垂自方向的 margins、padding。 </p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_relative.png" alt="设置relative的效果" />
<div class="caption">设置relative的效果"position: relative; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="floats">Floats</h4>

<blockquote>
  <h4 id="floats-1">如何使用浮动模式（floats）</h4>
  <hr />
  <p>CCS将盒子的<code>float</code>属性设置为<code>left</code>或<code>right</code>，并且将<code>position</code>属性设置为<code>static</code>或<code>relative</code>。</p>

  <p><code>float</code>的非<code>none</code>属性暗示了盒子是<code>block</code>类型，因此，非块类型盒子的<code>display</code>属性会因为设置了<code>float</code>属性而改变为块类型。</p>
</blockquote>

<p>当盒子采用floats算法定位时，盒子会从普通流中被抽取出来，向左或向右移动，直到遇到父盒子或设置了float属性盒子的边界。</p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_floats.png" alt="设置floats的效果" />
<div class="caption">设置<code>float</code>的效果</div>
</div>
</div>

<p>上图3个红色的方块，两个左浮（<code>float: left</code>），一个右浮。第二个左浮窗口位于第一个左浮动窗口右边，如果再增加左浮窗口，会不断的照此叠加，充满父盒子后会换行继续显示。</p>

<p>一个盒子设置了浮动后，会影响它的兄弟元素，具体的影响方式较为复杂，这要视乎这些兄弟元素是块级元素还是内联元素，若是块级元素会无视这个浮动的块框，使自身尽可能与这个浮动元素处于同一行，导致被浮动元素覆盖，除非这些<code>div</code>设置了宽度，并且父元素的宽度不足以包含它们，这样兄弟元素才会被强制换行；若是内联元素，则会尽可能围绕浮动元素。</p>

<p>浮动元素脱离了普通流，因此包含它的父元素不会因为这个浮动元素的存在而自动撑高，这就造成了高度塌陷。下图所示，由于左边盒子浮动而脱离了普通流，父元素的高度只是由<code>span</code>盒子决定，看起来就像父元素高度塌陷了。</p>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_clearfloat1.png" alt="浮动的影响" />
<div class="caption">浮动的影响</div>
</div>
</div>

<p>当浮动影响到盒子的布局时，需要<a href="#clearing-floats">清除浮动</a>。</p>

<h4 id="absolute-positioning">Absolute Positioning</h4>

<blockquote>
  <h4 id="absolute-positioning-1">如何采用绝对定位（absolute positioning）</h4>
  <hr />
  <p>CCS将盒子的<code>position</code>属性设置为<code>absolute</code>或<code>fixed</code>。</p>

  <p>当设置为<code>fixed</code>的时候，盒子的位置相对于浏览器可见的视窗固定，即使拖动浏览器的滚动条，盒子的位置也固定不变。</p>

</blockquote>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-25-css-essential_absolute.png" alt="设置absolute的效果" />
<div class="caption">设置absolute的效果"position: absolute; left: 20px; top: 20px;"</div>
</div>
</div>

<h4 id="clearing-floats">清除浮动（clearing floats）</h4>

<p>由于浮动元素会影响它的兄弟元素的位置和父元素产生高度塌陷，需要清除浮动。</p>

<p>常用的清除浮动方法是<code>clear: both</code>，<code>clear</code>的属性值<code>both</code>、<code>left</code>、<code>right</code>、<code>none</code>、<code>inherit</code> 分别代表在元素左右两侧不允许出现浮动元素、左侧不允许出现浮动元素、右侧不允许出现浮动元素、不清除浮动、继承父元素的值。</p>

<p>但是，<code>clear</code>只是清除了浮动对兄弟元素的影响，而高度塌陷问题还没有解决，需要更高级的清除浮动——闭合浮动。为什么叫闭合浮动？因为浮动的元素脱离了普通流，对于它的父元素，它并没有闭合，这时候就需要闭合浮动了。</p>

<blockquote>
  <h4 id="section-12">闭合浮动的3种方法</h4>
  <hr />
  <p>（1）空<code>div</code>方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">style=</span><span class="s">&quot;clear: both;&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside&quot;</span><span class="nt">&gt;</span>我是页脚，我的上面添加了一个设置了 clear: both 的空 div<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>空<code>div</code>方法很方便，但是加入了没有涵义的<code>div</code>，这违背了结构与表现分离的原则，并且后期维护也不方便。</p>

  <p>（2）<code>overflow</code>方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box&quot;</span> <span class="na">style=</span><span class="s">&quot;overflow: hidden; *zoom: 1;&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>当元素内包含会超出父元素边界的子元素时，<code>overflow</code>方法可能会覆盖掉有用的子元素，或是产生了多余的滚动条。</p>

  <p>（3）<code>:after</code>伪元素的方法</p>

  <div class="highlight"><pre><code class="language-html" data-lang="html"><span class="nt">&lt;style&gt;</span>
    <span class="nc">.clearfix</span> <span class="p">{</span><span class="c">/* 触发 hasLayout */</span> <span class="n">zoom</span><span class="o">:</span> <span class="m">1</span><span class="p">;</span> <span class="p">}</span>
    <span class="nc">.clearfix</span><span class="nd">:after</span> <span class="p">{</span><span class="k">content</span><span class="o">:</span> <span class="s1">&#39;.&#39;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span> <span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span> <span class="m">0</span><span class="p">;</span> <span class="k">clear</span><span class="o">:</span> <span class="k">both</span><span class="p">;</span> <span class="k">visibility</span><span class="o">:</span> <span class="k">hidden</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">&lt;/style&gt;</span>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;box clearfix&quot;</span><span class="nt">&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;main left&quot;</span><span class="nt">&gt;</span>我设置了左浮动 float: left<span class="nt">&lt;/div&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;aside left&quot;</span><span class="nt">&gt;</span>我是页脚，但是我也设置了左浮动。<span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span></code></pre></div>
  <p>这个办法不但完美兼容主流浏览器，并且也很方便，使用重用的类，可以减轻代码编写，另外网页的结构也会更加清晰。</p>
</blockquote>

<p>清除浮动的详细介绍可参考<a href="http://kayosite.com/remove-floating-style-in-detail.html">《详说清除浮动》</a>（<a href="/assets/images/../html/clearfloat.html">示例</a>）。</p>

<h2 id="section-13">块格式化环境</h2>

<blockquote>
  <h4 id="block-formatting-context">块格式化环境（block formatting context）</h4>
  <hr />
  <p>块格式化环境是CCS渲染Web页面的一块区域，块盒子在该区域内布局。</p>

  <p>块格式化环境对（<code>float</code>）定位和清除（<code>clear</code>）浮动至关重要，定位和清除浮动只对同一块格式化环境中的对象有效。<code>float</code>不会影响到其它块格式化环境中的盒子，<code>clear</code>只清除同一块格式化环境中之前的float效果。</p>

  <p>简单来说，块格式化环境是一种属性，这种属性会影响着元素的定位以及与其兄弟元素之间的相互作用。</p>
</blockquote>

<p>块格式化环境就是一个作用范围，可理解为一个独立的容器，这个容器的里盒子的布局与这个容器外的不相干。</p>

<p>块格式化环境不存在嵌套包含关系，块格式化环境只包含该环境内的对象，不会再包含该环境中对象再创建的块格式化环境。（A block formatting context contains everything inside of the element creating it that is not also inside a descendant element that creates a new block formatting context.）</p>

<blockquote>
  <h4 id="section-14">创建块格式化环境的条件（满足任意一条即可）</h4>
  <hr />
  <ul>
    <li>the root element or something that contains it；</li>
    <li><code>float</code>设置为<strong>非</strong><code>none</code>；</li>
    <li><code>position</code>设置为<code>absolute</code>或<code>fixed</code>；</li>
    <li><code>display</code>设置为<code>inline-block</code>、<code>table-cell</code>、<code>table-caption</code>、<code>flex</code>或<code>inline-flex</code>；</li>
    <li><code>overflow</code>设置为<strong>非</strong><code>visible</code>。</li>
  </ul>
</blockquote>

<p>块格式化环境主要有三个特性（详见<a href="http://www.cnblogs.com/leejersey/p/3991400.html">《详说 Block Formatting Contexts (块级格式化上下文)》</a>，<a href="/assets/images/../html/bfc.html">示例</a>）：</p>

<ol>
  <li>阻止<a href="#margin-collapsing">外边距折叠</a>；    </li>
  <li>包含浮动的元素（<a href="#clearing-floats">清除浮动</a>之<code>overflow</code>方法）；   </li>
  <li>阻止元素被浮动元素覆盖。 </li>
</ol>

<h2 id="section-15">高级选择器</h2>

<h4 id="css-21-">CSS 2.1 属性选择器</h4>

<table>
  <tbody>
    <tr>
      <td>E[att]</td>
      <td>匹配所有具有att属性的E元素，不考虑它的值。（注意：E在此处可以省略，比如”[cheacked]”。以下同。）</td>
    </tr>
    <tr>
      <td>E[att=val]</td>
      <td>匹配所有att属性等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att~=val]</td>
      <td>匹配所有att属性具有多个空格分隔的值、其中一个值等于”val”的E元素。</td>
    </tr>
    <tr>
      <td>E[att|=val]</td>
      <td>匹配所有att属性具有多个连字号分隔（hyphen-separated）的值、其中一个值以”val”开头的E元素，主要用于lang属性，比如”en”、”en-us”、”en-gb”等等。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="o">[</span><span class="nt">title</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">div</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">error</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">td</span><span class="o">[</span><span class="nt">headers</span><span class="o">~=</span><span class="nt">col1</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">p</span><span class="o">[</span><span class="nt">lang</span><span class="o">|=</span><span class="nt">en</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">blockquote</span><span class="o">[</span><span class="nt">class</span><span class="o">=</span><span class="nt">quote</span><span class="o">][</span><span class="nt">cite</span><span class="o">]</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#f00</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<h4 id="css-21-pseudo-classes">CSS 2.1 伪类（pseudo-classes）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-child</td>
      <td>匹配父元素的第一个子元素。</td>
    </tr>
    <tr>
      <td>E:link</td>
      <td>匹配所有未被点击的链接。</td>
    </tr>
    <tr>
      <td>E:visited</td>
      <td>匹配所有已被点击的链接。</td>
    </tr>
    <tr>
      <td>E:active</td>
      <td>匹配鼠标已经其上按下、还没有释放的E元素。</td>
    </tr>
    <tr>
      <td>E:hover</td>
      <td>匹配鼠标悬停其上的E元素。</td>
    </tr>
    <tr>
      <td>E:focus</td>
      <td>匹配获得当前焦点的E元素。</td>
    </tr>
    <tr>
      <td>E:lang(c)</td>
      <td>匹配lang属性等于c的E元素。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="nd">:first-child</span> <span class="p">{</span> <span class="k">font-style</span><span class="o">:</span><span class="k">italic</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus</span> <span class="p">{</span> <span class="k">color</span><span class="o">:</span><span class="m">#000</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="m">#ffe</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">input</span><span class="o">[</span><span class="nt">type</span><span class="o">=</span><span class="nt">text</span><span class="o">]</span><span class="nd">:focus:hover</span> <span class="p">{</span> <span class="k">background</span><span class="o">:</span><span class="m">#fff</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">q</span><span class="nd">:lang</span><span class="o">(</span><span class="nt">sv</span><span class="o">)</span> <span class="p">{</span> <span class="k">quotes</span><span class="o">:</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\201D&quot;</span> <span class="s2">&quot;\2019&quot;</span> <span class="s2">&quot;\2019&quot;</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<h4 id="css-21-pseudo-elements">CSS 2.1 伪元素（pseudo-elements）</h4>

<table>
  <tbody>
    <tr>
      <td>E:first-line</td>
      <td>匹配E元素的第一行。</td>
    </tr>
    <tr>
      <td>E:first-letter</td>
      <td>匹配E元素的第一个字母。</td>
    </tr>
    <tr>
      <td>E:before</td>
      <td>在E元素之前插入生成的内容。</td>
    </tr>
    <tr>
      <td>E:after</td>
      <td>在E元素之后插入生成的内容。</td>
    </tr>
  </tbody>
</table>

<p>示例：</p>

<div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span><span class="nd">:first-line</span> <span class="p">{</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="k">color</span><span class="p">;</span><span class="m">#600</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.preamble</span><span class="nd">:first-letter</span> <span class="p">{</span> <span class="k">font-size</span><span class="o">:</span><span class="m">1.5em</span><span class="p">;</span> <span class="k">font-weight</span><span class="o">:</span><span class="k">bold</span><span class="p">;</span> <span class="p">}</span>
<span class="nc">.cbb</span><span class="nd">:before</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span><span class="s2">&quot;&quot;</span><span class="p">;</span> <span class="k">display</span><span class="o">:</span><span class="k">block</span><span class="p">;</span> <span class="k">height</span><span class="o">:</span><span class="m">17px</span><span class="p">;</span> <span class="k">width</span><span class="o">:</span><span class="m">18px</span><span class="p">;</span> <span class="k">background</span><span class="o">:</span><span class="sx">url(top.png)</span> <span class="k">no-repeat</span> <span class="m">0</span> <span class="m">0</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span><span class="m">0</span> <span class="m">0</span> <span class="m">0</span> <span class="m">-18px</span><span class="p">;</span> <span class="p">}</span>
<span class="nt">a</span><span class="nd">:link:after</span> <span class="p">{</span> <span class="k">content</span><span class="o">:</span> <span class="s2">&quot; (&quot;</span> <span class="n">attr</span><span class="p">(</span><span class="n">href</span><span class="p">)</span> <span class="s2">&quot;) &quot;</span><span class="p">;</span> <span class="p">}</span></code></pre></div>

<p>更多关于CSS选选择器类容可参考<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference">CSS reference</a>。</p>

<h2 id="section-16">使用技巧</h2>

<blockquote>
  <h4 id="important">!important规则</h4>
  <hr />
  <p>多条CSS语句互相冲突时，具有!important的语句将覆盖其他语句。由于IE不支持!important，所以也可以利用它区分不同的浏览器。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span><span class="k">color</span><span class="o">:</span> <span class="nb">red</span> <span class="cp">!important</span><span class="p">;</span> <span class="k">color</span><span class="o">:</span> <span class="nb">blue</span><span class="p">;}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-17">容器水平居中</h4>
  <hr />
  <p>先为该容器设置一个明确宽度，然后将margin的水平值设为auto即可。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">div</span><span class="nf">#container</span> <span class="p">{</span><span class="k">width</span><span class="o">:</span> <span class="m">760px</span><span class="p">;</span> <span class="k">margin</span><span class="o">:</span> <span class="m">0</span> <span class="k">auto</span><span class="p">;}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-18">禁止自动换行</h4>
  <hr />
  <p>文字在一行中显示完成，不要自动换行。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span> <span class="k">white-space</span><span class="o">:</span> <span class="k">nowrap</span><span class="p">;</span> <span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-19">图片宽度自适应</h4>
  <hr />
  <p>如何使得较大的图片，能够自动适应小容器的宽度？</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">img</span> <span class="p">{</span><span class="k">max-width</span><span class="o">:</span> <span class="m">100%</span><span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="link">设置link状态的顺序</h4>
  <hr />
  <p>link的四种状态，需要按照下面的前后顺序进行设置。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">a</span><span class="nd">:link</span> 
<span class="nt">a</span><span class="nd">:visited</span> 
<span class="nt">a</span><span class="nd">:hover</span> 
<span class="nt">a</span><span class="nd">:active</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="text-transformfont-variant">Text-transform和Font Variant</h4>
  <hr />
  <p>Text-transform用于将所有字母变成小写字母、大写字母或首字母大写。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">uppercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">lowercase</span><span class="p">}</span> 
<span class="nt">p</span> <span class="p">{</span><span class="k">text-transform</span><span class="o">:</span> <span class="k">capitalize</span><span class="p">}</span></code></pre></div>
  <p>Font Variant用于将字体变成小型的大写字母（即与小写字母等高的大写字母）。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">p</span> <span class="p">{</span><span class="k">font-variant</span><span class="o">:</span> <span class="k">small-caps</span><span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-20">用图片充当列表标志</h4>
  <hr />
  <p>默认情况下，浏览器使用一个黑圆圈作为列表标志，可以用图片取代它。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">ul</span> <span class="p">{</span><span class="k">list-style</span><span class="o">:</span> <span class="k">none</span><span class="p">}</span>
<span class="nt">ul</span> <span class="nt">li</span> <span class="p">{</span>
    <span class="k">background-image</span><span class="o">:</span> <span class="sx">url(&quot;path-to-your-image&quot;)</span><span class="p">;</span>
    <span class="k">background-repeat</span><span class="o">:</span> <span class="k">none</span><span class="p">;</span>
    <span class="k">background-position</span><span class="o">:</span> <span class="m">0</span> <span class="m">0.5em</span><span class="p">;</span> 
<span class="p">}</span></code></pre></div>
</blockquote>

<blockquote>
  <h4 id="section-21">用图片替换文字</h4>
  <hr />
  <p>在标题栏中使用图片，但是又必须保证搜索引擎能够读到标题。</p>

  <div class="highlight"><pre><code class="language-css" data-lang="css"><span class="nt">h1</span> <span class="p">{</span> 
    <span class="k">text-indent</span><span class="o">:</span> <span class="m">-9999px</span><span class="p">;</span> 
    <span class="k">background</span><span class="o">:</span> <span class="sx">url(&quot;h1-image.jpg&quot;)</span> <span class="k">no-repeat</span><span class="p">;</span> 
    <span class="k">width</span><span class="o">:</span> <span class="m">200px</span><span class="p">;</span>
    <span class="k">height</span><span class="o">:</span> <span class="m">50px</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>
</blockquote>

<h2 id="section-22">预处理器</h2>

<p>CSS预处理器（css preprocessor）的基本思想是，用一种专门的编程语言，进行网页样式设计，然后再编译成CSS文件。常用的CSS预处理器有<a href="http://lesscss.org">Less</a>、<a href="http://sass-lang.com">Sass</a>等。</p>

<h3 id="less">Less</h3>

<p><a href="http://lesscss.org">Less</a>使用变量（variables）、混合（mixins）、函数（functions）和许多其他的技术，让你的CSS更具维护性、主题性、扩展性。Less可运行在Node环境，浏览器环境和Rhino环境，同时也有3种可选工具供你编译文件和监视任何改变。</p>

<p>具体用法可参考Bootstrap中文网的<a href="http://www.bootcss.com/p/lesscss/" title="LESS « 一种动态样式语言">Less教程</a>。</p>

<h3 id="sass--compass">Sass &amp; Compass</h3>

<p><a href="http://sass-lang.com">Sass</a>使用变量（variables）、混合（mixins）、嵌套规则（nested rules）、内联导入（inline imports）等，让CSS更加优雅和强大。Sass让CSS更好的组织、体积更小、速度更快。</p>

<p><a href="http://compass-style.org">Compass</a>是Sass的工具库（toolkit）。Sass本身只是一个编译器，Compass在它的基础上，封装了一系列有用的模块和模板，补充Sass的功能。它们之间的关系，有点像Javascript和jQuery、Ruby和Rails、python和Django的关系。</p>

<p>具体使用可参考阮一峰的<a href="http://www.ruanyifeng.com/blog/2012/06/sass.html" title="SASS用法指南">Sass</a>和<a href="http://www.ruanyifeng.com/blog/2012/11/compass.html" title="Compass用法指南">Compass</a>用法指南。</p>

<h2 id="section-23">参考资料</h2>

<ol>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS" title="Cascading Style Sheets">MDN: CSS</a></li>
  <li><a href="http://www.w3.org/Style/CSS/" title="Cascading Style Sheets home page">W3: Cascading Style Sheets home page</a></li>
  <li><a href="http://www.w3school.com.cn/css/index.asp" title="CSS 教程">w3school: CSS 教程</a></li>
  <li><a href="http://www.w3.org/TR/2001/WD-css3-values-20010713/#specified" title="Specified, computed, and actual values">W3: Specified, computed, and actual values</a></li>
  <li><a href="http://www.w3.org/TR/css3-values/" title="CSS Values and Units Module Level 3">W3: CSS Values and Units Module Level 3</a></li>
  <li><a href="http://www.w3.org/TR/2011/REC-css3-color-20110607/">W3: CSS Color Module Level 3</a></li>
  <li><a href="http://dev.w3.org/csswg/css-box/">W3: CSS basic box model</a></li>
  <li><a href="http://dev.w3.org/csswg/css-grid/">W3: CSS Grid Layout Module Level 1</a></li>
  <li><a href="http://dev.w3.org/csswg/css-flexbox/">W3: CSS Flexible Box Layout Module Level 1</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2010/03/css_cookbook.html">阮一峰：CSS使用技巧</a>    </li>
  <li><a href="http://www.ruanyifeng.com/blog/2009/03/css_selectors.html">阮一峰：CSS选择器笔记</a></li>
  <li><a href="http://www.cnblogs.com/cuishengli/archive/2012/06/22/2558859.html">cuishengli：CSS 框模型概述</a></li>
  <li><a href="http://www.cnblogs.com/leejersey/p/3991400.html">leejersey：详说 Block Formatting Contexts (块级格式化上下文)</a></li>
  <li><a href="http://kayosite.com/remove-floating-style-in-detail.html">Kayo：详说清除浮动</a></li>
  <li><a href="http://www.catswhocode.com/blog/8-css-preprocessors-to-speed-up-development-time" title="8 CSS preprocessors to speed up development time">Jean: 8 CSS preprocessors to speed up development time</a></li>
  <li><a href="http://www.bootcss.com/p/lesscss/" title="LESS « 一种动态样式语言">LESS « 一种动态样式语言</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2012/06/sass.html" title="SASS用法指南">阮一峰：SASS用法指南</a></li>
  <li><a href="http://www.ruanyifeng.com/blog/2012/11/compass.html" title="Compass用法指南">阮一峰：Compass用法指南</a></li>
</ol>

<h3 id="section-24">脚注</h3>
<div class="footnotes">
  <ol>
    <li id="fn:ccs_master">
      <p>也可参考”CSS Mastery: Advanced Web Standards Solutions, Second Edition”一书的”Chapter 3: Visual Formatting Model Overview”。 <a href="#fnref:ccs_master" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：感知器算法</title>
      <link href="http://qianjiye.de/2014/10/machine-learning-perceptron-learning-algorithm" />
      <pubdate>2014-10-21T18:09:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/10/machine-learning-perceptron-learning-algorithm</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">问题描述</h2>

<p>对于线性二分类问题<br />
\begin{equation}
y = \left\{
\begin{aligned}
&amp; +1 &amp; \sum\nolimits_{i=1}^dw_ix_i &gt; \mbox{threshold} \\
&amp; -1 &amp; \sum\nolimits_{i=1}^dw_ix_i &lt; \mbox{threshold}
\end{aligned}
\right. 
\end{equation}</p>

<p>也可以记为
\begin{equation}
h(x) = \mbox{sign}\left(\sum_{i=1}^dw_ix_i - \mbox{threshold}\right) = \mbox{sign}\left(\sum_{i=0}^dw_ix_i\right) = \mbox{sign}\left(\mathbf{w^Tx}\right)
\end{equation} <br />
其中，$w_0 = -\mbox{threshold}, x_0 = +1$。</p>

<p>问：如何估计模型模型$h(x)$的参数$\mathbf{w}$？  <br />
答：感知器算法（PLA，Perceptron Learning Algorithm）。</p>

<h2 id="section-1">感知器算法</h2>

<p>感知器算法通过迭代更新模型参数，直到没有错分的样本点。  </p>

<blockquote>
  <h4 id="pla">PLA</h4>
  <hr />
  <p>repeat until a full cycle of not encountering mistakes { <br />
1. 找到参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$。  <br />
}</p>
</blockquote>

<div class="image_line">
<div class="image_card">
<img src="/assets/images/2014-10-21-pla_1.png" alt="PLA更新模型参数示意图" />
<div class="caption">PLA更新模型参数示意图</div>
</div>
</div>

<p>PLA更新模型参数解释： <br />
1. 当$y=+1$时，若分错，$\mathbf w^T \mathbf x &lt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太大（大于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变小；<br />
2. 当$y=-1$时，若分错，$\mathbf w^T \mathbf x &gt; 0$，表示$\mathbf{w}$和$\mathbf x$的夹角太小（小于90度），需要调整$\mathbf w$，使其与$\mathbf x$的夹角变大。   </p>

<p>通过参数更新规则可知  <br />
\[
\mathbf w_{t+1} ＝ \mathbf w_t + y_{n}\mathbf x_{n}
\]<br />
两边转置后同时乘上$y_n\mathbf x_n$，可得
\[
y_n\mathbf w^T_{t+1} \mathbf x_n \geq y_n\mathbf w^T_t\mathbf x_n
\]
这表明，参数更新始终在试图纠正模型参数。  </p>

<p><strong>注意事项</strong>：$\mathbf w$是决策界（判别面）的法向量。</p>

<h2 id="section-2">理论分析</h2>

<blockquote>
  <ol>
    <li>PLA算法会终止么？</li>
    <li>能否从候选模式$h$中学习到目标模式$f$？</li>
  </ol>
</blockquote>

<p>如果数据集$\mathcal D$线性可分，存在一个完美$\mathbf w_f$，使得对数据集中所有样本点$y_n = \mbox{sign}\left(\mathbf w_f^T\mathbf x_n\right)$，对于任意一个样本点总有</p>

<p>\begin{equation}
y_{n(t)}\mathbf w_f^T\mathbf x_{n(t)} \geq \min_n y_n\mathbf x_f^T\mathbf x_n &gt; 0
\end{equation}</p>

<p>根据感知器算法的更新规则可知
\begin{equation}
\begin{aligned}
\mathbf w_f^T\mathbf w_{t+1} 
&amp; =  \mathbf w_f^T\left(\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\right) \\
&amp; \geq \mathbf w_f^T\mathbf w_t + \min_n y_n\mathbf x_f^T\mathbf x_n \\
&amp; &gt; \mathbf w_f^T\mathbf w_t 
\end{aligned}
\end{equation}
由此可见，参数更新后，$\mathbf w_{t+1}$<em>可能会</em>更加接近$\mathbf w_f$。但是，还不能确定是由于向量间夹角变小还是$\mathbf w_{t+1}$长度变大导致的内积增加。
\begin{equation}
\begin{aligned}
\|\mathbf w_{t+1}\|^2
&amp; = \|\mathbf w_t + y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; = \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 + 2y_{n(t)}\mathbf w_t^T\mathbf x_{n(t)} \\
&amp; \leq \|\mathbf w_t\|^2 + \|y_{n(t)}\mathbf x_{n(t)}\|^2 \\
&amp; \leq \|\mathbf w_t\|^2 + \max_n\|\mathbf x_{n}\|^2
\end{aligned}
\end{equation}
由此可见，每次更新的时候，向量长度的增加是有限的，最多增加样本最长向量的长度。</p>

<p>事实上，从$\mathbf w_0$开始，经过$T$次迭代后有
\begin{equation}
\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\frac{\mathbf w_T}{\|\mathbf w_T\|}\geq
\sqrt T \times \mbox{constant}
\label{eq:need_to_be_done_1}
\end{equation}
PLA算法迭代次数的上界满足$T\leq R^2/\rho^2$，其中
\begin{equation}
R^2 = \max_n \|\mathbf x_n\|^2,~~\rho=\min_n y_n\frac{\mathbf w_f^T}{\|\mathbf w_f\|}\mathbf x_n
\label{eq:need_to_be_done_2}
\end{equation}</p>

<p>相关的证明如下：  <br />
<img src="/assets/images/2014-10-21-pla_2.jpg" alt="相关的证明" /></p>

<p><strong>结论：</strong>     <br />
1. 对于线性可分的样本集合，通过PLA修正模型参数，$\mathbf w_f$和$\mathbf w_t$的内积增长快，$\mathbf w_t$的长度增长慢，$\mathbf w_t$越来越靠近$\mathbf w_f$，最终算最终收敛； <br />
2. 对于未知的样本集，作用在其上的PLA算法可能长时间不收敛，导致这样的情况可能是迭代次数不够（理论上的参数$T$由于目标模型$f$未知而难以估计）或者样本集存在噪声（线性不可分）。</p>

<h2 id="pocket-pla">Pocket PLA</h2>

<p>判断样本集是否线性可分的复杂度为NP-hard。实际上，通常样本集存在一些噪声，不可能严格的线性可分，PLA算法无法满足收敛条件。Pocket PLA通过改变PLA迭代结束的条件和参数更新规则仍然可以估计模型参数。</p>

<blockquote>
  <h4 id="pocket-pla-1">Pocket PLA</h4>
  <hr />
  <p>repeat until enough iterations {<br />
1. 随机抽取参数$\mathbf w_t$时对应错分的样本点$\left(\mathbf x_{n(t)}, y_{n(t)}\right)$，$\mbox{sign}\left(\mathbf w^T \mathbf x_{n(t)}\right) \neq y_{n(t)}$；     <br />
2. 修正参数，$\mathbf w_{t+1}\leftarrow \mathbf w_t + y_{n(t)}\mathbf x_{n(t)}$；<br />
3. 如果$\mathbf w_{t+1}$在样本集上的表现优于$\hat{\mathbf w}$，$\hat{\mathbf w}\leftarrow \mathbf w_{t+1}$。 <br />
}</p>
</blockquote>

<p>从第3步可知，Pocket PLA的算法时间复杂度要高于PLA，如果对于线性可分的样本集，Pocket PLA比PLA慢。</p>

<h2 id="section-3">思考问题</h2>

<ol>
  <li>PLA和梯度下降法有无联系？</li>
  <li>在分类性能上PLA求解的线性模型和Logistic回归有何差别？</li>
  <li>如何推导公式\eqref{eq:need_to_be_done_1}\eqref{eq:need_to_be_done_2}？ <a href="https://class.coursera.org/ntumlone-002/forum/thread?thread_id=28">答案</a></li>
</ol>

<h2 id="section-4">參考資料</h2>

<ol>
  <li><a href="https://class.coursera.org/ntumlone-002">機器學習基石(Machine Learning Foundations)</a>    </li>
  <li><a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf">Convergence Proof for the Perceptron Algorithm</a></li>
</ol>

]]&gt;</content:encoded>
    </item>
    
    <item>
      <title>机器学习：正则化</title>
      <link href="http://qianjiye.de/2014/10/machine-learning-regularization" />
      <pubdate>2014-10-20T00:00:00+08:00</pubdate>
      <author>Jiye Qian</author>
      <guid>http://qianjiye.de/2014/10/machine-learning-regularization</guid>
      <content:encoded>&lt;![CDATA[<h2 id="section">为什么需要正则化</h2>

<p>过拟合（overfitting）是指模型可以在训练集上表现出色，在新数据上性能却很差。解决过拟合问题的方法：</p>

<ul>
  <li>减少特征数目：手工选择特征、利用模型选择；</li>
  <li>正则化（regularization）：保留所有特征，但是减小$\theta_j$，使特征对预测$y$贡献小。</li>
</ul>

<p>正则化通过在代价函数中加入正则化项限制模型参数，避免过拟合。</p>

<p>线性回归和Logistic回归的代价函数追加的正则化项是$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。</p>

<ul>
  <li>$\lambda$称为正则化参数，增大$\lambda$以减小过拟合；</li>
  <li>$\lambda$过大（$10^{10}$）时，有$\theta_j\approx 0~~(j = 1,2,\ldots,n)$，则$h_\theta(x) = \theta_0$，这会导致欠拟合（underfitting）；</li>
  <li>正则化只作用于$j\ge 1$的非常数项，实际上，正则化常数项对结果影响也不大。</li>
</ul>

<p>资料来源：<a href="#ng_ml_r_2014">[1]</a>。</p>

<p>bias &amp; variance:</p>

<ul>
  <li>high bias: 欠拟合；</li>
  <li>high variance: 过拟合。</li>
</ul>

<h2 id="section-1">正则化线性回归</h2>

<h3 id="section-2">代价函数</h3>

<p>\begin{equation}
J(\theta) = \frac{1}{2m}\left( \sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)^2 + \lambda\sum_{j=1}^n\theta_j^2 \right)
\label{eq:cf-linear-regression-r}
\end{equation}</p>

<h3 id="section-3">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<p>迭代过程可以化为如下形式：
\begin{equation*}
\theta_j := \theta_j\left(1 - \alpha\frac{\lambda}{m} \right) - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)};~~(j = 1, 2, \ldots, n)
\end{equation*}</p>

<p>通常$1 - \alpha\frac{\lambda}{m} &lt; 1$，与非正则化的梯度下降法比较，$\theta_j$减小更快。</p>

<h3 id="section-4">正规方程估计参数</h3>

<p>\begin{equation}
\theta = \left(X^TX + \lambda
\begin{bmatrix}
0  &amp;   &amp;        &amp; \\
   &amp; 1 &amp;        &amp; \\
   &amp;   &amp; \ddots &amp; \\
   &amp;   &amp;        &amp; 1
\end{bmatrix}
\right)^{-1}X^Ty
\end{equation}</p>

<p>可以证明，加入正则化项后矩阵始终可逆。</p>

<h2 id="logistic">正则化Logistic回归</h2>

<h3 id="section-5">代价函数</h3>

<p>\begin{equation}
\begin{aligned}
J(\boldsymbol\theta)  = &amp;-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)+\left(1-y^{(i)}\right)\log \left(1-h_{\boldsymbol\theta}\left(\mathbf x^{(i)}\right)\right)\right) \\
&amp; + \frac{\lambda}{2m}\sum_{j=1}^n\boldsymbol\theta_j^2
\end{aligned}
\label{eq:cf-logistic-regression-r}
\end{equation}</p>

<h3 id="section-6">梯度下降法估计参数</h3>

<p>repeat until convergence {
\begin{equation*}
\begin{aligned}
\theta_0 &amp; := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_0^{(i)} \\ <br />
\theta_j &amp; := \theta_j - \alpha\frac{1}{m}\left(\sum_{i=1}^m\left( h_\theta\left(x^{(i)}\right) - y^{(i)} \right)x_j^{(i)} + \lambda\theta_j\right)~~(j = 1, 2, \ldots, n)
\end{aligned}
\end{equation*}
}</p>

<h3 id="matlab">Matlab实现</h3>

<p>第一步：实现Logistic函数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>g <span class="p">=</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span>z<span class="p">)</span><span class="w"></span>
<span class="n">g</span> <span class="p">=</span> <span class="mf">1.0</span> <span class="o">./</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="k">end</span></code></pre></div>

<p>第二步：实现代价函数（包含梯度计算）</p>

<div class="highlight"><pre><code class="language-matlab"><span class="k">function</span><span class="w"> </span>[J, grad] <span class="p">=</span><span class="w"> </span><span class="nf">costFunctionReg</span><span class="p">(</span>theta, X, y, lambda<span class="p">)</span><span class="w"></span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>

<span class="n">h</span> <span class="p">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">J</span> <span class="p">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="o">+</span> <span class="c">...</span>
    <span class="n">lambda</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.^</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">grad</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">;</span> <span class="n">theta</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)])</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="k">end</span></code></pre></div>

<p>第三步：估计参数</p>

<div class="highlight"><pre><code class="language-matlab"><span class="n">initial_theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">lambda</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">options</span> <span class="p">=</span> <span class="n">optimset</span><span class="p">(</span><span class="s">&#39;GradObj&#39;</span><span class="p">,</span> <span class="s">&#39;on&#39;</span><span class="p">,</span> <span class="s">&#39;MaxIter&#39;</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">exit_flag</span><span class="p">]</span> <span class="p">=</span> <span class="c">...</span>
	<span class="n">fminunc</span><span class="p">(@(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunctionReg</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span></code></pre></div>

<h2 id="section-7">思考问题</h2>

<ol>
  <li>如何推导正则化线性回归的正规方程解？</li>
  <li>求解正则化线性回归的正规方程时，为何矩阵始终可逆？</li>
  <li>如何选取合适的$\lambda$？</li>
</ol>

<h2 id="section-8">参考资料</h2>

<ol class="bibliography"><li><span id="ng_ml_r_2014">[1]A. Ng, “Regularization: The problem of overfitting.” Coursera, 2014.</span>

[<a href="https://www.coursera.org/course/ml">Online</a>]

</li></ol>
]]&gt;</content:encoded>
    </item>
    
  </channel>
</rss>
